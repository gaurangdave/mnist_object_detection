{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b335c0b",
   "metadata": {},
   "source": [
    "# Training & Evaluation\n",
    "* In this notebook we are going to 2 kinds of object detection models,\n",
    "    * An Object Detection CNN from scratch, the hypothesis is that since the dataset is simple, we won't need an expert model for object detection. A simple CNN should be able to give us a descent accuracy.\n",
    "    * Object detection using transfer learning - This is going to more of a hands on experience of transfer learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4838cf",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0af58b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 09:55:48.043977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753462548.062826   31863 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753462548.068911   31863 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753462548.084455   31863 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753462548.084478   31863 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753462548.084480   31863 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753462548.084482   31863 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-25 09:55:48.089687: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d221c8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "## validate tensorflow \n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e49fdf",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c523f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"..\",\"data\")\n",
    "models_dir = Path(\"..\",\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c628582",
   "metadata": {},
   "source": [
    "## Import Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "733001cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Build an absolute path from this notebook's parent directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "# Add to sys.path if not already present\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from src import data_generator\n",
    "\n",
    "## logic to auto reload scripts without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac63cdf",
   "metadata": {},
   "source": [
    "## Training Object Detection CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf21a93",
   "metadata": {},
   "source": [
    "### Step 1: Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a181f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path(data_dir,\"raw\",\"raw_mnist_data.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aab81f",
   "metadata": {},
   "source": [
    "### Step 2: Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e66e460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_images = data.drop(columns=[\"class\"])\n",
    "raw_labels = data[\"class\"]\n",
    "raw_images.shape, raw_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1f691",
   "metadata": {},
   "source": [
    "### Step 3: Initialize Map Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f3b52c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753462557.205147   31863 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6055 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:2d:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "X_tensor = tf.convert_to_tensor(raw_images, dtype=tf.float32)\n",
    "X_tensor = tf.reshape(X_tensor,shape=(-1,28,28,1))\n",
    "y_tensor = tf.convert_to_tensor(raw_labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor,y_tensor))\n",
    "\n",
    "processed_dataset = raw_dataset.map(lambda X,y: tf.numpy_function(data_generator.generate_training_example, inp=[X,y],Tout=(tf.float32,tf.float32)), num_parallel_calls=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc65486",
   "metadata": {},
   "source": [
    "### Step 4: Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ce7114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Rescaling(scale=1./255),\n",
    "\n",
    "    ## starting with a larger filter since we are dealing with 100x100x1 image\n",
    "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    ## rest of the layers are same as our original mnist classifier\n",
    "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    ## finaly layers to output 6x6x45 grid of predictions\n",
    "    tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear'),\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "961fbeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling_2 (\u001b[38;5;33mRescaling\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42f731",
   "metadata": {},
   "source": [
    "### Step 4.1 Define Custom Loss Function\n",
    "Right now our model output has the shape of 6x6x45. Which means we have 36 grid cells, each with 3 possible bounding box and each bounding box is defined by 1 cell for objectness i.e. is a object present this this box, 4 cells for bounding box dimensions (2 for center coordinates and 2 for width and height), and finally 10 cells for one hot encoded classification of digits from 0 to 9.\n",
    "\n",
    "We can break down prediction of these slices into different machine learning problems, for e.g.\n",
    "\n",
    "- 0/1 for objectness score is a binary classification problem\n",
    "- 4 digit bounding box prediction is a regression problem\n",
    "- 10 digits number prediction is multi-class classification problem\n",
    "\n",
    "Since these are different ML problems we cannot have same activation function for them, so our final activation layer needs to be linear (i.e. no activation) and then we’ll apply specific activation to specific slice based on the prediction we want. So\n",
    "\n",
    "- 0/1 problem will be activated using `Sigmoid Activation` function\n",
    "- 4 digit bounding box will be same, a linear activation\n",
    "- 10 digit number problem will be activated using `Softmax Activation`\n",
    "\n",
    "Similarly we’ll use different loss functions for all 3, so\n",
    "\n",
    "- 0/1 problem will use LogLoss function\n",
    "- 4 digit bounding box will use RMSE or MSE loss function\n",
    "- 10 digit class prediction we’ll use `SparseCategoricalCrossEntropy`\n",
    "\n",
    "#### Match Making\n",
    "* We'll also need to find the right grid to calculate the loss function against. We do that by finding the grid cell in which center of ground truth might be present.\n",
    "* In our case we have a 6x6 grid, so 36 cells and lets say we just have 2 images in our ground truth and 3 anchor boxes.\n",
    "* In our first step out of 36 cells we find one or may be 2 cells where the center of these cells lie\n",
    "* For each of cell we'll have 3 anchor boxes and so we'll calculate the IOUs and the 2 with maximum IOUs will win\n",
    "* We take those 2 anchor box, slice the output, apply activation function for objectness and 10 digit classification. Keep coordinates as it is since output of final layer is already linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9707dc57",
   "metadata": {},
   "source": [
    "Quick Notes on how to Match make\n",
    "* Create a empty 100x100 grid (we can also create constant for grid cells)\n",
    "* For each cell check if "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66c6938",
   "metadata": {},
   "source": [
    "##### Cutom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d4e5bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is temp code to test the loss function do not use this for training.\n",
    "X_tensor = tf.convert_to_tensor(raw_images.iloc[0:10], dtype=tf.float32)\n",
    "X_tensor = tf.reshape(X_tensor, shape=(-1, 28, 28, 1))\n",
    "y_tensor = tf.convert_to_tensor(raw_labels.iloc[0:10], dtype=tf.float32)\n",
    "\n",
    "\n",
    "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
    "\n",
    "\n",
    "def generative_py_function(func, inp, Tout, shape_out):\n",
    "    # This is the bridge that calls your NumPy code\n",
    "    y = tf.numpy_function(func, inp, Tout)\n",
    "    # This is the crucial step: re-apply the shape information\n",
    "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
    "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
    "    return y\n",
    "\n",
    "# Define the exact output shapes you expect\n",
    "output_shapes = ([100, 100, 1], [5, 15]) \n",
    "# Define the exact output data types you expect\n",
    "output_types = (tf.float32, tf.float32)\n",
    "\n",
    "\n",
    "# processed_dataset = raw_dataset.map(lambda X, y: tf.numpy_function(data_generator.generate_training_example, inp=[X, y], Tout=(\n",
    "#     tf.float32, tf.float32)), num_parallel_calls=15);\n",
    "\n",
    "# Use the wrapper inside the map\n",
    "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
    "    data_generator.generate_training_example, \n",
    "    inp=[X, y], \n",
    "    Tout=output_types, # Pass the dtypes to Tout\n",
    "    shape_out=output_shapes # Pass the shapes to our new argument\n",
    ")).batch(batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1383b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = Path(\"./output.txt\")\n",
    "\n",
    "\n",
    "def calculate_model_loss(y_true, y_pred):\n",
    "    x_grid_size = y_pred.shape[1]\n",
    "    y_grid_size = y_pred.shape[2]\n",
    "\n",
    "    # we have 6x6, each grid cell has 3 anchor box i.e 108 anchor boxes per insantance\n",
    "    anchor_boxes = tf.reshape(\n",
    "        y_pred, shape=(-1, x_grid_size, y_grid_size, 3, 15))\n",
    "\n",
    "    # Read the bounding box centers\n",
    "    # Each instance in the bach will have 5 bounding box centers\n",
    "    bounding_box_centers = y_true[:, :, 1:3]\n",
    "\n",
    "    # TODO:  here we are assuming number of rows and columns in grid is same. Confirm the assumption.\n",
    "    # The general formula is: grid_index = floor(pixel_coordinate * (grid_size / image_size))\n",
    "    # convert each 5 bounding box centers to 5 possible grids for each instance\n",
    "    grid_indices = tf.cast(\n",
    "        tf.floor(bounding_box_centers * (x_grid_size / 100)), dtype=tf.int32)\n",
    "\n",
    "    # out of 36 grid cells (per instance) select at most 5 grid cells that have ground truth bounding box\n",
    "    # so out of 108 anchor boxes (per instance) we only need to check 15 anchor boxes\n",
    "    selected_anchor_boxes = tf.gather_nd(\n",
    "        anchor_boxes, batch_dims=1, indices=grid_indices)\n",
    "    \n",
    "    ## calcualte the IOU between anchor boxes and ground truth    \n",
    "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
    "    x_min_for_intersection = tf.maximum(selected_anchor_boxes[:,:,:,1],expanded_y_true[:,:,:,1])\n",
    "    print(f\"x_min_for_intersection {x_min_for_intersection}\")\n",
    "\n",
    "    # read the bounding box from grid indices\n",
    "    # grid_data = y_pred[grid_indices]?\n",
    "    print(\"----- True Values -----\")\n",
    "    print(f\"y_true.shape {y_true.shape}\")\n",
    "\n",
    "    print(\"----- Pred Values -----\")\n",
    "    print(f\"y_pred.shape {y_pred.shape}\")\n",
    "    print(f\"anchor_boxes.shape {anchor_boxes.shape}\")\n",
    "    print(f\"grid indices shape {grid_indices.shape}\")\n",
    "    print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
    "\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "\n",
    "def calculate_model_metrics(y_true, y_pred):\n",
    "    return 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b302aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting Tensors Passed to Loss Function ---\n",
      "Shape of y_pred from model: (5, 6, 6, 45)\n",
      "x_min_for_intersection [[[2.1000000e+01 2.1000000e+01 2.1000000e+01]\n",
      "  [1.3000000e+01 1.3000000e+01 1.3000000e+01]\n",
      "  [0.0000000e+00 1.1534715e-02 0.0000000e+00]\n",
      "  [0.0000000e+00 1.1534715e-02 0.0000000e+00]\n",
      "  [0.0000000e+00 1.1534715e-02 0.0000000e+00]]\n",
      "\n",
      " [[1.6000000e+01 1.6000000e+01 1.6000000e+01]\n",
      "  [5.8000000e+01 5.8000000e+01 5.8000000e+01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[7.8000000e+01 7.8000000e+01 7.8000000e+01]\n",
      "  [9.8000000e+01 9.8000000e+01 9.8000000e+01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[8.3000000e+01 8.3000000e+01 8.3000000e+01]\n",
      "  [6.3000000e+01 6.3000000e+01 6.3000000e+01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "\n",
      " [[2.5000000e+01 2.5000000e+01 2.5000000e+01]\n",
      "  [8.3000000e+01 8.3000000e+01 8.3000000e+01]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      "  [0.0000000e+00 0.0000000e+00 0.0000000e+00]]]\n",
      "----- True Values -----\n",
      "y_true.shape (5, 5, 15)\n",
      "----- Pred Values -----\n",
      "y_pred.shape (5, 6, 6, 45)\n",
      "anchor_boxes.shape (5, 6, 6, 3, 15)\n",
      "grid indices shape (5, 5, 2)\n",
      "selected_anchor_boxes.shape :(5, 5, 3, 15)\n",
      "\n",
      "--- Final Calculated Loss ---\n",
      "Total Loss for the batch: -5.0996827e-05\n",
      "\n",
      "--- Manual Step Complete ---\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=calculate_model_loss,\n",
    "              metrics=[calculate_model_metrics])\n",
    "# Step 1: Get one batch of data from your dataset pipeline\n",
    "# The .take(1) method creates a new dataset with only the first element.\n",
    "one_batch = processed_dataset.take(1)\n",
    "\n",
    "# Step 2: Iterate over the single batch to get the tensors\n",
    "for images, labels in one_batch:\n",
    "    \n",
    "    # --- THIS IS YOUR DEBUGGING ZONE ---\n",
    "    # Now you have the concrete tensors for one batch.\n",
    "    # You can inspect them with regular print() and .numpy()\n",
    "    \n",
    "    # print(\"--- Inspecting Data Before Loss Calculation ---\")\n",
    "    # print(\"Shape of images (X_batch):\", images.shape)\n",
    "    # print(\"Shape of labels (y_true_batch):\", labels.shape)\n",
    "    # print(\"\\nSample y_true label tensor:\\n\", labels.numpy()[0]) # Print the first label in the batch\n",
    "    # ------------------------------------\n",
    "\n",
    "    # Step 3: Manually run the forward pass and gradient calculation\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Get the model's raw predictions for this batch\n",
    "        y_pred = model(images, training=True)  # Pass the images through the model\n",
    "        \n",
    "        # --- MORE DEBUGGING ---\n",
    "        print(\"\\n--- Inspecting Tensors Passed to Loss Function ---\")\n",
    "        print(\"Shape of y_pred from model:\", y_pred.shape)\n",
    "        # print(\"\\nSample y_pred tensor (first 5 values of first anchor):\\n\", y_pred.numpy()[0, 0, 0, :5])\n",
    "        # ----------------------\n",
    "\n",
    "        # Call your custom loss function\n",
    "        # You can now add print statements INSIDE your loss function too!\n",
    "        loss_value = calculate_model_loss(labels, y_pred)\n",
    "        \n",
    "        print(\"\\n--- Final Calculated Loss ---\")\n",
    "        print(\"Total Loss for the batch:\", loss_value.numpy())\n",
    "        # -----------------------------\n",
    "\n",
    "    # Step 4 (Optional): Calculate and apply gradients to see the full loop\n",
    "    # grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "    # model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    print(\"\\n--- Manual Step Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56deb57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2109c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
