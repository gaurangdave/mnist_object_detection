{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88d9b8e6",
      "metadata": {
        "id": "88d9b8e6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurangdave/mnist_object_detection/blob/main/notebooks/03_google_colab_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5547be1",
      "metadata": {
        "id": "d5547be1"
      },
      "source": [
        "# MNIST Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0tP3N0G6DNgp"
      },
      "id": "0tP3N0G6DNgp"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.datasets import fetch_openml\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import PIL.Image\n",
        "from matplotlib import patches\n",
        "\n"
      ],
      "metadata": {
        "id": "C88hhW3fDQFO"
      },
      "id": "C88hhW3fDQFO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dMCwJ7fDeN1",
        "outputId": "22b3d4e1-a7b2-4aaa-89bd-7dfa67e250e5"
      },
      "id": "4dMCwJ7fDeN1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Access"
      ],
      "metadata": {
        "id": "VPEcmSCfCH8g"
      },
      "id": "VPEcmSCfCH8g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Constants  "
      ],
      "metadata": {
        "id": "wa9FmTPCDbn1"
      },
      "id": "wa9FmTPCDbn1"
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path(\"data\")\n",
        "models_dir = Path(\"models\")"
      ],
      "metadata": {
        "id": "xIdEzErNEHs3"
      },
      "id": "xIdEzErNEHs3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "17DdP_OEEW9f"
      },
      "id": "17DdP_OEEW9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBYeASoEEXRF",
        "outputId": "f43550c3-4835-443a-e110-c5206a1a906a"
      },
      "id": "WBYeASoEEXRF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xe9omI65EXT6"
      },
      "id": "xe9omI65EXT6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL1_1CCfEXWQ"
      },
      "id": "YL1_1CCfEXWQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "52Wv6_QDCRi5"
      },
      "id": "52Wv6_QDCRi5"
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_MNIST_DATA_PIXELS = x_train\n",
        "ALL_MNIST_DATA_CLASSES = y_train\n",
        "\n",
        "# number of digits to overlay on canvas\n",
        "num_of_digits = 2\n",
        "\n",
        "# max digits to define the shape of prediction output\n",
        "MAX_DIGITS = 5\n",
        "\n",
        "\n",
        "# Sample Base Digits\n",
        "def get_sample_indices(dataset, size=5):\n",
        "  random_indices = np.random.choice(len(dataset), size=size, replace=False)\n",
        "  return random_indices\n",
        "\n",
        "# helper function to sample number of digits from master dataset\n",
        "def sample_base_digits(num_of_digits):\n",
        "    \"\"\"\n",
        "    Sample a specified number of digit images and their class labels from the master MNIST dataset.\n",
        "\n",
        "    Args:\n",
        "        num_of_digits (int): Number of digit samples to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sample_pixels, sample_values)\n",
        "            sample_pixels (np.ndarray): Array of digit images with shape (num_of_digits, 28, 28, 1).\n",
        "            sample_values (np.ndarray): Array of class labels with shape (num_of_digits, 1).\n",
        "    \"\"\"\n",
        "    sample_indices = get_sample_indices(ALL_MNIST_DATA_PIXELS, size=num_of_digits)\n",
        "    sample_pixels = ALL_MNIST_DATA_PIXELS[sample_indices]\n",
        "    sample_pixels = sample_pixels.reshape(-1,28,28,1)\n",
        "\n",
        "    sample_values = ALL_MNIST_DATA_CLASSES[sample_indices]\n",
        "    sample_values = sample_values.reshape(-1, 1)\n",
        "\n",
        "    # split the digits into pixels and class values\n",
        "    # reshape the data to expected values\n",
        "    # sample_pixels = sample.drop(\n",
        "    #     columns=[\"class\"]).to_numpy().reshape(-1, 28, 28, 1)\n",
        "    # sample_values = sample[\"class\"].values.reshape(-1, 1)\n",
        "    return sample_pixels, sample_values\n",
        "# Augment Digits\n",
        "\n",
        "\n",
        "def plot_before_after(before_image, after_image):\n",
        "    \"\"\"\n",
        "    Display two images side by side for visual comparison (e.g., before and after augmentation).\n",
        "\n",
        "    Args:\n",
        "        before_image (np.ndarray): The original image.\n",
        "        after_image (np.ndarray): The image after transformation or augmentation.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "    axs[0].imshow(before_image)\n",
        "    axs[1].imshow(after_image)\n",
        "\n",
        "    plt.axis(\"off\")  # Remove axes for better visualization\n",
        "    plt.show()\n",
        "# helper function to apply random augmentation to digits\n",
        "\n",
        "\n",
        "def augment_digits(digits, debug=False):\n",
        "    \"\"\"\n",
        "    Apply random augmentations (translation, zoom, rotation) to a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        digits (np.ndarray): Array of digit images to augment.\n",
        "        debug (bool, optional): If True, displays before/after images for each digit. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Augmented digit images as a numpy array.\n",
        "    \"\"\"\n",
        "    tensor_digits = tf.convert_to_tensor(digits)\n",
        "\n",
        "    # step 2: apply random augmentation\n",
        "    augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomTranslation(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomRotation(\n",
        "            factor=0.1, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "    ])\n",
        "    augmented_tensor_digits = augmentation(tensor_digits)\n",
        "\n",
        "    # if debug is true render before digits\n",
        "    if debug == True:\n",
        "        for translated_imgs in range(tensor_digits.shape[0]):\n",
        "            plot_before_after(\n",
        "                tensor_digits[translated_imgs], augmented_tensor_digits[translated_imgs])\n",
        "\n",
        "    # convert the tensor back to numpy to simplify use in map function\n",
        "    return augmented_tensor_digits.numpy()\n",
        "# Calculate Tight BBox\n",
        "# helper function to calculate bounding box for each instance and return it.\n",
        "# we are going to refactor the POC that we created ealier to use it with numpy arrays in the map function\n",
        "\n",
        "\n",
        "def calculate_bounding_box(pixels, class_value, padding=1):\n",
        "    \"\"\"\n",
        "    Calculate the tight bounding box for a digit image and return its coordinates and class value.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): 2D array representing the digit image.\n",
        "        class_value (int): The class label of the digit.\n",
        "        padding (int, optional): Padding to add around the bounding box. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        dict: Bounding box information including coordinates, center, width, height, and class value.\n",
        "    \"\"\"\n",
        "    # calculate active rows & columns\n",
        "    active_rows = np.sum(pixels, axis=1)\n",
        "    active_columns = np.sum(pixels, axis=0)\n",
        "\n",
        "    # calculate x_min and x_max coordinate\n",
        "    x_min = np.nonzero(active_columns)[0][0]\n",
        "    x_max = np.nonzero(active_columns)[0][-1]\n",
        "    y_min = np.nonzero(active_rows)[0][0]\n",
        "    y_max = np.nonzero(active_rows)[0][-1]\n",
        "\n",
        "    # add padding to pixels\n",
        "    x_min = x_min - (padding if (x_min != 0) else 0)\n",
        "    x_max = x_max + (padding if (x_max != 27) else 0)\n",
        "    y_min = y_min - (padding if (y_min != 0) else 0)\n",
        "    y_max = y_max + (padding if (y_max != 27) else 0)\n",
        "\n",
        "    # calcualte x_center and y_center\n",
        "    x_center = round((x_min + x_max) / 2)\n",
        "    y_center = round((y_min + y_max) / 2)\n",
        "\n",
        "    # calculate width and height\n",
        "    width = x_max - x_min + 1\n",
        "    height = y_max - y_min + 1\n",
        "\n",
        "    return {\n",
        "        \"x_min\": x_min,\n",
        "        \"x_max\": x_max,\n",
        "        \"y_min\": y_min,\n",
        "        \"y_max\": y_max,\n",
        "        \"x_center\": x_center,\n",
        "        \"y_center\": y_center,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"class_value\": class_value\n",
        "    }\n",
        "\n",
        "\n",
        "# helper function to visualize the bounding box\n",
        "\n",
        "\n",
        "def visualize_bounding_box(pixel_data, bounding_boxes, num_of_columns=5):\n",
        "    \"\"\"\n",
        "    Visualize digit images with their corresponding bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        pixel_data (np.ndarray): Array of digit images.\n",
        "        bounding_boxes (list): List of bounding box dictionaries for each digit.\n",
        "        num_of_columns (int, optional): Number of columns in the plot grid. Defaults to 5.\n",
        "    \"\"\"\n",
        "    num_of_columns = num_of_columns if num_of_columns <= 5 else 5\n",
        "    num_instances = pixel_data.shape[0]\n",
        "    num_of_rows = int(num_instances / num_of_columns) + \\\n",
        "        (1 if int(num_instances % num_of_columns) > 0 else 0)\n",
        "\n",
        "    fig, axs = plt.subplots(num_of_rows, num_of_columns, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for idx in range(0, num_instances, 1):\n",
        "\n",
        "        original = tf.constant(pixel_data[idx].reshape(28, 28, 1))\n",
        "        converted = tf.image.grayscale_to_rgb(original)\n",
        "        target_data = bounding_boxes[idx]\n",
        "        x_center = target_data[\"x_center\"]\n",
        "        y_center = target_data[\"y_center\"]\n",
        "        width = target_data[\"width\"]\n",
        "        height = target_data[\"height\"]\n",
        "\n",
        "        x = target_data[\"x_min\"]\n",
        "        y = target_data[\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        image_data = converted.numpy().astype(\"uint8\")\n",
        "        axs[idx].imshow(image_data)\n",
        "        axs[idx].add_patch(rect)\n",
        "\n",
        "        axs[idx].set_title(target_data[\"class_value\"])\n",
        "        axs[idx].axis(\"off\")\n",
        "    plt.show()\n",
        "# helper function to calculate bounding box for digits.\n",
        "\n",
        "\n",
        "def calculate_tight_bbox(pixels, class_values, debug=False):\n",
        "    \"\"\"\n",
        "    Calculate tight bounding boxes for a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_values (np.ndarray): Array of class labels for each digit.\n",
        "        debug (bool, optional): If True, visualizes the bounding boxes. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    class_with_bbox = []\n",
        "    for idx in range(pixels.shape[0]):\n",
        "        class_with_bbox.append(calculate_bounding_box(\n",
        "            pixels[idx], class_values[idx][0]))\n",
        "\n",
        "    # if debug true render digits with bbox\n",
        "    if debug == True:\n",
        "        visualize_bounding_box(pixels, class_with_bbox, pixels.shape[0])\n",
        "\n",
        "    return class_with_bbox\n",
        "# Create Blank Canvas\n",
        "# helper function to create  blank canvas\n",
        "\n",
        "\n",
        "def create_blank_canvas(shape=(100, 100, 1)):\n",
        "    \"\"\"\n",
        "    Create a blank canvas for placing digit images.\n",
        "\n",
        "    Args:\n",
        "        shape (tuple, optional): Shape of the canvas. Defaults to (100, 100, 1).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Blank canvas array.\n",
        "    \"\"\"\n",
        "    canvas = np.zeros(shape=(100, 100, 1), dtype=np.float32)\n",
        "    return canvas\n",
        "# Create Prediction Object\n",
        "# helper function to create empty predition structure based on MAX_DIGITS\n",
        "\n",
        "\n",
        "def create_prediction_object():\n",
        "    \"\"\"\n",
        "    Create an empty prediction object for storing digit detection results.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Prediction array of shape (MAX_DIGITS, 15).\n",
        "    \"\"\"\n",
        "    prediction = np.zeros(shape=(MAX_DIGITS, 15), dtype=np.float32)\n",
        "    return prediction\n",
        "# Place Digit On Canvas\n",
        "\n",
        "\n",
        "def is_valid_coordinates(top, left, class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Check if the proposed top-left coordinates for a digit's bounding box are valid (within canvas and non-overlapping).\n",
        "\n",
        "    Args:\n",
        "        top (int): Proposed top coordinate.\n",
        "        left (int): Proposed left coordinate.\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if coordinates are valid, False otherwise.\n",
        "    \"\"\"\n",
        "    # # make sure the top and left are withing the canvas\n",
        "    # if (top + 28 >= 100 or left + 28 >= 100):\n",
        "    #     return False\n",
        "    # read current class values\n",
        "    # curr_x_center = class_bbox_value[\"x_center\"]\n",
        "    # curr_y_center = class_bbox_value[\"y_center\"]\n",
        "    curr_width = class_bbox_value[\"width\"]\n",
        "    curr_height = class_bbox_value[\"height\"]\n",
        "    curr_x_min = left\n",
        "    curr_y_min = top\n",
        "    curr_x_max = left + curr_width\n",
        "    curr_y_max = top + curr_height\n",
        "\n",
        "    # recalculate center with proposed top and left values\n",
        "    # curr_x_center = curr_x_center + left\n",
        "    # curr_y_center = curr_y_center + top\n",
        "\n",
        "    # check 1: will the new bounding box go beyond the grid?\n",
        "    if ((curr_x_min + curr_width) >= 100) or ((curr_y_min + curr_height) >= 100):\n",
        "        return False\n",
        "\n",
        "    # check 2: do bounding boxes overlap\n",
        "    # check the current bounding box with every existing box\n",
        "    for coord_idx in range(len(existing_coordinates)):\n",
        "        existing_x_min = existing_coordinates[coord_idx][\"x_min\"]\n",
        "        existing_y_min = existing_coordinates[coord_idx][\"y_min\"]\n",
        "        existing_x_max = existing_coordinates[coord_idx][\"x_max\"]\n",
        "        existing_y_max = existing_coordinates[coord_idx][\"y_max\"]\n",
        "        if ((curr_x_min <= existing_x_max and curr_x_max >= existing_x_min) and (curr_y_min <= existing_y_max and curr_y_max >= existing_y_min)):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def select_top_left(class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Randomly select valid top-left coordinates for placing a digit on the canvas.\n",
        "\n",
        "    Args:\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (top, left) coordinates if valid, otherwise (-1, -1).\n",
        "    \"\"\"\n",
        "    got_valid_coordinates = False\n",
        "    # limiting the loop to run only 20 times\n",
        "    retries = 0\n",
        "    while ((not got_valid_coordinates) and (retries < 50)):\n",
        "        top = np.random.randint(0, high=100)\n",
        "        left = np.random.randint(0, high=100)\n",
        "        got_valid_coordinates = is_valid_coordinates(\n",
        "            top, left, class_bbox_value, existing_coordinates)\n",
        "        retries = retries+1\n",
        "\n",
        "    if got_valid_coordinates:\n",
        "        return top, left\n",
        "    return -1, -1\n",
        "\n",
        "\n",
        "# helper function to render the canvas\n",
        "# update the original plotting function to plot canvas as well.\n",
        "\n",
        "\n",
        "def render_canvas(canvas, class_bbox):\n",
        "    \"\"\"\n",
        "    Render the canvas with all placed digits and their bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The canvas image.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    num_of_digits = len(class_bbox)\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
        "    axs.imshow(canvas)  # Use 'gray' colormap to render grayscale\n",
        "    axs.axis(\"off\")\n",
        "\n",
        "    for idx in range(0, num_of_digits, 1):\n",
        "        width = class_bbox[idx][\"width\"]\n",
        "        height = class_bbox[idx][\"height\"]\n",
        "\n",
        "        x = class_bbox[idx][\"x_min\"]\n",
        "        y = class_bbox[idx][\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        axs.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def place_digit_on_canvas(canvas, pixels, class_bbox, debug=False):\n",
        "    \"\"\"\n",
        "    Place digit images on the canvas at valid, non-overlapping locations.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The blank canvas to place digits on.\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "        debug (bool, optional): If True, renders the canvas after placement. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, class_bbox) with updated canvas and bounding boxes.\n",
        "    \"\"\"\n",
        "    # list to save all the valid existing coordinates\n",
        "    existing_coordinates = []\n",
        "\n",
        "    # in case if the algorithm cannot place a digit on canvas we'll drop that digit from pixel and class_bbox\n",
        "    digits_to_drop = []\n",
        "\n",
        "    total_digits = pixels.shape[0]\n",
        "    # loop thru all the pixel values\n",
        "    for idx in range(total_digits):\n",
        "        class_bbox_value = class_bbox[idx]\n",
        "        x_center = class_bbox_value[\"x_center\"]\n",
        "        y_center = class_bbox_value[\"y_center\"]\n",
        "        width = class_bbox_value[\"width\"]\n",
        "        height = class_bbox_value[\"height\"]\n",
        "        class_value = class_bbox_value[\"class_value\"]\n",
        "        x_min = class_bbox_value[\"x_min\"]\n",
        "        y_min = class_bbox_value[\"y_min\"]\n",
        "        x_max = class_bbox_value[\"x_max\"]\n",
        "        y_max = class_bbox_value[\"y_max\"]\n",
        "\n",
        "        # print(f\"Width {width} & {int(width)}, Height {height} & {int(height)}\")\n",
        "        # width = int(width)\n",
        "        # height = int(height)\n",
        "\n",
        "        # step 1: find the right coordinates to place the digit\n",
        "        top, left = select_top_left(class_bbox_value, existing_coordinates)\n",
        "        if top != -1 and left != -1:\n",
        "            # step 2: place the digit\n",
        "            # canvas[y_min + top:y_min + top+height, x_min + left:x_min +\n",
        "            #        left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            canvas[top:top+height, left:\n",
        "                   left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            # step 3: recalculate the center based on top,left and update the class values with new center\n",
        "            class_bbox_value[\"x_center\"] = x_center + left\n",
        "            class_bbox_value[\"x_min\"] = left\n",
        "            class_bbox_value[\"x_max\"] = left + width\n",
        "\n",
        "            class_bbox_value[\"y_center\"] = y_center + top\n",
        "            class_bbox_value[\"y_min\"] = top\n",
        "            class_bbox_value[\"y_max\"] = top + height\n",
        "\n",
        "            # update the array\n",
        "            class_bbox[idx] = class_bbox_value\n",
        "            # step 5: save the existing bounding box coordinates to help select the new one\n",
        "            existing_coordinates.append(\n",
        "                class_bbox_value\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Error placing digit {class_value} on canvas. Couldn't fild valid coordinates\")\n",
        "            digits_to_drop.append(idx)\n",
        "\n",
        "    # drop any bbox for which we couldn't find space in canvas\n",
        "    filtered_bbox = [bbox for idx, bbox in enumerate(\n",
        "        class_bbox) if idx not in digits_to_drop]\n",
        "\n",
        "    if debug == True:\n",
        "        render_canvas(canvas=canvas, class_bbox=filtered_bbox)\n",
        "\n",
        "    return canvas, class_bbox\n",
        "# Translate BBox To Prediction Object\n",
        "# helper function to convert bbox diction to prediction object\n",
        "\n",
        "\n",
        "def translate_bbox_to_prediction(current_bbox, prediction, debug=False):\n",
        "    \"\"\"\n",
        "    Convert bounding box dictionaries to a prediction object suitable for training.\n",
        "\n",
        "    Args:\n",
        "        current_bbox (list): List of bounding box dictionaries.\n",
        "        prediction (np.ndarray): Prediction array to update.\n",
        "        debug (bool, optional): If True, prints mapping details. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Updated prediction array.\n",
        "    \"\"\"\n",
        "    # Sanity check - ideally prediction shape should be larger than or equal to number of elements in bbox\n",
        "    if (prediction.shape[0] < len(current_bbox)):\n",
        "        print(f\"Error shape mismatch between prediction and bbox\")\n",
        "        return prediction\n",
        "\n",
        "    for idx, bbox in enumerate(current_bbox):\n",
        "        # set the flag indicating the digit is present\n",
        "        prediction[idx][0] = 1\n",
        "        # set x_center\n",
        "        prediction[idx][1] = bbox[\"x_center\"]\n",
        "        # set y_center\n",
        "        prediction[idx][2] = bbox[\"y_center\"]\n",
        "        # set width\n",
        "        prediction[idx][3] = bbox[\"width\"]\n",
        "        # set height\n",
        "        prediction[idx][4] = bbox[\"height\"]\n",
        "        # set one hot encoded value of the class\n",
        "        # read the class value\n",
        "        class_value = int(bbox[\"class_value\"])\n",
        "        # set the cell corresponding to class value to 1\n",
        "        prediction[idx][5 + class_value] = 1\n",
        "        if debug == True:\n",
        "            print(f\"current bbox is {bbox}\")\n",
        "            print(f\"mapped prediction is {prediction[idx]}\")\n",
        "\n",
        "    return prediction\n",
        "# Generate Training Example\n",
        "# helper map function to map 28x28x1 image and its class to 100x100x1 canvas and prediction object\n",
        "\n",
        "\n",
        "def generate_training_example(x, y, debug=False):\n",
        "    \"\"\"\n",
        "    Generate a training example by placing digits on a canvas and creating the corresponding prediction object.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Input digit image(s).\n",
        "        y (np.ndarray): Corresponding class label(s).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, prediction) where canvas is the composed image and prediction is the label array.\n",
        "    \"\"\"\n",
        "    pixels = x.reshape(-1, 28, 28, 1)\n",
        "    class_values = y.reshape(-1, 1)\n",
        "    # step 1: sample additional digits\n",
        "    if num_of_digits - 1 > 0:\n",
        "        sample_pixels, sample_values = sample_base_digits(num_of_digits - 1)\n",
        "        pixels = np.concatenate((pixels, sample_pixels))\n",
        "        class_values = np.concatenate((class_values, sample_values), axis=0)\n",
        "\n",
        "    # step 2: augment digits\n",
        "    pixels = augment_digits(pixels, debug=debug)\n",
        "\n",
        "    # step 3: calculate bounding box\n",
        "    class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n",
        "\n",
        "    # step 4: create blank canvas and prediction\n",
        "    canvas = create_blank_canvas()\n",
        "    prediction = create_prediction_object()\n",
        "\n",
        "    # step 5: place digit on canvas\n",
        "    canvas, class_bbox = place_digit_on_canvas(\n",
        "        canvas, pixels, class_with_bbox, debug=debug)\n",
        "\n",
        "    # step 6: translate bbox to prediction object\n",
        "    prediction = translate_bbox_to_prediction(\n",
        "        class_bbox, prediction, debug=debug)\n",
        "\n",
        "    # print(f\"Final canvas shape {canvas.shape}, final prediction shape {prediction.shape}\")\n",
        "    return (canvas, prediction)\n"
      ],
      "metadata": {
        "id": "ZzOBbarPERb0"
      },
      "id": "ZzOBbarPERb0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = get_sample_indices(x_train, 1)\n",
        "test_x = x_train[sample_indices]\n",
        "test_y = y_train[sample_indices]\n",
        "canvas, prediction = generate_training_example(test_x,test_y,debug=True)"
      ],
      "metadata": {
        "id": "noLqDnfCEtSQ",
        "outputId": "a9dc5aa1-ad7d-47d2-ed57-023afb2b058c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "noLqDnfCEtSQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJx1JREFUeJzt3WuPXVd64Pf/sy57n/upKlaxeJEoqdU3t9rd7YnbThDMDCbBxAFmgJkXM5/An8KfYD7FvA8wbzODGJMgMTyxkY7d8Vg23K3WhRJJUaxi3c5139ZaebFPFSmBaokSJZ6q8/wASqKqWDwi9K+9z9rrIimlhFLqSjMv+wUopb55GrpSG0BDV2oDaOhKbQANXakNoKErtQE0dKU2gIau1AbQ0JXaAO7LfuI/N//2m3wdSqmv6D/H//CFn6NXdKU2gIau1AbQ0JXaABq6UhtAQ1dqA2joSm0ADV2pDaChK7UBNHSlNoCGrtQG0NCV2gAaulIbQENXagNo6EptAA1dqQ2goSu1ATR0pTaAhq7UBtDQldoAGrpSG0BDV2oDaOhKbQANXakNoKErtQE0dKU2gIau1AbQ0JXaABq6UhtAQ1dqA2joSm0ADV2pDaChK7UBNHSlNoCGrtQG0NCV2gAaulIbQENXagNo6EptAA1dqQ2goSu1ATR0pTaAhq7UBtDQldoAGrpSG0BDV2oDaOhKbQANXakNoKErtQE0dKU2gIau1AbQ0JXaABq6UhtAQ1dqA2joSm0ADV2pDaChK7UBNHSlNoCGrtQG0NCV2gAaulIbQENXagNo6EptAA1dqQ2goSu1ATR0pTaAhq7UBtDQldoAGrpSG0BDV2oDaOhKbQANXakNoKErtQE0dKU2gIau1AbQ0JXaABq6UhtAQ1dqA2joSm0ADV2pDaChK7UBNHSlNoCGrtQGcC/7BXyT9tKCMeXLfhnfujNyDqX3sl+GWiNXNvS9tODf86d0CS/7pXzrllj+OP2Rxq4uXNnQx5R0Cfw7/oCPGL7sl/OtucOUP+EXjCk5RENXrSsb+rmPGPKubL/sl/HtSS/7Bah1pINxSm0ADV2pDXDlb92/Frn4yxdIesus1pqG/nmsRUTAGBABkfbnKbVNp1XZKUGMpBjbf05avFo/GvqzGAFnSUYwTjBWEGn/tXAeuxAj7d8bSBVt5CFq7GrtaOgCGIsYA8Yg3hP6junvjEhjx2u3D7lx7ZR9t+Q72QQn4DGEZHiv7vKoyXj00TYfvXMdmdbkH54iZU2qG4jxZf/XKQVseujSvv8WYxBrwXuk2yFt5yx+vEtzy9P/8TGvvLHgh/kp/333IR2BXDxNsvz5cptf1z2Kv96jcvvIo4L8cYFEIYWooau1sZmhn7/n7nbAO8rbjvqGo98J7I4K3Kjg9d+Z4ncSP9r5hNfyCbdcQSYGK+1NgEiiZ0q2bSLrl5S7EVtByh2yDIgxOj6n1sbGhi7OIdtj6HdY/OOMsz/0dIan3N4/Yieb84+GH3HNL9jzsOXAIWRiEKAdjots2Tk3WdDbXjB/I+ANbA0yKICyaL+h6Pt1tQY2L/TV+/CUOcI4I40zejsN+U7J/mDOjfGCbbfkWqdiy9b0jSEXg2E1ELeSgCYZ6mQJjcGUglRAaEfh9XKu1snmhH5+u55nmNGIMMw4/dmY+obn99/6DT9+7UNuZwt+2j2lawIjE/DicQLmU4m3UhKOwoCP6h7zowGD9yzmYYPMSlJRQgh6NVdrY7NCNwa8I/U9aeQxu2D3EztbS+70TrnpCvb9ghxw8qy8n0hASJY6WVIw+CohdUJi4vzmXql1sTGhS7eLDHpUtzMmfzigt1PzP/30v3J794zvXTvjtXxOXwK5GCz81sgBrCRe9ad0zITFKz3e+4NdmnsGDsbExwE5OIH5khRCe3VX6iXanNBzjwwGNPueyU87ZLsNP/3+h/x0/IBt4xgb146mf6kpr2BIXLMLuqbhg2sTejKlcl3K3S1SAJkukLJuJ9do6Oolu/qhGwPGkjIPvQzbEwbdin6nJDMRx7NX9qTV7Xf6zG34098G2lF4x3fyGf90dJeTG33e/XnD4ihjst2hOMzxxwX+8RKaQCqr1UCd3tirb9eVD12MQZwjdTLisIMd1oyHC7YGS7oukEnCPuMiHleJh5SIn3nHff7uvSOGnjh+0j3lO/lj7vXH/K/jxKP5kOObdzi7N2b4myl5nMCyJDZnQPNkXrxS35IrHzp2Na2156iHlt6g4Eb3lL18ysAGvBjM6jrdAHVKVMlyWA0oo6UqDaEyEMEESAZSDsYm9rsLRr4CIh1JjF3Dq70ZfUmwf8guS4qJYXmQI3MwVQfqBqpqNSc+QtTg1Tfvyocu3S4yHFHc6jH9fsb+6wf8ixt/y/XBnDfyki2TX3zuIiYehcRh3eU/Hf2Qj4stZveHLA672AL8HEInUdyK+H7Nv3zjv/LT3Xtct/CKhVd8zb8a3aMeGI5+/mvmlfCft3/MfzS/hz1pGOYdzLJBTidQVKSq/aFXd/VNu7qhi7TLxL0l9jxmCH6rpj8s2c9n7PkFXWOwCDXQJJg3luPScFh2+Ph0wIPFkMnBkMUnPWyR8DOIHVi4iB/WPLzW51a/QzeviDZgJbFjS5KF7rBiGQN71+YM9hrERfyZQRaWpvYkSZAC1Kv3DRq7+gZdzdCdw+R9mMHy9SFnb27zxs8+4fWf/Yo3hqe81qkYWcilXWZ6v865H3I+fHiN/+fv32B+6jn9B48/iwwPH9M5DUiTMHUi5ZbB9T5mYHj74zd4/84r/Py1D7Hf/xVDG7nlEh7oGUculv/hzQd8d/xnHCxz/vZwzGSacfeXe0wfXSP/8JT83oTU1O0kG41dfUOuZOhiDZJnMIN6u8PydpfRnSU/eP0+t33Bjo90DVjaiS8n0fNR3eGd021++e6bNI+F7f/vmOy4JD+b42eL9v10SpB57LGQejkP+zvMmz47vRm/+x1DlMR+Ai8Jj8ELfHf3jO/unvF+PeBgehszGcDZbUo/xE0q5GAJJJKUOm1WfWOuZOjJmPZxGhAGUI0jvW7BDTdh24ZPTWuNSbj/YI+/vnebk7t9Or+eEc8icjwlTs/fQz+13DRE4nwOdUnnA4OdVRyGAX9mf8LeeMHyO58w7lTsuyVD02AQDMLYVPysc8SEKdtvNRzf7PHB1ogPr2/hjht67/eQooHJHOpad6tRL9SVDB1roNP+pzUDQ7WT6PULbrszRlbw4i9G2lMU7t69wV/84sdk9xcM3z7GLCri6YRY16sr+VNfOwTSdAZG6L2TkHtLHk1GvD+/w41XTgjXGq6bCf+NlHSkwYnFYNm2Ff9t94CqA9/7vfucNYb/8MrP+as3X6N/N9KJNfasQlYbViSdK69eoKsZ+mdJu37cSFrNfmu1i00TMQqxMaQgSEjtI6/zK+rntZYSqa7bhTKnBfZhQS2RTz7appxl7O83pIFhbBu2TYMIWAEPDGxCJLHTL9jbmmK2LfVuRrAef9LBJJCi1BF59cJsRuifIxKJACFhGjBN+vK3zAnSckkqCvzdCn+0JOzm/L/lD8h2E2f/5B1uvX7Ej7JDfrdzQIahJ55MhH0LjRHeunbKgf+Ag2yLd6Z3SMeenWoHe1SRHp8gp4GUdKca9fVtXOjn01oDwix6lsFSBYNp0upqHp9c0b/IaqWalA0iJY23LA8sdRCOj3r4rZLj4YyJ93QN5Lb9A3e0G02Ofc2N7ow08DzaWdCIRXaEmDwsPSw8hAZqfb+uvp4rG3o6fzxtoL0XXv2cRJUCi5jxf05f5V4x5DdHW3SOAvasWU1kqZ9rmmpqGtIiIqGm+8sK6Ts+rrY5fGWPxe8OOfpZl5vZnN/vPqJvarrisQhv5afcdEtO/MfcG7zL43mP/+P2D3l4OiL/qzHZr4bI2Rx5dKyr4NTXcmVDPy9bJH1qJUpKiUikiMJ71Yh3ljucLTu4ZcSUEZrw/JtGxPb2WkLAFzUp90y3twmLMflegS3mpCTMs0c4Et60V/RdW7JrC2Y+steveFCO+C/mNerJCPcwJz1yECLy2CAxkUTnyKuv5kqGnpyhGbT/aePtJa/eOGJvuGQo4AViSpRROJgNuD/Zwkw9dl4jRd1eOWPkKz3UTqm9E6gb7P0j5GzGYj7l4Tsw3dvi8Y9/n/4w8MYrJ4wHBa/6KbfdDAvsGE/08JPxI0a+5MHtGxye7JJbT++THJGaVOhIvPpqrmTo0QlNv32OPt5e8sr+EbvDJQPTPjsPRMoIB7MBD8622JkFxosaiobUhNWCk6/4m6fUhv7gCCuw/DUsBco3tvgvszfJ9xM/z3/NTXvMf0fNnjvFY9k2Hufgx6MDtjsTprdGvDd3DMqMXjdHkkBVgo7Lqa/gSoYuMWHKBoDl3HNyNmCWe8rh+SMuQ99EfjB8hKSG2XiLxXCE1B7rV38k8QVcPc9/eRJkHsg/nOPmhpPrfVJleG+3pnstsO1qXvNLEg17rsCkyGvbJ5zcOiBMI/UrXZg47P0SKWrdj049tysZuqkj2aQG4OhgwHsf7fMw+5DTa4m+EfrGsucD//rm25xcc/xv7/6cP793HW8beo+7SFkRQyC9wNlp2XGB/4tPSMOMj+o93r/d5fFbY35lt/lh5zHD0W/omZIf+IboLXznA7ZunHJ3tM8vq++RDtq7Dnu8JJVFexKMUl/SlQydRPuoDAjBUDeOJhjO33kbEYTEtq9w1HTzQMwMyZt2Vp35Bk6TDgk7r4kI4QiazDA9yzlYDNmXOUUSMqAvEUEY+JJRd0Gn1xB7ltTlyWuTL7fdlVLnrmboPHm89iyC4ETYNcLQGbodQz0QTM9C7tu92Z8O6kVc1WMglgXEhsE/HJPuL5j4jIPubXrXGw7GGUEiXWvJReibim07o+cLYicRc0hOECMkMXo4hHouVzb0Z51rni4+cr4VlOCMwTuQPCJZIjmLOPsk9BcVU2I1yNfgTgpkmZgcdTg963PW6zJtHF3nCAYQwUmkYxqcCSQHybL6xqNXc/X8rmzo8vSweYKIEJMQP/0RRBLfe/0B/2MWOLw74r20Tzzt0n03YacVabEkleWLW02WUjt1tq7JHvUZvQ+TZsT/fust9vtT/uetu9zK5tSpfWqQ5PzHxaFvX/81qI1zZUM/r1lWf08JAkL6zD29kcRrtx4Rrp/w9vB1/v70derHlvw04qRsr8L1882U++2vK5GKkiQl/nHN4D5M/YC/OHqTm80pv9f/mLE/o376OdrFhVxAL+rqK7iaoafUBgqYAtxMOFqM+IflTW64krGZ0JWIE4sAYxO4ZWsW4zN+8p27TLdyDmc9pscd/IOAO3RIWSGzAlJsj0ROT30HeR4X57ELKTM0HfB5opOV9H1FZhIOGJsGoeT2YMbrtw8osEgfOMugKLmIXt+nqy/hSoYuISFVOy/cTxP5oeHu7nX+9BR+2D3klezv2LI1QxE8hpuuYd81XL/xkK3xMfdnY/6X3T/kk+Mttt/uM/wgYo+mmAdH7RTZ1f7sKa7mnn/Z2M7Pf/MecZbQc1RDQ2cQ2OvN2e0s6NlILsItW3HLNqTrBxx13+F0q8cHf3aT+VkXlsv2xFZtXH1JVzL0lCI07XNmO6twRwXhQJhv9zgZ9bnfGTH3Ja9mNQObMCQ8Qs81XOsk6pRxZ2eCs478FmSNEPoVjRNibaiXGTEkpEhQJ0yTMPVqNt3nRW+E5AzJGuLQQ27xe4Hx7ozt8ZSb+ZTrfk5HIlaEkIQAVMmwCJ4i+PPFcko9tysZOlVNOJsA0Pn7R2x9KJh3+sz/Zodfv9bnwT8bcW0859/svc2bnSMGxtEVy1AiP8jgDTvhez/4a5aN4703uzycZzxaDHj/dJt5lXN6OqQsHf6+wx1b8uOG3qMKQkKaiDwj9tCx1Fs5Tc8w+R402/DWm5/ww9fe55X+lH+08wlDV3PD1XQk4+MgHEbD353u8Od3v0993zCcLLFF0d5V6C27eg5XM/TVfHMAOynxiznJdGiip8wMj88ME5tzNM7Y8waHkNv24MQhEVzk2vCYgBB7fVLdoS7h0XxErBL22GIKhzMZruvIMkPegISIqcNTV90nMYauQ65lmIHB3kmE3cTodsnNG6fcclPudM7oSsTjSckwj46T2nJcdHk8GZCm0K2W2CY82YhCY1df0tUM/SkpRlLTtIcemhOi98S/GTDZyfiP5U/4y50pf7B1n58OH9GTxLZpj2gyGAxwx5WMTcP3XcPv5XPqYJmOOoTGYK8bpDC4ZcTPA0SQ2N7Cf/bMtuQMoWNJXih3EqEDNwYTrnenDExNTwxNsnwYukyC5y8fvcrbx9c5vT9i8PcWOWwwp9Xq7PVGI1fP5cqH3h5qGGBRYJIgWZ/0fs7itMtfbb+OpIq+X3Kr94gtA0PThtpOk4VdW7Nra/AFcPrpr33r83/bz4b+LOcTd6B9zl8ny8Mm45O6w18f3+QvH3yXwT249kHCnkbMrIaqakf9lXoOVz/0cyG0E1/OIPvgkHhgcFWArchHb23zf33vR9zqzSh2HtO3Dbu2IpcGK+1pLvDpI5WffpT9+Uctp2f+7PxbwDQa5slwWne5u9xhVmZ88GjI2Txj8esOWx/OyA8D9l6NLCooSt0dVn0lGxN6qmtSUyPFkvzxGRhB/iaH3PH2P7/JX1Q/4nf2P6bq/i272YK3skPGtqaDxWMwItinDlhOcLFlNLR3AE9/7FmX8/PA28H5xGEUHjSOX82u8Z8Of8xs0qP62yHxsaP79in77x61cU/m7f50MbYr6pR6ThsTOtCuakur7ZxFEAkQQB4HuBcpC8dhPqbOcno5DG2N7VpMZshcpJsHjEScRIwkzqefe4lkErEkcmkn2Z5HXSTLMjqqZJg1GSEKTWEJjeHR0nJYWB5Ph5QHjmYqxPsN6STCSYlZVKSqeTJBRyNXX9Fmhf60lNp90xth+Ddn9D+qCWPHL2/9CJPB/91N4GH+mmF5Q9jZmfHKrSNyV7PtF3gT2gMaTGDbLti1U/qm4VVXr2IHEN6vu/ymHnFYDvm76S0Wy5zJ+2Pqswz3ScQ/itSziHsU6BdL4uERadlglhWhWA266XbP6mva3NBhNVU2Yac1vilJC8MkdEjeErqW4IWJF+YYpkFInYKuryiyRCYNHdvgTaB2AbKK0lpGWViF3qZ+XOYcLjt8Una5d9Jntuhw9mhEdZzT/zjQfxgw8xJ/MMVWDXJWQFW3s+70Cq5ekM0OfSVVZbujTFnAfI4YwVrBGGF4F7o9odOpWAxKShNZGsGIw2U51hn8+Dr5Tobpgr0O4p987erjSHU/sVxCc1zgqpLh6ZxQGtw8YpYJqQOxqNv34FW9elKgkasXR0MHaEK7KWQJMlsAT0bVu099WrkacFsAiMF0ciTzxN0x4eYO1dAyfd0TcnMx7DZ6d87oVzNYFMSjE0xoyM9Hzp+KWbNW3yQN/Xl85iqbmoaUIul0isSA6xi6Z5boWK0sA/e4Ip2W7ZU66MCaejk09K/qfDAPoCiQo1OsQP+zj9RTu4+8XrLVy6ShvwirB+eiMas19Q1sd6qUWjcaulIbQENXagNo6EptAA1dqQ1w5Ufd7zDdqEdbd5i+7Jeg1tCVDf2MnCWWP+EXL/ulfOuWWM7IX/bLUGvkyoZ+KD3+OP0RY8qX/VK+dWfkHErvZb8MtUaubOjQxn6I/g+vlA7GKbUBNHSlNoCGrtQG0NCV2gAaulIbQENXagNo6EptAA1dqQ2goSu1ATR0pTaAhq7UBtDQldoAl2JRy15afPOr0ES+lb3WdWWZehnWPvS9tODf86d0Cd/sb/QtbU6xxPLH6Y80dvWtWvvQx5R0Cfw7/oCPGH61LyKy+gEiAmLaY5OtBdP+XKxpzz67OElldRZ5ZHWlj6SYvtYpK3eY8if8gjGlLp9V36q1D/3cRwx5V7af7xddBC5gBBEDxoBbBe4cYgxYA8auQl8dVRxCe456XB1bHBPp/My08BVPOt2gLa3Uerk0oT8XaxHv24i7OThHyhxk7QGI1VZGckKypj0jbfXNIBlIJrZBhoTEhFsmbBkxywY3LaFuSLM5NE8dlJgS6fwMcz3LXK2hqxe6CCKCeIdYB70+5B46GambEQeWaj8neiEZeXJsKhAdxAxIIKH9kU8jzCNMK7AFUlRQ1UDdXu1Te0sv0N7qK7WGrkbo1rZX78xDnpM6jnq7S8oszXZO7FriwBAHBulF7O4CY4UyOEJavXcHMt/Q75XEJEyXHZrGEKeGYiHYhcPt5UjpsMcgVcSUEVMnTNFg5zVSN6T5HELUK7taK5c/dAFxDnEORgPYGhGGnuLVPk3XsLwuNH0hbDeErYZup+ba1hxIzJddqsau7twT/c6S6+NTmmiYn+5Qlxn1PKNZOkzpsEuPqSA7GWIryKbtrX12UuIOC1gU7dU+1e3VXa/wak1c7tBF2sG1bk7KM8JWTtjzhKGj2UvETsTsJFwX7LAh9RucD9QIKRlCMMRgkdVJqLV4lrMOTTSEhSPVltS09/bJQehAshBqIVXtXUDMBAkWu/QIDSZzF7f0hG/4kaBSX9LlDV2kvYo7R9zfJu4Mmb/imb3hSYOI3KyRPNDvV3gf8L6NvKgdJ8seTW1pJhmxskgDEoRJ6lCkASkKobYQBcnAeEg+EQehHYDPDQTBViC10Bl6Qu5wp4beZIGxFXEa2lF6pdbA5Q0d2ufh1hB7njDKSVuCXEvQjzAOiA+QBcRGjER8jNQNxKUlVhbmBqkEU4M0kKKhrs3FYJwFUhckhxQT0aSLs9ARiE4QgZgZYpZI3oC17eM6kS948Up9ey5v6MYgmSf1PLM7nuV3HNdfP+H7331MYSyPpUcVLfNJl2nhyGeQzyAVQu/UQi3Ys4CpAlIGTB3bR2pNvPj6iBBzS8ykXRVgE9EJTdcQHTRdIWTg5wlbJkydnhqI0/fnan1c3tBFEGeJ3lFtW5Y3DNmNgls3j5iHnOXMIlXOWeFYTDuEx0I8EmwBnZOIqSLZJGGqiClrTBnaQJvQXo2dJRkhZY7k7cVvGzNDuZ0IuVBuAz3BlAkTQMLqvXlM2rlaK5cvdCOIsaRuTrM3Io4y7F4iu7bE9RoAmtIyPRiwXGTk7xv8ccAf1fijClNF3CwgTcQsmvYKXjfEJiCJNlKhvQUXAWcQZ5+MoPc8DLchNyRpB+ckJmwRsEWEqiE1TRu8UmviEobeznpLvQ7NjS3idoa9UZBfX+IG7VB4UzrOPhlSnObs/KqmexCwR3Ps0RSaQKqqdkprXbfTXmN7u51g9d5a2ufyQvsWQWTVeULGXbg9AvHtTLpV6G4ZkaKBuoG6bufFK7UmLl3oYg10clLXU48scWwgTzgbKBrPo+mI2aSHPRGyk4g7LTFnNTIvoKhIMbQxPj1nPcbPPPNezXFfLV1NqymyYgzJGeq+EIYg2w1uGJE5hEywTgfg1Hq6dKGT58jWmLibMbvjCTuG/jjS8Q2P50M+PNjFHRgG7xj6Jw353VPs0ZxUloSyuliZdjF6/tkL7yr4i38t7V8k80iWEfuexW1DvS/0X1nQ3SnBdKgPe8Tacj59Xql1cvlCt4bU8aSOI/Uh9hPiE0YioTEUi4x8DmYWcfMGWdZQVu1VPASeGfdvc/E4TcA7yCyhA6GTkG7EdRtCHgleECf6WE2tpUsXeug6wvUezb7BvlYgO5FsWJHbQGcOnXuO/KAhezjDTWqYLUll2b4P/xpTUlMnJ+5uEa95whbEUSDrVvR9yTLLKbsCuQFnngzkKbUmLt2ecckbQq9dhWbGDXZc47IGayK2SripwU4Sdlph5iVUdTtD7esuMnG2nWrby0g5pDxiXcSbiHGJ6Nppsu2KONHbd7VWLs8V3RrEelLmCF3BdgN7gxmuX9FxDc5EYuixnCT8NMB8SZpXT9aNvzAJMQkMlMExr7N2YcxqjoyslskmvaKrNXJpQpfzx2qZo+kYfDey15vR6Ze077uFMtTks4iZNW3oy6p9pv0irFajrcbmQKCKlkXtCY2FCJKe7Gaj1Dq5NKFfSO0MtBSEZZPR1AlnIoZEwBCtIFaePP9+UVfWEJGyRqqEMYKxgkh7pyARbAOmWT2W0+Wpas1cutAlJmwNoTI8XvYxnYy+r8hNQyGOkAniV/vCvcBBMakbZLLAbDm8aQfejGmDNk2C1ZZTNO0Gk7rbjFonlyf081vnuFp4UkPVWEyT6NimnZcushoUk3YFmbXtVf3r/t5PbzBpBGsjyYIxsf1QStAkpDnfSPLrjfAr9aJdmtBTjMSqwixq8tOaqitMph2SD/RshfGJ2ElUY4GlhUEPSZZUldA0Xy28iy2hDfQ6xO0BsgXDfk3qRqyJGEmkxsG0QWY1LAtiWba38EqtiUsTevt4LCJ1wJQRKQ11aYm1kJJgJIGFkEHMBTIHPraRQjuA9jztXVzFDWItyTtix0MeyXwB7smmEiklqCPUkdQEJOiCFrVeLk/o0N66lzXmeIrJM8zxAKyhMw70XclJv6baa5AoVHs9XOYxiznSBAjhyZbMT1/dU7qIut3jvd3zXbIMbLtKLnUyyv0OizdzsusVe8MSl9UczgdMyw6dpaV3vp795fzJKPVbXa7QAaq6HRTrReRshMkNeYj0bIXt1TRbEVMJzXYHxJEdZsiiJCEIzWoGbKL9h/PIDWLkyX7w1kK/hzhLGPdJvZzqpmP6WkZ/J9DrV+S+4mEzYrLoQJnoN6tNJ/SOXa2hyxd6jNA0SBHITyLRGSZnXeJgzCJk5J0aMzQsb3rswNA0feyOQ4qAKQMSUnvrHxNSrfZmt7bdaMJbQi8DbwjDnOQtzU5GGFjMrYa92wuGoyX73TMyX/NR2CEVFqoATfu1tXO1ji5f6CGSygo7s/QfBMLC8uj6mHv5AO8Cg9GS2nsmyZMKR76/i5uDnyayWcQWkfy0gSpgzhZI1ZC6GSnzxJ6j2s5Xu8dYQi4U1xPVVuLm3pw3X3/IVrbge4NPcCnyq+o2ceJhHjFVA3V4sqZdR93VGrl0oaeUkBCgDthFA5mFiSGeCbELsd++D7fdBnFCHsD3E9I1xK6BwlA5i1SC9RlSWVLXkXJH6FnqLSFlAtsRkye62w3dccPOaMb17oSRLxjZJRLBx4g0Aue37VFv3dV6unShE9qrpiwM2ccT0llG0xtiTnKqPc/kRk7Wqbm2NyWzDTuvL+lSczQf8HjWpywcxWlGqi127qGB1E3E7mq5a7/GuchgWJD7htujE3b7M+50jnhr+IBcGrqmpqw9o7LGTwU7j8iybJfD6pVcraHLFzrtM3VpAmZZkxDsacRnQp0bwsiACXjbkOU1nU5J11XYLCPlHUIhlA5SI5i+QRoh9iIxjxgfcZ0ELiL9ButqeoOCre6cbT9n181wEpHV+nQXI1JLO1GmCe1VXak1dPlCP98BpmlIsxmUnvyBwc8a3Mzj5zkycMxmY+Z5ZDoeYPKGUiyVsUSBvF9BFFLHkGK7QiWmdgZdWJ3Octr0mZpItXTcz7aZjzzX/QQrkSo5llXOYtnBL8AuAqlol8TqrDi1ji5f6HCx31tatnH5owwpwDQ9TMiph5aZeJoO1FUg9BK+W+O7DSIJ32lXtIVoSEmIpUUqB0mIASKWpmr/aGZVhrGRkV1wst3FmsQ85iyanKry2CIhVYS6/uoz8JT6hl3O0OHiTHIhkYoCUsKYiJeImRqo27PQ6xND7IL0DfQ9YlbryRPYCoiCrw1xdcba+Yi51G2wzbYlDCx1nnPW9EkCh9WQRZGzXGa4gvbghotDFTV0tX4ub+gAMbTbp8/mpEWBWS7Jz5bgHd2POyTvCFsdYsdTjSz10LZbNBuQBH7eHnss7exaYPWxKLhlO7lm8oZjccNS5D0OqhGVWN6f7zJfdpjNuvh5bFetnS9P1c7VGrrcoa+kmBDabZyTqZAQEBGwdTvVvWxwwZFqRxLaDbRSwi5WoafUhi60V/SYMEU7l90sDVI7CEKTLFVylMFRNo4YBQmsdpfRdehqfV2J0IlhtYtzhGp1nvF02s5fP8mw1mJzT9bxq18g7ZbPZb0aKZcnO8dAe1WOiWQEe20fv52TCsMyeJZ4plWHeZVjK9N+o2jayDVzta6uRujQxvmpY5BWq8tWU1ylie1BiudiItVVG7oInyr9/MpsDaYKq8MThZAMAUNIQoyCjefnra1u2fWKrtbU1Qn9c1zsABtCuzb94gO0xzCl9OnIWW1KYx0YwS0j+VmEubBoMmpr6Pkam0OMGalymCqQmtXdgcau1tCVD/18m+cUwhd84hNJBJH2YEVTgVsmKIUqOqKBzAbEQpUioUkQEuniuGSl1s/VD/0rOx9Bb6/QVbKcVR1SbHd/jckSnCHkBuvbzSmAJ2velVojGvrneWoUPRmhCJ6q6CFZaneziULjLbFrkXy1EeX5+na9fVdr5tKd1PLtSkQL0QE2YU3Crua6C+2RycFDOj9DfXVVV2rd6BX9WVZHKacoBA91z+C6iZ6vwCWaaIkGYgfqgcH1HNLtgKnaAb+gV3S1XjT0z3GRqmG1tXPCmwCrvdyjAVwi+vY8OJxFGj1cUa0nDf0LRCvEDPK8Zr8zRVykSZbaOubDEeUWZCNHGvTavedmc1J8+gx2pV4+Df23WZ2xFo3gbLvTrDkPPQVsHggdIXYMZB6qZnUUlFmd1KKlq/WgoT+LCOI9eIuIwYSEiW33uWnYs1OiFe6O94jLmvRJInmDuHZsUyfDqnWjoT+LCOIcZB4R0y5cSQkhkZuaW9kJJiZ64wWxqYmjSPIWrGkj16u5WjP6eO3zpKfmsAMxGepkaaJFoH2W/tti1s7VGtHQn2n1eC2Ei9ibaFg0GUX0SEpYYvssPUFKT42063JVtYb01v1Z0uo8taeijbTr0etoKZMnJkNsDFKZ9sinmPRgRbW2NPTPEwI0crG6rYmWRchITeI3xT4mJKaHI9zHHvu4wcwKWJbt4ha9oqs1o6E/02ojiRQ/dUWvokWC56TuIUGolxlmZjFLkKppt3zWyNUa0tCf5XycLYIt26Ocqqnj0WSIcZEDGSI1FA8zBh8HsscVabZYHeCgq9fU+tHQP8f5YzJbJlgklnPHZNZvF7k0Fqlg+9DQfxSwxxXMF6Tz9+pKrRkN/fOsBuLsskJOC/yhkN9b7SLbCFJDdlhjTwNmUbaRh4A+V1PrSEP/PKtDItzJEmnOkKKLKYbt7jMhYZpI/uEcd7QgzebEstT352ptaehfpG6gKDEzgz3yJBFMSEiIyLxqd5LVQTi15jT0z7O6dY+zGSyWcGbwD1cbS5wP1tWRGOJz7Uen1MugoX+RJgABqUDmn/mYCEm3jlKXgIb+dZwHrqGrNXdpQr/DdD0HtJ/jNd1h+s29DqV+i7UP/YycJZY/4Rcv+6W8EEssZ+Qv+2WoDbP2oR9Kjz9Of8SY8os/+RI4I+dQei/7ZagNs/ahQxv7IRqHUl+VpKQjSUpddbrxhFIbQENXagNo6EptAA1dqQ2goSu1ATR0pTaAhq7UBtDQldoAGrpSG+D/B418cP3THa8WAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current bbox is {'x_min': 13, 'x_max': np.int64(35), 'y_min': 70, 'y_max': np.int64(98), 'x_center': 27, 'y_center': 84, 'width': np.int64(22), 'height': np.int64(28), 'class_value': np.uint8(9)}\n",
            "mapped prediction is [ 1. 27. 84. 22. 28.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
            "current bbox is {'x_min': 19, 'x_max': np.int64(46), 'y_min': 36, 'y_max': np.int64(63), 'x_center': 32, 'y_center': 49, 'width': np.int64(27), 'height': np.int64(27), 'class_value': np.uint8(0)}\n",
            "mapped prediction is [ 1. 32. 49. 27. 27.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "bb6G8EUtCZqe"
      },
      "id": "bb6G8EUtCZqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utils"
      ],
      "metadata": {
        "id": "zGOJZdF7TAJz"
      },
      "id": "zGOJZdF7TAJz"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# helper function to convert box values to corner coordinates\n",
        "\n",
        "\n",
        "def convert_boxes_to_corners(box_center_format):\n",
        "    # we'll use the following formulas\n",
        "    # x_min = floor(x_center - (width/2))\n",
        "    # x_max = floor(x_center + (width/2))\n",
        "    # y_min = floor(y_center - (height/2))\n",
        "    # y_max = floor(y_cetner + (height /2))\n",
        "    # calculate true values.\n",
        "    x_min = tf.floor(\n",
        "        box_center_format[:, :, :, 0] - (box_center_format[:, :, :, 2])/2)\n",
        "    x_max = tf.floor(\n",
        "        box_center_format[:, :, :, 0] + (box_center_format[:, :, :, 2])/2)\n",
        "    y_min = tf.floor(\n",
        "        box_center_format[:, :, :, 1] - (box_center_format[:, :, :, 3])/2)\n",
        "    y_max = tf.floor(\n",
        "        box_center_format[:, :, :, 1] + (box_center_format[:, :, :, 3])/2)\n",
        "\n",
        "    coordinates = tf.stack(values=[x_min, y_min, x_max, y_max], axis=3)\n",
        "    return coordinates\n",
        "\n",
        "# helper function to find the intersection box corners from given 2 boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_corners(box_1_corners, box_2_corners):\n",
        "    x_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 0], box_2_corners[:, :, :, 0])\n",
        "    y_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 1], box_2_corners[:, :, :, 1])\n",
        "    x_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 2], box_2_corners[:, :, :, 2])\n",
        "    y_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 3], box_2_corners[:, :, :, 3])\n",
        "    intersection_box_corners = tf.stack(\n",
        "        values=[x_min_for_intersection, y_min_for_intersection, x_max_for_intersection, y_max_for_intersection], axis=3)\n",
        "    return intersection_box_corners\n",
        "\n",
        "# helper function to calculate the area of intersection between two boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_area(intersection_box_corners):\n",
        "    # find the width = x_max - x_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_width = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 2] - intersection_box_corners[:, :, :, 0])\n",
        "    # find the height = y_max - y_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_height = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 3] - intersection_box_corners[:, :, :, 1])\n",
        "    # intersection area = width * height\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    return intersection_area\n",
        "\n",
        "# helper function to calcualte the area of union between two boxes\n",
        "\n",
        "\n",
        "def calculate_union_area(box_1_dimensions, box_2_dimensions, intersection_area):\n",
        "    box_1_area = box_1_dimensions[:, :, :, 0] * box_1_dimensions[:, :, :, 1]\n",
        "    box_2_area = box_2_dimensions[:, :, :, 0] * box_2_dimensions[:, :, :, 1]\n",
        "    union_area = box_1_area + box_2_area - intersection_area\n",
        "    return union_area\n",
        "\n",
        "# helper function to calculate the IOU ration between boxes\n",
        "\n",
        "\n",
        "def calculate_iou(intersection_area, union_area):\n",
        "    iou = intersection_area / (union_area + 1e-8)\n",
        "    return iou\n",
        "\n",
        "# helper function to calculate indices for the grid cells that contain the object\n",
        "\n",
        "\n",
        "def calculate_grid_cell_indices(y_true, y_pred):\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "\n",
        "\n",
        "    # Read the bounding box centers\n",
        "    # Each instance in the bach will have 5 bounding box centers\n",
        "    # select boxes with objectness equal to 1\n",
        "    # objectness_mask = y_true[:, :, 0] == 1.0\n",
        "    # bounding_boxes_with_objects = tf.boolean_mask(y_true, mask=objectness_mask)\n",
        "    # print(f\"bounding_boxes_with_objects.shape : {bounding_boxes_with_objects.shape}\")\n",
        "    bounding_box_centers = y_true[:, :, 1:3]\n",
        "\n",
        "    # TODO:  here we are assuming number of rows and columns in grid is same. Confirm the assumption.\n",
        "    # The general formula is: grid_index = floor(pixel_coordinate * (grid_size / image_size))\n",
        "    # convert each 5 bounding box centers to 5 possible grids for each instance\n",
        "\n",
        "    grid_indices = tf.cast(\n",
        "        tf.floor(bounding_box_centers * tf.cast((x_grid_size / 100),tf.float32)), dtype=tf.int32)\n",
        "\n",
        "    ## grid_indices can be out of bounds if the box centers lie on coordinates that are in multiple of 100\n",
        "    ## so we need to clip them to be maximum of 5 and minimum of 0\n",
        "    grid_indices = tf.clip_by_value(grid_indices, clip_value_min=0, clip_value_max=x_grid_size - 1)\n",
        "\n",
        "    # grid_indices = tf.reshape(grid_indices,shape=(batch_size,-1,2))\n",
        "    # print(f\"grid indices shape {tf.shape(grid_indices)}\")\n",
        "    # print(f\"grid indices {grid_indices}\")\n",
        "\n",
        "\n",
        "    return grid_indices\n",
        "\n",
        "# Helper function to calculate anchor box indices.\n",
        "\n",
        "\n",
        "def calculate_anchorbox_indices(y_true, y_pred, grid_cell_indices):\n",
        "    x_grid_size = y_pred.shape[1]\n",
        "    y_grid_size = y_pred.shape[2]\n",
        "\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(-1, x_grid_size, y_grid_size, 3, 15))\n",
        "    # print(f\"anchor_boxes.shape {anchor_boxes.shape}\")\n",
        "\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    # print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    # calcualte the IOU between anchor boxes and ground truth\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "\n",
        "    # calculate min and max values for ground truth and anchor boxes\n",
        "    y_true_box_corners = convert_boxes_to_corners(\n",
        "        expanded_y_true[:, :, :, 1:5])\n",
        "    y_pred_box_corners = convert_boxes_to_corners(\n",
        "        selected_anchor_boxes[:, :, :, 1:5])\n",
        "    # print(f\"y_true_boxes.shape {y_true_box_corners.shape}\")\n",
        "    # print(f\"y_pred_boxes.shape {y_pred_box_corners.shape}\")\n",
        "\n",
        "    # calculate the intersection coordinates between ground truth and anchor boxes\n",
        "    intersection_box_corners = calculate_intersection_corners(\n",
        "        y_true_box_corners[:, :, :, 0:], y_pred_box_corners[:, :, :, 0:])\n",
        "    # print(f\"intersection_box_corners.shape {intersection_box_corners.shape}\")\n",
        "\n",
        "    # calculate the IOU\n",
        "    # calculate intersection area\n",
        "    intersection_area = calculate_intersection_area(intersection_box_corners)\n",
        "    # print(f\"intersection_area.shape {intersection_area.shape}\")\n",
        "\n",
        "    # calculate union area\n",
        "    # we just need the width and length for union area\n",
        "    union_area = calculate_union_area(\n",
        "        expanded_y_true[:, :, :, 3:5], selected_anchor_boxes[:, :, :, 3:5], intersection_area)\n",
        "    # print(f\"union_area.shape {union_area.shape}\")\n",
        "\n",
        "    # calculate IOU\n",
        "    iou = calculate_iou(intersection_area, union_area)\n",
        "    # print(f\"iou.shape {iou.shape}\")\n",
        "\n",
        "    # select the anchor box based on best iou score\n",
        "    # select the index with highest iou\n",
        "    highest_iou_index = tf.argmax(iou, axis=2, output_type=tf.int32)\n",
        "    # print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "\n",
        "    highest_iou_index = tf.expand_dims(highest_iou_index, axis=2)\n",
        "    # highest_iou_index = tf.reshape(highest_iou_index, shape=(iou.shape[0],iou.shape[1],-1))\n",
        "    # print(f\"expanded highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    return highest_iou_index\n",
        "\n",
        "\n",
        "\n",
        "def calculate_best_anchor_boxes(y_true, y_pred):\n",
        "    # helper function to calculate best anchor boxes\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "    y_grid_size = tf.shape(y_pred)[2]\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # print(\"----- True Values -----\")\n",
        "    # print(f\"y_true.shape {tf.shape(y_true)}\")\n",
        "\n",
        "    # print(\"----- Pred Values -----\")\n",
        "    # print(f\"y_pred.shape {tf.shape(y_pred)}\")\n",
        "\n",
        "    # we have 6x6, each grid cell has 3 anchor box i.e 108 anchor boxes per insantance\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(batch_size, x_grid_size, y_grid_size, 3, 15))\n",
        "    # print(f\"anchor_boxes.shape {tf.shape(anchor_boxes)}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # out of 36 grid cells (per instance) select at most 5 grid cells that have ground truth bounding box\n",
        "    # so out of 108 anchor boxes (per instance) we only need to check 15 anchor boxes\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        params=anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    # print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "    # select the anchor box based on the index\n",
        "    best_anchor_boxes = tf.gather(\n",
        "        selected_anchor_boxes, indices=highest_iou_index, batch_dims=2)\n",
        "    # print(f\"best_anchor_boxes.shape {best_anchor_boxes.shape}\")\n",
        "\n",
        "    return best_anchor_boxes\n",
        "\n",
        "# helper function to split and calculate loss\n",
        "\n",
        "\n",
        "def calculate_loss(predicted_values, true_values):\n",
        "\n",
        "    objectness_mask = true_values[:, :, :, 0] == 1.0\n",
        "\n",
        "    true_values_with_objects = tf.boolean_mask(\n",
        "        true_values, mask=objectness_mask)\n",
        "    predicted_values_with_objects = tf.boolean_mask(\n",
        "        predicted_values, mask=objectness_mask)\n",
        "\n",
        "    # print(f\"true_values_with_objects.shape : {true_values_with_objects.shape}\")\n",
        "    # print(\n",
        "    #     f\"predicted_values_with_objects.shape : {predicted_values_with_objects.shape}\")\n",
        "    # slice the 3 properties that we are tyring to calculate loss against\n",
        "    # predicted values\n",
        "\n",
        "    y_pred_objectness = predicted_values_with_objects[:, 0]\n",
        "    # print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_bounding_box = predicted_values_with_objects[:, 1:5]\n",
        "    # print(f\"y_pred_bounding_box.shape : {y_pred_bounding_box.shape}\")\n",
        "\n",
        "    y_pred_classification = predicted_values_with_objects[:, 5:]\n",
        "    # print(f\"y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = true_values_with_objects[:, 0]\n",
        "    # print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "    y_true_bounding_box = true_values_with_objects[:, 1:5]\n",
        "    # print(f\"y_true_bounding_box.shape : {y_true_bounding_box.shape}\")\n",
        "\n",
        "    y_true_classification = true_values_with_objects[:, 5:]\n",
        "    # print(f\"y_true_classification.shape : {y_true_classification.shape}\")\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    # print(\n",
        "        # f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_classification = tf.keras.activations.softmax(y_pred_classification)\n",
        "    # print(\n",
        "    #     f\"Post Activation y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectness_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "    bounding_box_loss = tf.keras.losses.MeanSquaredError()(\n",
        "        y_true_bounding_box, y_pred_bounding_box)\n",
        "    classification_loss = tf.keras.losses.CategoricalCrossentropy()(\n",
        "        y_true_classification, y_pred_classification)\n",
        "\n",
        "    return objectness_loss, bounding_box_loss, classification_loss\n",
        "\n",
        "# helper function to calculate loss for object less cells\n",
        "\n",
        "\n",
        "def calculate_objectless_loss(y_true, y_pred):\n",
        "    # step 1: create placeholder y_true\n",
        "    batch_size = tf.shape(y_true)[0] # Get batch size dynamically\n",
        "\n",
        "    bounding_box_with_object_mask = y_true[:, :, 0] == 1.0\n",
        "\n",
        "    # step 2: prepare mask for positive values\n",
        "    # hard coding the grid size\n",
        "    positive_mask = tf.fill(dims=(batch_size, 6, 6, 3), value=False) # Adjust shape\n",
        "\n",
        "    # print(f\"positive_mask.shape {positive_mask.shape}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # grid cell indices will have shape (m, 5, 2)\n",
        "    # here 5 is max images and 2 is row and column index\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "\n",
        "    # highest_iou_index = tf.boolean_mask(\n",
        "    #     highest_iou_index, mask=bounding_box_with_object_mask)\n",
        "    # print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    # highest iou index will have shpae (m,5,1)\n",
        "    # here 5 is max images and 1 represents best anchor box in the cell.\n",
        "\n",
        "    # we need to combine both the indices to create tensor of shape (m, row indices, column indices, box index)\n",
        "    combine_update_index = tf.range(batch_size)\n",
        "    # expand dims\n",
        "    combine_update_index = tf.reshape(\n",
        "        combine_update_index, shape=(batch_size, 1, 1))\n",
        "    # combine_update_index = tf.expand_dims(combine_update_index, axis=2)\n",
        "    combine_update_index = tf.tile(\n",
        "        combine_update_index, [1, 5, 1])\n",
        "    combine_update_index = tf.concat(\n",
        "        [combine_update_index, grid_cell_indices, highest_iou_index], axis=2)\n",
        "\n",
        "    combine_update_index = tf.boolean_mask(\n",
        "        combine_update_index, mask=bounding_box_with_object_mask)\n",
        "\n",
        "    # print(f\"combine_update_index.shape : {combine_update_index.shape}\")\n",
        "\n",
        "    positive_mask_shape = tf.shape(positive_mask)\n",
        "    positive_mask = tf.scatter_nd(\n",
        "        indices=combine_update_index,\n",
        "        shape=positive_mask_shape,\n",
        "        updates=tf.fill(dims=(tf.shape(combine_update_index)[0],), value=True))\n",
        "\n",
        "    # select predicted anchor boxes based on negative masked values\n",
        "    negative_mask = tf.logical_not(positive_mask)\n",
        "    # print(f\"negative_mask.shape : {negative_mask.shape}\")\n",
        "\n",
        "    objectless_anchorboxes = tf.boolean_mask(tf.reshape(\n",
        "        y_pred, shape=(batch_size, 6, 6, 3, -1)), mask=negative_mask)\n",
        "    # print(f\"masked_values.shape : {objectless_anchorboxes.shape}\")\n",
        "\n",
        "    y_true_objectless = tf.zeros(\n",
        "        shape=tf.shape(objectless_anchorboxes), dtype=tf.float32)\n",
        "    # print(f\"y_true_objectless.shape {y_true_objectless.shape}\")\n",
        "\n",
        "\n",
        "    y_pred_objectness = objectless_anchorboxes[:, 0]\n",
        "    # print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = y_true_objectless[:, 0]\n",
        "    # print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    # print(\n",
        "    #     f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectless_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "\n",
        "    return objectless_loss\n",
        "\n",
        "# loss function for the model\n",
        "\n",
        "\n",
        "def calculate_model_loss(y_true, y_pred):\n",
        "    # Find best anchor box\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "\n",
        "    print(\"\\n\\n----- Localization Loss -----\")\n",
        "    print(f\"objectness_loss : {objectness_loss}\")\n",
        "    print(f\"bounding_box_loss : {bounding_box_loss}\")\n",
        "    print(f\"classification_loss : {classification_loss}\")\n",
        "\n",
        "    # objectless loss calculation\n",
        "    print(\"\\n\\n----- Calculation Object Less Loss -----\")\n",
        "    objectless_loss = calculate_objectless_loss(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    print(\"\\n\\n----- Object Less Loss -----\")\n",
        "    print(f\"objectless_loss : {objectless_loss}\")\n",
        "\n",
        "    # scale the losses\n",
        "    lambda_objectness = 1\n",
        "    lambda_bounding_box = 0.001\n",
        "    lambda_classification = 1\n",
        "    lambda_objectless = 1\n",
        "\n",
        "    total_loss = (objectness_loss * lambda_objectness) + (bounding_box_loss *\n",
        "                                                          lambda_bounding_box) + (classification_loss * lambda_classification) + (objectless_loss * lambda_objectless)\n",
        "\n",
        "    print(f\"\\n\\nTotal Loss : {total_loss}\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def objectness_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return objectness_loss\n",
        "\n",
        "\n",
        "def bounding_box_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return bounding_box_loss\n",
        "\n",
        "\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return classification_loss"
      ],
      "metadata": {
        "id": "9N1MLKguS9Hh"
      },
      "id": "9N1MLKguS9Hh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running The Model Manually"
      ],
      "metadata": {
        "id": "ErjI4ehgTVl0"
      },
      "id": "ErjI4ehgTVl0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Model"
      ],
      "metadata": {
        "id": "-Ci-WgQ3TKaC"
      },
      "id": "-Ci-WgQ3TKaC"
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Rescaling(scale=1./255),\n",
        "\n",
        "    ## starting with a larger filter since we are dealing with 100x100x1 image\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    ## rest of the layers are same as our original mnist classifier\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    ## finaly layers to output 6x6x45 grid of predictions\n",
        "    tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear'),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "GtSDiOxNTFtD"
      },
      "id": "GtSDiOxNTFtD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqzrwQnvTPSM",
        "outputId": "fd1ff659-7828-454b-aec8-b1e649f0579b"
      },
      "id": "aqzrwQnvTPSM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " rescaling_1 (\u001b[38;5;33mRescaling\u001b[0m)          ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " max_pooling2d_8 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n",
              " max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  ?                                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)               ?                         \u001b[38;5;34m0\u001b[0m (unbuilt) \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " rescaling_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)          ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " max_pooling2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              " max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  ?                                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               ?                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run in Eager Mode"
      ],
      "metadata": {
        "id": "ZfPbA4gSg0Hp"
      },
      "id": "ZfPbA4gSg0Hp"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is temp code to test the loss function do not use this for training.\n",
        "X_tensor = tf.convert_to_tensor(x_train[0:10], dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor, shape=(-1, 28, 28, 1))\n",
        "y_tensor = tf.convert_to_tensor(y_train[0:10], dtype=tf.float32)\n",
        "\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5, 15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "\n",
        "processed_dataset = raw_dataset.map(lambda X, y: tf.numpy_function(generate_training_example, inp=[X, y], Tout=(\n",
        "    tf.float32, tf.float32)), num_parallel_calls=15);\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=8)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "# Step 1: Get one batch of data from your dataset pipeline\n",
        "# The .take(1) method creates a new dataset with only the first element.\n",
        "one_batch = processed_dataset.take(10)\n",
        "\n",
        "# Step 2: Iterate over the single batch to get the tensors\n",
        "for images, labels in one_batch:\n",
        "\n",
        "    # --- THIS IS YOUR DEBUGGING ZONE ---\n",
        "    # Now you have the concrete tensors for one batch.\n",
        "    # You can inspect them with regular print() and .numpy()\n",
        "\n",
        "    # print(\"--- Inspecting Data Before Loss Calculation ---\")\n",
        "    # print(\"Shape of images (X_batch):\", images.shape)\n",
        "    # print(\"Shape of labels (y_true_batch):\", labels.shape)\n",
        "    # print(\"\\nSample y_true label tensor:\\n\", labels.numpy()[0]) # Print the first label in the batch\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Step 3: Manually run the forward pass and gradient calculation\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the model's raw predictions for this batch\n",
        "        y_pred = model(images, training=True)  # Pass the images through the model\n",
        "\n",
        "        # --- MORE DEBUGGING ---\n",
        "        print(\"\\n--- Inspecting Tensors Passed to Loss Function ---\")\n",
        "        print(\"Shape of y_pred from model:\", y_pred.shape)\n",
        "        # print(\"\\nSample y_pred tensor (first 5 values of first anchor):\\n\", y_pred.numpy()[0, 0, 0, :5])\n",
        "        # ----------------------\n",
        "\n",
        "        # Call your custom loss function\n",
        "        # You can now add print statements INSIDE your loss function too!\n",
        "        loss_value = calculate_model_loss(labels, y_pred)\n",
        "\n",
        "        print(\"\\n--- Final Calculated Loss ---\")\n",
        "\n",
        "        print(\"Total Loss for the batch:\", loss_value.numpy())\n",
        "        # -----------------------------\n",
        "\n",
        "    # Step 4 (Optional): Calculate and apply gradients to see the full loop\n",
        "    # grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    # model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(\"\\n--- Manual Step Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJL4G8cVTYqo",
        "outputId": "d3efc438-5ee0-4ccd-ed78-80b901af6369"
      },
      "id": "lJL4G8cVTYqo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (8, 6, 6, 45)\n",
            "----- True Values -----\n",
            "y_true.shape [ 8  5 15]\n",
            "----- Pred Values -----\n",
            "y_pred.shape [ 8  6  6 45]\n",
            "anchor_boxes.shape [ 8  6  6  3 15]\n",
            "grid indices shape [8 5 2]\n",
            "grid indices [[[4 3]\n",
            "  [0 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 2]\n",
            "  [1 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [5 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 3]\n",
            "  [1 2]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 2]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 1]\n",
            "  [3 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "anchor_boxes.shape (8, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "y_true_boxes.shape (8, 5, 1, 4)\n",
            "y_pred_boxes.shape (8, 5, 3, 4)\n",
            "intersection_box_corners.shape (8, 5, 3, 4)\n",
            "intersection_area.shape (8, 5, 3)\n",
            "union_area.shape (8, 5, 3)\n",
            "iou.shape (8, 5, 3)\n",
            "highest_iou_index.shape (8, 5)\n",
            "expanded highest_iou_index.shape (8, 5, 1)\n",
            "best_anchor_boxes.shape (8, 5, 1, 15)\n",
            "true_values_with_objects.shape : (16, 15)\n",
            "predicted_values_with_objects.shape : (16, 15)\n",
            "y_pred_objectness.shape : (16,)\n",
            "y_pred_bounding_box.shape : (16, 4)\n",
            "y_pred_classification.shape : (16, 10)\n",
            "y_true_objectness.shape : (16,)\n",
            "y_true_bounding_box.shape : (16, 4)\n",
            "y_true_classification.shape : (16, 10)\n",
            "Post Activation y_pred_objectness.shape : (16,)\n",
            "Post Activation y_pred_classification.shape : (16, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : 0.6936220526695251\n",
            "bounding_box_loss : 1907.470458984375\n",
            "classification_loss : 2.304104804992676\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (8, 6, 6, 3)\n",
            "grid indices shape [8 5 2]\n",
            "grid indices [[[4 3]\n",
            "  [0 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 2]\n",
            "  [1 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [5 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 3]\n",
            "  [1 2]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 2]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 1]\n",
            "  [3 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "anchor_boxes.shape (8, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "y_true_boxes.shape (8, 5, 1, 4)\n",
            "y_pred_boxes.shape (8, 5, 3, 4)\n",
            "intersection_box_corners.shape (8, 5, 3, 4)\n",
            "intersection_area.shape (8, 5, 3)\n",
            "union_area.shape (8, 5, 3)\n",
            "iou.shape (8, 5, 3)\n",
            "highest_iou_index.shape (8, 5)\n",
            "expanded highest_iou_index.shape (8, 5, 1)\n",
            "highest_iou_index.shape (8, 5, 1)\n",
            "combine_update_index.shape : (16, 4)\n",
            "negative_mask.shape : (8, 6, 6, 3)\n",
            "masked_values.shape : (848, 15)\n",
            "y_true_objectless.shape (848, 15)\n",
            "y_pred_objectness.shape : (848,)\n",
            "y_true_objectness.shape : (848,)\n",
            "Post Activation y_pred_objectness.shape : (848,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6930412650108337\n",
            "\n",
            "\n",
            "Total Loss : 5.598238468170166\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: 5.5982385\n",
            "\n",
            "--- Manual Step Complete ---\n",
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (2, 6, 6, 45)\n",
            "----- True Values -----\n",
            "y_true.shape [ 2  5 15]\n",
            "----- Pred Values -----\n",
            "y_pred.shape [ 2  6  6 45]\n",
            "anchor_boxes.shape [ 2  6  6  3 15]\n",
            "grid indices shape [2 5 2]\n",
            "grid indices [[[1 4]\n",
            "  [2 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "anchor_boxes.shape (2, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "y_true_boxes.shape (2, 5, 1, 4)\n",
            "y_pred_boxes.shape (2, 5, 3, 4)\n",
            "intersection_box_corners.shape (2, 5, 3, 4)\n",
            "intersection_area.shape (2, 5, 3)\n",
            "union_area.shape (2, 5, 3)\n",
            "iou.shape (2, 5, 3)\n",
            "highest_iou_index.shape (2, 5)\n",
            "expanded highest_iou_index.shape (2, 5, 1)\n",
            "best_anchor_boxes.shape (2, 5, 1, 15)\n",
            "true_values_with_objects.shape : (4, 15)\n",
            "predicted_values_with_objects.shape : (4, 15)\n",
            "y_pred_objectness.shape : (4,)\n",
            "y_pred_bounding_box.shape : (4, 4)\n",
            "y_pred_classification.shape : (4, 10)\n",
            "y_true_objectness.shape : (4,)\n",
            "y_true_bounding_box.shape : (4, 4)\n",
            "y_true_classification.shape : (4, 10)\n",
            "Post Activation y_pred_objectness.shape : (4,)\n",
            "Post Activation y_pred_classification.shape : (4, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : 0.6934143304824829\n",
            "bounding_box_loss : 1874.39990234375\n",
            "classification_loss : 2.301903486251831\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (2, 6, 6, 3)\n",
            "grid indices shape [2 5 2]\n",
            "grid indices [[[1 4]\n",
            "  [2 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 1]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "anchor_boxes.shape (2, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "y_true_boxes.shape (2, 5, 1, 4)\n",
            "y_pred_boxes.shape (2, 5, 3, 4)\n",
            "intersection_box_corners.shape (2, 5, 3, 4)\n",
            "intersection_area.shape (2, 5, 3)\n",
            "union_area.shape (2, 5, 3)\n",
            "iou.shape (2, 5, 3)\n",
            "highest_iou_index.shape (2, 5)\n",
            "expanded highest_iou_index.shape (2, 5, 1)\n",
            "highest_iou_index.shape (2, 5, 1)\n",
            "combine_update_index.shape : (4, 4)\n",
            "negative_mask.shape : (2, 6, 6, 3)\n",
            "masked_values.shape : (212, 15)\n",
            "y_true_objectless.shape (212, 15)\n",
            "y_pred_objectness.shape : (212,)\n",
            "y_true_objectness.shape : (212,)\n",
            "Post Activation y_pred_objectness.shape : (212,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6930784583091736\n",
            "\n",
            "\n",
            "Total Loss : 5.562796115875244\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: 5.562796\n",
            "\n",
            "--- Manual Step Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model for 20 Epochs\n",
        "\n",
        "* So far our loss functions work as expected in Eager mode.  \n",
        "* In this section we'll run the loss functions in optimized `Graph Mode` to see if it works as expected and also if the loss decreases as we progress.\n",
        "* Our experiments in other notebooks we know that we'll need to create the model using Functional API."
      ],
      "metadata": {
        "id": "p5Ky6tdhbwEr"
      },
      "id": "p5Ky6tdhbwEr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Dataset"
      ],
      "metadata": {
        "id": "svkr5mXxfHNG"
      },
      "id": "svkr5mXxfHNG"
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor,shape=(-1,28,28,1))\n",
        "y_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "batch_size = 32\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5,15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=batch_size)"
      ],
      "metadata": {
        "id": "gkzQx-sBeYr_"
      },
      "id": "gkzQx-sBeYr_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Model"
      ],
      "metadata": {
        "id": "YgkYBImJfQbl"
      },
      "id": "YgkYBImJfQbl"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(100,100,1),batch_size=batch_size ,name=\"input_layer\")\n",
        "\n",
        "x = tf.keras.layers.Rescaling(scale=1./255, name=\"rescaling\")(inputs)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear')(x)\n",
        "\n",
        "# Define the final model by specifying its inputs and outputs\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "XOoSfkYEfNyj",
        "outputId": "4d0ddbb4-88f3-4034-e39a-3501476f54de"
      },
      "id": "XOoSfkYEfNyj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)         (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " rescaling (\u001b[38;5;33mRescaling\u001b[0m)            (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_27 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 \u001b[38;5;34m208\u001b[0m \n",
              "\n",
              " conv2d_28 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)               \u001b[38;5;34m1,608\u001b[0m \n",
              "\n",
              " max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_29 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m584\u001b[0m \n",
              "\n",
              " conv2d_30 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m584\u001b[0m \n",
              "\n",
              " max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m8\u001b[0m)                     \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)                \u001b[38;5;34m1,168\u001b[0m \n",
              "\n",
              " conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)                \u001b[38;5;34m2,320\u001b[0m \n",
              "\n",
              " max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)                    \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m4,640\u001b[0m \n",
              "\n",
              " conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)                \u001b[38;5;34m9,248\u001b[0m \n",
              "\n",
              " max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)                      \u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m45\u001b[0m)                  \u001b[38;5;34m1,485\u001b[0m \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)            (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> \n",
              "\n",
              " conv2d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> \n",
              "\n",
              " max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> \n",
              "\n",
              " conv2d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> \n",
              "\n",
              " max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> \n",
              "\n",
              " conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> \n",
              "\n",
              " max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> \n",
              "\n",
              " conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> \n",
              "\n",
              " max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,485</span> \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Callbacks"
      ],
      "metadata": {
        "id": "oYkmsyYFfVG_"
      },
      "id": "oYkmsyYFfVG_"
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: Define the callbacks\n",
        "checkpoint_filepath = './models/experiment_1_{epoch:02d}.keras'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=1,\n",
        "    )"
      ],
      "metadata": {
        "id": "j-30sp7AfSYt"
      },
      "id": "j-30sp7AfSYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile & Fit"
      ],
      "metadata": {
        "id": "Yzyrwlo6fZR1"
      },
      "id": "Yzyrwlo6fZR1"
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "## step 5: Fit the model\n",
        "epochs=20\n",
        "\n",
        "history = model.fit(\n",
        "  processed_dataset,\n",
        "  epochs=epochs,\n",
        "  callbacks=[model_checkpoint_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gAB0cfrqfW3s",
        "outputId": "9313c24f-d8c7-4b5d-d5c9-888938c55b52"
      },
      "id": "gAB0cfrqfW3s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"compile_loss/calculate_model_loss/Shape_3:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"compile_loss/calculate_model_loss/Shape_4:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"compile_loss/calculate_model_loss/Shape_5:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_7:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy/Mean:0\", shape=(), dtype=float32)\n",
            "bounding_box_loss : Tensor(\"compile_loss/calculate_model_loss/mean_squared_error/div_no_nan:0\", shape=(), dtype=float32)\n",
            "classification_loss : Tensor(\"compile_loss/calculate_model_loss/categorical_crossentropy/div_no_nan:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (None, 6, 6, 3)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_10:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "highest_iou_index.shape (None, 5, 1)\n",
            "combine_update_index.shape : (None, 4)\n",
            "negative_mask.shape : (None, 6, 6, 3)\n",
            "masked_values.shape : (None, None)\n",
            "y_true_objectless.shape (None, None)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_true_objectness.shape : (None,)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy_1/Mean:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "Total Loss : Tensor(\"compile_loss/calculate_model_loss/add_14:0\", shape=(), dtype=float32)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_5:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_6:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_7:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_9:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_13:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_14:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_15:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_17:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_21:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_22:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_23:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_25:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_2:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"compile_loss/calculate_model_loss/Shape_3:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"compile_loss/calculate_model_loss/Shape_4:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"compile_loss/calculate_model_loss/Shape_5:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_7:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy/Mean:0\", shape=(), dtype=float32)\n",
            "bounding_box_loss : Tensor(\"compile_loss/calculate_model_loss/mean_squared_error/div_no_nan:0\", shape=(), dtype=float32)\n",
            "classification_loss : Tensor(\"compile_loss/calculate_model_loss/categorical_crossentropy/div_no_nan:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (None, 6, 6, 3)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_10:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "highest_iou_index.shape (None, 5, 1)\n",
            "combine_update_index.shape : (None, 4)\n",
            "negative_mask.shape : (None, 6, 6, 3)\n",
            "masked_values.shape : (None, None)\n",
            "y_true_objectless.shape (None, None)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_true_objectness.shape : (None,)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy_1/Mean:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "Total Loss : Tensor(\"compile_loss/calculate_model_loss/add_14:0\", shape=(), dtype=float32)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_5:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_6:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_7:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_9:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_13:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_14:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_15:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_17:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_21:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_22:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_23:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_25:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_2:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - bounding_box_metrics: 452.1248 - classification_metrics: 2.1821 - loss: 3.3135 - objectness_metrics: 0.2542\n",
            "Epoch 1: loss improved from inf to 2.83304, saving model to ./models/experiment_2_01.keras\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2763s\u001b[0m 1s/step - bounding_box_metrics: 452.0234 - classification_metrics: 2.1820 - loss: 3.3132 - objectness_metrics: 0.2542\n",
            "Epoch 2/20\n",
            "\u001b[1m 129/1875\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m44:48\u001b[0m 2s/step - bounding_box_metrics: 137.0666 - classification_metrics: 1.6009 - loss: 2.2942 - objectness_metrics: 0.2160"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3685437085.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprocessed_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "namrr1vJfbEV"
      },
      "id": "namrr1vJfbEV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}