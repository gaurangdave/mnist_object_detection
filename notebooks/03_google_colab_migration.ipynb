{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "88d9b8e6",
      "metadata": {
        "id": "88d9b8e6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurangdave/mnist_object_detection/blob/main/notebooks/03_google_colab_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5547be1",
      "metadata": {
        "id": "d5547be1"
      },
      "source": [
        "# MNIST Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0tP3N0G6DNgp"
      },
      "id": "0tP3N0G6DNgp"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.datasets import fetch_openml\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import PIL.Image\n",
        "from matplotlib import patches\n",
        "\n"
      ],
      "metadata": {
        "id": "C88hhW3fDQFO"
      },
      "id": "C88hhW3fDQFO",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dMCwJ7fDeN1",
        "outputId": "56adca6c-d4cd-4f78-f73b-550b9b798452"
      },
      "id": "4dMCwJ7fDeN1",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Access"
      ],
      "metadata": {
        "id": "VPEcmSCfCH8g"
      },
      "id": "VPEcmSCfCH8g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Constants  "
      ],
      "metadata": {
        "id": "wa9FmTPCDbn1"
      },
      "id": "wa9FmTPCDbn1"
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path(\"data\")\n",
        "models_dir = Path(\"models\")"
      ],
      "metadata": {
        "id": "xIdEzErNEHs3"
      },
      "id": "xIdEzErNEHs3",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "17DdP_OEEW9f",
        "outputId": "323aebd1-122c-4281-e581-aac2e1fb587c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "17DdP_OEEW9f",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBYeASoEEXRF",
        "outputId": "861a02c7-5e2b-42dd-b525-48a65aaf9d07"
      },
      "id": "WBYeASoEEXRF",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xe9omI65EXT6"
      },
      "id": "xe9omI65EXT6",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL1_1CCfEXWQ"
      },
      "id": "YL1_1CCfEXWQ",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "52Wv6_QDCRi5"
      },
      "id": "52Wv6_QDCRi5"
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_MNIST_DATA_PIXELS = x_train\n",
        "ALL_MNIST_DATA_CLASSES = y_train\n",
        "\n",
        "# number of digits to overlay on canvas\n",
        "num_of_digits = 2\n",
        "\n",
        "# max digits to define the shape of prediction output\n",
        "MAX_DIGITS = 5\n",
        "\n",
        "\n",
        "# Sample Base Digits\n",
        "def get_sample_indices(dataset, size=5):\n",
        "  random_indices = np.random.choice(len(dataset), size=size, replace=False)\n",
        "  return random_indices\n",
        "\n",
        "# helper function to sample number of digits from master dataset\n",
        "def sample_base_digits(num_of_digits):\n",
        "    \"\"\"\n",
        "    Sample a specified number of digit images and their class labels from the master MNIST dataset.\n",
        "\n",
        "    Args:\n",
        "        num_of_digits (int): Number of digit samples to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sample_pixels, sample_values)\n",
        "            sample_pixels (np.ndarray): Array of digit images with shape (num_of_digits, 28, 28, 1).\n",
        "            sample_values (np.ndarray): Array of class labels with shape (num_of_digits, 1).\n",
        "    \"\"\"\n",
        "    sample_indices = get_sample_indices(ALL_MNIST_DATA_PIXELS, size=num_of_digits)\n",
        "    sample_pixels = ALL_MNIST_DATA_PIXELS[sample_indices]\n",
        "    sample_pixels = sample_pixels.reshape(-1,28,28,1)\n",
        "\n",
        "    sample_values = ALL_MNIST_DATA_CLASSES[sample_indices]\n",
        "    sample_values = sample_values.reshape(-1, 1)\n",
        "\n",
        "    # split the digits into pixels and class values\n",
        "    # reshape the data to expected values\n",
        "    # sample_pixels = sample.drop(\n",
        "    #     columns=[\"class\"]).to_numpy().reshape(-1, 28, 28, 1)\n",
        "    # sample_values = sample[\"class\"].values.reshape(-1, 1)\n",
        "    return sample_pixels, sample_values\n",
        "# Augment Digits\n",
        "\n",
        "\n",
        "def plot_before_after(before_image, after_image):\n",
        "    \"\"\"\n",
        "    Display two images side by side for visual comparison (e.g., before and after augmentation).\n",
        "\n",
        "    Args:\n",
        "        before_image (np.ndarray): The original image.\n",
        "        after_image (np.ndarray): The image after transformation or augmentation.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "    axs[0].imshow(before_image)\n",
        "    axs[1].imshow(after_image)\n",
        "\n",
        "    plt.axis(\"off\")  # Remove axes for better visualization\n",
        "    plt.show()\n",
        "# helper function to apply random augmentation to digits\n",
        "\n",
        "\n",
        "def augment_digits(digits, debug=False):\n",
        "    \"\"\"\n",
        "    Apply random augmentations (translation, zoom, rotation) to a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        digits (np.ndarray): Array of digit images to augment.\n",
        "        debug (bool, optional): If True, displays before/after images for each digit. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Augmented digit images as a numpy array.\n",
        "    \"\"\"\n",
        "    tensor_digits = tf.convert_to_tensor(digits)\n",
        "\n",
        "    # step 2: apply random augmentation\n",
        "    augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomTranslation(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomRotation(\n",
        "            factor=0.1, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "    ])\n",
        "    augmented_tensor_digits = augmentation(tensor_digits)\n",
        "\n",
        "    # if debug is true render before digits\n",
        "    if debug == True:\n",
        "        for translated_imgs in range(tensor_digits.shape[0]):\n",
        "            plot_before_after(\n",
        "                tensor_digits[translated_imgs], augmented_tensor_digits[translated_imgs])\n",
        "\n",
        "    # convert the tensor back to numpy to simplify use in map function\n",
        "    return augmented_tensor_digits.numpy()\n",
        "# Calculate Tight BBox\n",
        "# helper function to calculate bounding box for each instance and return it.\n",
        "# we are going to refactor the POC that we created ealier to use it with numpy arrays in the map function\n",
        "\n",
        "\n",
        "def calculate_bounding_box(pixels, class_value, padding=1):\n",
        "    \"\"\"\n",
        "    Calculate the tight bounding box for a digit image and return its coordinates and class value.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): 2D array representing the digit image.\n",
        "        class_value (int): The class label of the digit.\n",
        "        padding (int, optional): Padding to add around the bounding box. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        dict: Bounding box information including coordinates, center, width, height, and class value.\n",
        "    \"\"\"\n",
        "    # calculate active rows & columns\n",
        "    active_rows = np.sum(pixels, axis=1)\n",
        "    active_columns = np.sum(pixels, axis=0)\n",
        "\n",
        "    # calculate x_min and x_max coordinate\n",
        "    x_min = np.nonzero(active_columns)[0][0]\n",
        "    x_max = np.nonzero(active_columns)[0][-1]\n",
        "    y_min = np.nonzero(active_rows)[0][0]\n",
        "    y_max = np.nonzero(active_rows)[0][-1]\n",
        "\n",
        "    # add padding to pixels\n",
        "    x_min = x_min - (padding if (x_min != 0) else 0)\n",
        "    x_max = x_max + (padding if (x_max != 27) else 0)\n",
        "    y_min = y_min - (padding if (y_min != 0) else 0)\n",
        "    y_max = y_max + (padding if (y_max != 27) else 0)\n",
        "\n",
        "    # calcualte x_center and y_center\n",
        "    x_center = round((x_min + x_max) / 2)\n",
        "    y_center = round((y_min + y_max) / 2)\n",
        "\n",
        "    # calculate width and height\n",
        "    width = x_max - x_min + 1\n",
        "    height = y_max - y_min + 1\n",
        "\n",
        "    return {\n",
        "        \"x_min\": x_min,\n",
        "        \"x_max\": x_max,\n",
        "        \"y_min\": y_min,\n",
        "        \"y_max\": y_max,\n",
        "        \"x_center\": x_center,\n",
        "        \"y_center\": y_center,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"class_value\": class_value\n",
        "    }\n",
        "\n",
        "\n",
        "# helper function to visualize the bounding box\n",
        "\n",
        "\n",
        "def visualize_bounding_box(pixel_data, bounding_boxes, num_of_columns=5):\n",
        "    \"\"\"\n",
        "    Visualize digit images with their corresponding bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        pixel_data (np.ndarray): Array of digit images.\n",
        "        bounding_boxes (list): List of bounding box dictionaries for each digit.\n",
        "        num_of_columns (int, optional): Number of columns in the plot grid. Defaults to 5.\n",
        "    \"\"\"\n",
        "    num_of_columns = num_of_columns if num_of_columns <= 5 else 5\n",
        "    num_instances = pixel_data.shape[0]\n",
        "    num_of_rows = int(num_instances / num_of_columns) + \\\n",
        "        (1 if int(num_instances % num_of_columns) > 0 else 0)\n",
        "\n",
        "    fig, axs = plt.subplots(num_of_rows, num_of_columns, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for idx in range(0, num_instances, 1):\n",
        "\n",
        "        original = tf.constant(pixel_data[idx].reshape(28, 28, 1))\n",
        "        converted = tf.image.grayscale_to_rgb(original)\n",
        "        target_data = bounding_boxes[idx]\n",
        "        x_center = target_data[\"x_center\"]\n",
        "        y_center = target_data[\"y_center\"]\n",
        "        width = target_data[\"width\"]\n",
        "        height = target_data[\"height\"]\n",
        "\n",
        "        x = target_data[\"x_min\"]\n",
        "        y = target_data[\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        image_data = converted.numpy().astype(\"uint8\")\n",
        "        axs[idx].imshow(image_data)\n",
        "        axs[idx].add_patch(rect)\n",
        "\n",
        "        axs[idx].set_title(target_data[\"class_value\"])\n",
        "        axs[idx].axis(\"off\")\n",
        "    plt.show()\n",
        "# helper function to calculate bounding box for digits.\n",
        "\n",
        "\n",
        "def calculate_tight_bbox(pixels, class_values, debug=False):\n",
        "    \"\"\"\n",
        "    Calculate tight bounding boxes for a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_values (np.ndarray): Array of class labels for each digit.\n",
        "        debug (bool, optional): If True, visualizes the bounding boxes. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    class_with_bbox = []\n",
        "    for idx in range(pixels.shape[0]):\n",
        "        class_with_bbox.append(calculate_bounding_box(\n",
        "            pixels[idx], class_values[idx][0]))\n",
        "\n",
        "    # if debug true render digits with bbox\n",
        "    if debug == True:\n",
        "        visualize_bounding_box(pixels, class_with_bbox, pixels.shape[0])\n",
        "\n",
        "    return class_with_bbox\n",
        "# Create Blank Canvas\n",
        "# helper function to create  blank canvas\n",
        "\n",
        "\n",
        "def create_blank_canvas(shape=(100, 100, 1)):\n",
        "    \"\"\"\n",
        "    Create a blank canvas for placing digit images.\n",
        "\n",
        "    Args:\n",
        "        shape (tuple, optional): Shape of the canvas. Defaults to (100, 100, 1).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Blank canvas array.\n",
        "    \"\"\"\n",
        "    canvas = np.zeros(shape=(100, 100, 1), dtype=np.float32)\n",
        "    return canvas\n",
        "# Create Prediction Object\n",
        "# helper function to create empty predition structure based on MAX_DIGITS\n",
        "\n",
        "\n",
        "def create_prediction_object():\n",
        "    \"\"\"\n",
        "    Create an empty prediction object for storing digit detection results.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Prediction array of shape (MAX_DIGITS, 15).\n",
        "    \"\"\"\n",
        "    prediction = np.zeros(shape=(MAX_DIGITS, 15), dtype=np.float32)\n",
        "    return prediction\n",
        "# Place Digit On Canvas\n",
        "\n",
        "\n",
        "def is_valid_coordinates(top, left, class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Check if the proposed top-left coordinates for a digit's bounding box are valid (within canvas and non-overlapping).\n",
        "\n",
        "    Args:\n",
        "        top (int): Proposed top coordinate.\n",
        "        left (int): Proposed left coordinate.\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if coordinates are valid, False otherwise.\n",
        "    \"\"\"\n",
        "    # # make sure the top and left are withing the canvas\n",
        "    # if (top + 28 >= 100 or left + 28 >= 100):\n",
        "    #     return False\n",
        "    # read current class values\n",
        "    # curr_x_center = class_bbox_value[\"x_center\"]\n",
        "    # curr_y_center = class_bbox_value[\"y_center\"]\n",
        "    curr_width = class_bbox_value[\"width\"]\n",
        "    curr_height = class_bbox_value[\"height\"]\n",
        "    curr_x_min = left\n",
        "    curr_y_min = top\n",
        "    curr_x_max = left + curr_width\n",
        "    curr_y_max = top + curr_height\n",
        "\n",
        "    # recalculate center with proposed top and left values\n",
        "    # curr_x_center = curr_x_center + left\n",
        "    # curr_y_center = curr_y_center + top\n",
        "\n",
        "    # check 1: will the new bounding box go beyond the grid?\n",
        "    if ((curr_x_min + curr_width) >= 100) or ((curr_y_min + curr_height) >= 100):\n",
        "        return False\n",
        "\n",
        "    # check 2: do bounding boxes overlap\n",
        "    # check the current bounding box with every existing box\n",
        "    for coord_idx in range(len(existing_coordinates)):\n",
        "        existing_x_min = existing_coordinates[coord_idx][\"x_min\"]\n",
        "        existing_y_min = existing_coordinates[coord_idx][\"y_min\"]\n",
        "        existing_x_max = existing_coordinates[coord_idx][\"x_max\"]\n",
        "        existing_y_max = existing_coordinates[coord_idx][\"y_max\"]\n",
        "        if ((curr_x_min <= existing_x_max and curr_x_max >= existing_x_min) and (curr_y_min <= existing_y_max and curr_y_max >= existing_y_min)):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def select_top_left(class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Randomly select valid top-left coordinates for placing a digit on the canvas.\n",
        "\n",
        "    Args:\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (top, left) coordinates if valid, otherwise (-1, -1).\n",
        "    \"\"\"\n",
        "    got_valid_coordinates = False\n",
        "    # limiting the loop to run only 20 times\n",
        "    retries = 0\n",
        "    while ((not got_valid_coordinates) and (retries < 50)):\n",
        "        top = np.random.randint(0, high=100)\n",
        "        left = np.random.randint(0, high=100)\n",
        "        got_valid_coordinates = is_valid_coordinates(\n",
        "            top, left, class_bbox_value, existing_coordinates)\n",
        "        retries = retries+1\n",
        "\n",
        "    if got_valid_coordinates:\n",
        "        return top, left\n",
        "    return -1, -1\n",
        "\n",
        "\n",
        "# helper function to render the canvas\n",
        "# update the original plotting function to plot canvas as well.\n",
        "\n",
        "\n",
        "def render_canvas(canvas, class_bbox):\n",
        "    \"\"\"\n",
        "    Render the canvas with all placed digits and their bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The canvas image.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    num_of_digits = len(class_bbox)\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
        "    axs.imshow(canvas)  # Use 'gray' colormap to render grayscale\n",
        "    axs.axis(\"off\")\n",
        "\n",
        "    for idx in range(0, num_of_digits, 1):\n",
        "        width = class_bbox[idx][\"width\"]\n",
        "        height = class_bbox[idx][\"height\"]\n",
        "\n",
        "        x = class_bbox[idx][\"x_min\"]\n",
        "        y = class_bbox[idx][\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        axs.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def place_digit_on_canvas(canvas, pixels, class_bbox, debug=False):\n",
        "    \"\"\"\n",
        "    Place digit images on the canvas at valid, non-overlapping locations.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The blank canvas to place digits on.\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "        debug (bool, optional): If True, renders the canvas after placement. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, class_bbox) with updated canvas and bounding boxes.\n",
        "    \"\"\"\n",
        "    # list to save all the valid existing coordinates\n",
        "    existing_coordinates = []\n",
        "\n",
        "    # in case if the algorithm cannot place a digit on canvas we'll drop that digit from pixel and class_bbox\n",
        "    digits_to_drop = []\n",
        "\n",
        "    total_digits = pixels.shape[0]\n",
        "    # loop thru all the pixel values\n",
        "    for idx in range(total_digits):\n",
        "        class_bbox_value = class_bbox[idx]\n",
        "        x_center = class_bbox_value[\"x_center\"]\n",
        "        y_center = class_bbox_value[\"y_center\"]\n",
        "        width = class_bbox_value[\"width\"]\n",
        "        height = class_bbox_value[\"height\"]\n",
        "        class_value = class_bbox_value[\"class_value\"]\n",
        "        x_min = class_bbox_value[\"x_min\"]\n",
        "        y_min = class_bbox_value[\"y_min\"]\n",
        "        x_max = class_bbox_value[\"x_max\"]\n",
        "        y_max = class_bbox_value[\"y_max\"]\n",
        "\n",
        "        # print(f\"Width {width} & {int(width)}, Height {height} & {int(height)}\")\n",
        "        # width = int(width)\n",
        "        # height = int(height)\n",
        "\n",
        "        # step 1: find the right coordinates to place the digit\n",
        "        top, left = select_top_left(class_bbox_value, existing_coordinates)\n",
        "        if top != -1 and left != -1:\n",
        "            # step 2: place the digit\n",
        "            # canvas[y_min + top:y_min + top+height, x_min + left:x_min +\n",
        "            #        left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            canvas[top:top+height, left:\n",
        "                   left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            # step 3: recalculate the center based on top,left and update the class values with new center\n",
        "            class_bbox_value[\"x_center\"] = x_center + left\n",
        "            class_bbox_value[\"x_min\"] = left\n",
        "            class_bbox_value[\"x_max\"] = left + width\n",
        "\n",
        "            class_bbox_value[\"y_center\"] = y_center + top\n",
        "            class_bbox_value[\"y_min\"] = top\n",
        "            class_bbox_value[\"y_max\"] = top + height\n",
        "\n",
        "            # update the array\n",
        "            class_bbox[idx] = class_bbox_value\n",
        "            # step 5: save the existing bounding box coordinates to help select the new one\n",
        "            existing_coordinates.append(\n",
        "                class_bbox_value\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Error placing digit {class_value} on canvas. Couldn't fild valid coordinates\")\n",
        "            digits_to_drop.append(idx)\n",
        "\n",
        "    # drop any bbox for which we couldn't find space in canvas\n",
        "    filtered_bbox = [bbox for idx, bbox in enumerate(\n",
        "        class_bbox) if idx not in digits_to_drop]\n",
        "\n",
        "    if debug == True:\n",
        "        render_canvas(canvas=canvas, class_bbox=filtered_bbox)\n",
        "\n",
        "    return canvas, class_bbox\n",
        "# Translate BBox To Prediction Object\n",
        "# helper function to convert bbox diction to prediction object\n",
        "\n",
        "\n",
        "def translate_bbox_to_prediction(current_bbox, prediction, debug=False):\n",
        "    \"\"\"\n",
        "    Convert bounding box dictionaries to a prediction object suitable for training.\n",
        "\n",
        "    Args:\n",
        "        current_bbox (list): List of bounding box dictionaries.\n",
        "        prediction (np.ndarray): Prediction array to update.\n",
        "        debug (bool, optional): If True, prints mapping details. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Updated prediction array.\n",
        "    \"\"\"\n",
        "    # Sanity check - ideally prediction shape should be larger than or equal to number of elements in bbox\n",
        "    if (prediction.shape[0] < len(current_bbox)):\n",
        "        print(f\"Error shape mismatch between prediction and bbox\")\n",
        "        return prediction\n",
        "\n",
        "    for idx, bbox in enumerate(current_bbox):\n",
        "        # set the flag indicating the digit is present\n",
        "        prediction[idx][0] = 1\n",
        "        # set x_center\n",
        "        prediction[idx][1] = bbox[\"x_center\"]\n",
        "        # set y_center\n",
        "        prediction[idx][2] = bbox[\"y_center\"]\n",
        "        # set width\n",
        "        prediction[idx][3] = bbox[\"width\"]\n",
        "        # set height\n",
        "        prediction[idx][4] = bbox[\"height\"]\n",
        "        # set one hot encoded value of the class\n",
        "        # read the class value\n",
        "        class_value = int(bbox[\"class_value\"])\n",
        "        # set the cell corresponding to class value to 1\n",
        "        prediction[idx][5 + class_value] = 1\n",
        "        if debug == True:\n",
        "            print(f\"current bbox is {bbox}\")\n",
        "            print(f\"mapped prediction is {prediction[idx]}\")\n",
        "\n",
        "    return prediction\n",
        "# Generate Training Example\n",
        "# helper map function to map 28x28x1 image and its class to 100x100x1 canvas and prediction object\n",
        "\n",
        "\n",
        "def generate_training_example(x, y, debug=False):\n",
        "    \"\"\"\n",
        "    Generate a training example by placing digits on a canvas and creating the corresponding prediction object.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Input digit image(s).\n",
        "        y (np.ndarray): Corresponding class label(s).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, prediction) where canvas is the composed image and prediction is the label array.\n",
        "    \"\"\"\n",
        "    pixels = x.reshape(-1, 28, 28, 1)\n",
        "    class_values = y.reshape(-1, 1)\n",
        "    # step 1: sample additional digits\n",
        "    if num_of_digits - 1 > 0:\n",
        "        sample_pixels, sample_values = sample_base_digits(num_of_digits - 1)\n",
        "        pixels = np.concatenate((pixels, sample_pixels))\n",
        "        class_values = np.concatenate((class_values, sample_values), axis=0)\n",
        "\n",
        "    # step 2: augment digits\n",
        "    pixels = augment_digits(pixels, debug=debug)\n",
        "\n",
        "    # step 3: calculate bounding box\n",
        "    class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n",
        "\n",
        "    # step 4: create blank canvas and prediction\n",
        "    canvas = create_blank_canvas()\n",
        "    prediction = create_prediction_object()\n",
        "\n",
        "    # step 5: place digit on canvas\n",
        "    canvas, class_bbox = place_digit_on_canvas(\n",
        "        canvas, pixels, class_with_bbox, debug=debug)\n",
        "\n",
        "    # step 6: translate bbox to prediction object\n",
        "    prediction = translate_bbox_to_prediction(\n",
        "        class_bbox, prediction, debug=debug)\n",
        "\n",
        "    # print(f\"Final canvas shape {canvas.shape}, final prediction shape {prediction.shape}\")\n",
        "    return (canvas, prediction)\n"
      ],
      "metadata": {
        "id": "ZzOBbarPERb0"
      },
      "id": "ZzOBbarPERb0",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = get_sample_indices(x_train, 1)\n",
        "test_x = x_train[sample_indices]\n",
        "test_y = y_train[sample_indices]\n",
        "canvas, prediction = generate_training_example(test_x,test_y,debug=True)"
      ],
      "metadata": {
        "id": "noLqDnfCEtSQ",
        "outputId": "de746332-8c6e-4cd1-8f31-3d6b9a0e818a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "noLqDnfCEtSQ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqEAAAERCAYAAABLgH62AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEpJJREFUeJzt3V1s1mfdB/Df3TfaUmAFhLIxViCKCIzhnmziwkKYMfNAErLEmO3A15gdTBMT9cQTo2aHHuqhRt10O1KnomYzU5u9MF0nSNiQbtCuMKBdee1KX+4+B2w8z/I8u67Olusu3J9PsqS5v//+/79pt/vL1fXXyvT09HQAAEBBDbUeAACA+qOEAgBQnBIKAEBxSigAAMUpoQAAFKeEAgBQnBIKAEBxSigAAMUpoQAAFKeEAgBQnBJaJ/7973/HZz/72Vi9enW0t7fHhz/84fjud78bo6OjtR4NgDr0+c9/PiqVynv+NTg4WOsRucoqfnf89W9gYCBuvfXWWLJkSTz44IOxdOnSePbZZ+MnP/lJ7N69O37961/XekQA6syzzz4bfX1973pteno6Hnzwweju7o6DBw/WaDJKaar1AFx9P/vZz+LMmTPR09MTmzZtioiIr3zlK1GtVuOnP/1pjIyMRGdnZ42nBKCebN++PbZv3/6u13p6emJ0dDQeeOCBGk1FSb4dXwfOnTsXERErV6581+urVq2KhoaGaGlpqcVYAPAujz76aFQqlbj//vtrPQoFKKF1YOfOnRER8aUvfSleeumlGBgYiMceeyx+9KMfxde+9rVYuHBhbQcEoO5NTEzE448/Hh//+Meju7u71uNQgG/H14F77703vve978XDDz8cv/nNb668/u1vfzu+//3v13AyALjsj3/8YwwPD/tWfB1RQutEd3d33H333XHffffFsmXL4ne/+108/PDD0dXVFQ899FCtxwOgzj366KPR3Nwcn/nMZ2o9CoX46fg68Mtf/jK++MUvxuHDh2P16tVXXv/CF74Qjz/+ePT398eyZctqOCEA9ezChQuxcuXK2LVrVzzxxBO1HodC/DehdeCHP/xhbNu27V0FNCJi9+7dMTo6Gr29vTWaDAAifvWrX/mp+DqkhNaBkydPxtTU1P95fWJiIiIiJicnS48EAFc88sgj0dHREbt37671KBSkhNaBD33oQ9Hb2xuHDx9+1+u/+MUvoqGhIW699dYaTQZAvTt9+nQ8+eSTsWfPnmhvb6/1OBTkB5PqwDe/+c3Yu3dv7NixIx566KFYtmxZ/Pa3v429e/fGl7/85bjxxhtrPSIAdeqxxx6LyclJ34qvQ34wqU7s27cvvvOd70Rvb28MDw/H2rVr43Of+1x861vfiqYmfxYBoDa2b98er776ahw/fjwaGxtrPQ4FKaEAABTnvwkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAipvxlvJKpXI15wD+Q1b9Us+8N8H8NJP3JiehAAAUp4QCAFDcvP6l4TdHxPJaDwEAwJybtyX05og4FBELaz0IAABzbt6W0OVxuYA+EJfLKPD/e7HWAwDAf2DeltB3HIqI3loPAQDAnPKDSQAAFKeEAgBQnBIKAEBxSigAAMUpoQAAFKeEAgBQnBIKAEBxSigAAMUpoQAAFDfvf2MSXC2VSiV7TWNj46zuMTU1lcyr1Wp2BgC4HjkJBQCgOCUUAIDilFAAAIpTQgEAKE4JBQCgOCUUAIDilFAAAIpTQgEAKM6yemqioSH/55/ZLnLv6OhI5s3Nzdl7dHV1JfPc38fJkyeT+cjISHaG3MJ7ALgWOQkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAirMntA5VKpVZ36OtrS2ZL168eFafHxHR2to6q7yzszOZr1mzJjvD1q1bk/mFCxeS+dNPP53Mn3vuuewMo6Oj2WsA4FrjJBQAgOKUUAAAivPteACuGzdHxPJaDwFzaCgiBmo9xFWihAJwXbg5Ig5FxMJaDwJz6GJEbIzrs4gqoQBcF5bH5QL6QFwuo3Ct2xgRj8Tlr20lFADmuUMR0VvrIYAsP5gEAEBxTkKvQc3Nzck8t4NzJjs6V6xYkcxzOzZXr149q/tHRFSr1WR+/vz5ZJ7bVbp58+bsDN3d3ck8N+PY2FgyP3LkSHaGoaGh7DUAcK1xEgoAQHFKKAAAxSmhAAAUp4QCAFCcEgoAQHFKKAAAxSmhAAAUZ09oYY2NjdlrWltbk/n69euT+bp165L5THZ05p6xaNGiZL506dJkPjk5mZ3hH//4RzI/ceJEMh8cHEzmw8PD2Rk6OjqS+YYNG5J5bl/qBz/4wewMM7kGAK41TkIBAChOCQUAoDglFACA4pRQAACKU0IBAChOCQUAoDglFACA4pRQAACKs6x+jjU3NyfzVatWZe9x2223JfM77rgjmXd1dSXzmSzMP3fuXDI/depUMn/ttdeSeV9fX3aGl19+OZkPDQ1l75Fyyy23ZK/5xCc+kcxzv1ggt7T/7rvvzs7Q1OQfU3gvbW1tVz5urVYjLl2K1gULoq3h8hlLtVpNfn4uj4iYmppK5tPT07PKoV45CQUAoDglFACA4pRQAACKU0IBAChOCQUAoDglFACA4pRQAACKs4Bwji1evDiZ79y5M3uP3bt3J/Pcbsrcjs6DBw9mZzhy5EgyHx4eTuYjIyOzyiMixsfHk/mCBQuS+bp165L5TP6/WLNmTTLP7VxdsWJFMj9+/Hh2hv7+/uw1UK+eeeaZKx+3HToUcf/98eMf/zje2rgxIiL279+f/PwTJ05kn/HXv/41mb/++uvJPLfT+OzZs9kZJiYmknluF+lc7Eu175S55iQUAIDilFAAAIpTQgEAKE4JBQCgOCUUAIDilFAAAIpTQgEAKM6e0Dm2aNGiZL5r167sPe64445knttZ94c//CGZv/DCC9kZLl68mMxzOzynpqaSeXNzc3aGpUuXJvNNmzYl83vuuSeZb9++PTtDS0tLMn/55ZeT+b59+5J5T09Pdoa+vr5k/oMf/CB7D7herV279srHDW/v27zxxhuj+vbrW7ZsmfUzvvGNb8zq83O7mwcGBrL3GB0dTeZjY2PJPLcf+s9//nN2htw9zp07l8xzu0jnYg/pbPelUpaTUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4e0Lfp8bGxmS+cOHCZD6TPWi5HZuTk5PJPLdzbmhoKDtDbpdaU1P6Syf3v8Pq1auzM2zbti2Z5/Z8fuQjH0nm58+fz85w4MCBZL5///5kntvJOpP9gDOZE+pVb2/vlY87Dh+O/4qIV155JS68/e/arVu3Jj+/ra0t+4yGhvR5TS7/37tM/5N8JiqVSjLfs2dPMv/617+efcapU6eS+YULF5J57r0nt3c5IqK9vT2Z//Of/0zmf/rTn5L5kSNHsjNMTExkr2FmnIQCAFCcEgoAQHFKKAAAxSmhAAAUp4QCAFCcEgoAQHFKKAAAxSmhAAAUZ1n9+5RbJD86OprMjx07ln3GxYsXk/lNN92UzDds2JDMT5w4kZ0ht/h42bJlybyrqyuZ52aMiLj99tuT+ZIlS5J5bmlxbpF8RMSrr76azI8ePZrMh4eHk3nu6yliZr/gAOrVpz71qSsf31atxrMR8dWvfjVeenuBfG4R/Cc/+cnsMzZu3JjM77rrrmS+YsWKZN7S0pKdIfcLQpqbm5N5bqF+a2trdoZbbrklmef+XZV7X9m5c2d2hpzXX389med++cfx48ezzzh79uz7mon35iQUAIDilFAAAIpTQgEAKE4JBQCgOCUUAIDilFAAAIpTQgEAKM6e0Dl25syZZP73v/89e4+Pfexjyfzmm29O5rlda21tbdkZcjvlcntC29vbk3m1Ws3O8NprryXz3D633t7eZD44OJid4cKFC8n80qVLydyOT7i6xsbGrnz8zj+Nl8bH451XDx06lPz8XD4XcnuRb7jhhuw9Ojo6knlul2l3d3cyX758eXaGpUuXJvOhoaFkntszOpMZcntfc3u0P/rRjybzmeyPzn3NTE5OZu/BZU5CAQAoTgkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOLqak9opVLJXtPa2prMFy9enMxz+zPPnz+fnSG3WzK3B+3ee+9N5tu2bcvO0NfXl8yHh4eTeX9/fzJ/6aWXsjOcPHkymZ86dSqZj4yMJPOpqansDPZ8ArP1yiuvXPVnPPXUU1f9GVfbkiVLstf8/Oc/T+Y7duyY1Qwz6QnMHSehAAAUp4QCAFCcEgoAQHFKKAAAxSmhAAAUp4QCAFCcEgoAQHF1tSe0s7Mze82mTZuS+ebNm5P5+vXrk/m6deuyM3R3dyfz3H7LVatWJfNqtZqd4Yknnkjmzz//fDI/duxYMj99+nR2hosXLybzmez5BODacPbs2ew1Y2NjyTz3/nbnnXcm8+eeey47Q26P9uTkZPYeXOYkFACA4pRQAACKU0IBAChOCQUAoDglFACA4pRQAACKU0IBAChOCQUAoLi6Wla/ZcuW7DV79uxJ5rfffnsyv+mmm5L5TJbYDgwMJPMTJ04k87Vr1ybz/v7+7AwHDhxI5rmFvm+99Vb2GTnT09OzvgcAvGN8fDyZT0xMFJqECCehAADUgBIKAEBxSigAAMUpoQAAFKeEAgBQnBIKAEBxSigAAMXV1Z7QO++8M3vNPffck8xvuOGGZP7mm28m82eeeSY7w759+5L54sWLk/l9992XzGeyf7OpKf2lUalUZv0MAJhLufeeN954I5kPDQ1lnzE1NfW+ZuK9OQkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAiruu9oR2d3fPKo+IaG9vT+YHDhxI5k899VQy7+npyc5w9OjRZN7V1ZXMP/3pT8/q82fCHlAA5lJu//RMr0kZHBxM5qdOncreY3JyclYz8D+chAIAUJwSCgBAcUooAADFKaEAABSnhAIAUJwSCgBAcUooAADFXVd7Qjs7O5N5a2tr9h7Hjh1L5k8//XQy37t3bzLP7SiLiKhWq8k89/eZc+nSpew14+PjydyeNADmm9we0dnuGWVuOQkFAKA4JRQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAilNCAQAo7rpaVj82NpbMR0dHs/c4ffp0Ms8taV+5cmUyb2lpyc7Q1taWzHfs2JHMFy1alMz/9a9/ZWcYHh5O5tPT09l7AMBMzcUi+dwve8nl3tvKchIKAEBxSigAAMUpoQAAFKeEAgBQnBIKAEBxSigAAMUpoQAAFHdN7QnN7RAbGhpK5vv3788+46677krmW7ZsSebr169P5oODg9kZcntCt27dOqtnPPnkk9kZ+vv7k/nU1FT2HgDwjqamdOXIvf9GRKxZsyaZnz17NpkfPHgwmR8/fjw7Q25fODPnJBQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAilNCAQAo7praEzo9PZ3M33zzzWT+t7/9LfuM3B6z3J7Qzs7OZD42NpadoaEh/WeDvr6+ZP78888n856enuwMJ0+eTObVajV7DwB4R27X9wc+8IHsPXJ7tHM7PEdHR5P5+Ph4dgbmjpNQAACKU0IBAChOCQUAoDglFACA4pRQAACKU0IBAChOCQUAoLhrak9oztTUVDI/evRo9h6///3vk/mLL76YzBcsWJDMcztAI/K71IaHh5P54OBgMj9z5kx2BntAAZhLjY2NyXzTpk3Ze3R0dCTz06dPJ/PcnlDvfWU5CQUAoDglFACA4pRQAACKU0IBAChOCQUAoDglFACA4pRQAACKU0IBACjuulpWn/PWW29lr8kttM/lLS0tyXwmy+pzS/cnJyez90iZnp6e1ecDwPvV1JSuHBs2bMjeo729PZm/8MILyTz3Hj4xMZGdgbnjJBQAgOKUUAAAilNCAQAoTgkFAKA4JRQAgOKUUAAAilNCAQAorq72hJYwPj5e6xEAoLgFCxYk882bNyfzbdu2ZZ+xaNGiZF6pVJK5Pdnzi5NQAACKU0IBAChOCQUAoDglFACA4pRQAACKU0IBAChOCQUAoDh7QgGAWcvt6Gxubk7mTU0qSb1xEgoAQHFKKAAAxc37s++NtR4AAIA5N29L6FBEXIyIR2o9CAAAc27eltCBuHwKurzWg8A892KtBwCA/8C8LaERl4voQK2HAABgzvnBJAAAipvXJ6EAQH2Ynp6eF/egHCehAAAUp4QCAFCcEgoAQHFKKAAAxSmhAAAUp4QCAFCcEgoAQHFKKAAAxVlWDwDM2tTUVDJ/4403kvlf/vKX7DN27dqVzI8ePZrMR0ZGknm1Ws3OwNxxEgoAQHFKKAAAxSmhAAAUp4QCAFCcEgoAQHFKKAAAxSmhAAAUV5menp6u9RAAANQXJ6EAABSnhAIAUJwSCgBAcUooAADFKaEAABSnhAIAUJwSCgBAcUooAADFKaEAABT33/GwI4fVBXOrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHi5JREFUeJzt3dluJUl62PH/FxG5nI1k7VW91PSMdsEybMMLDBvwlS3A94YvfDnP4It5Aj2FHsDwEwgy4AvDN3MhG4YljUaz9PRML1VkcTtLbhHx+SIPq7pbs8hdC1mV3w9gk2CdIrOI/jMyIzMjRVUVY8w7zV33BhhjXj8L3ZgJsNCNmQAL3ZgJsNCNmQAL3ZgJsNCNmQAL3ZgJsNCNmYDw933hv3X/4XVuhzHmG/rz/F9/42tsRDdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAsJ1b4C5Pvd0xyHddW/GS7ug4ljm170ZN5qFPlH3dMef8mfMSNe9KS+twfNd/WOL/dew0CfqkI4ZiT/hn/MJq+venG/sMWu+x/c5pOMYC/1XsdAn7hNW/EhuXfdmfHN63RvwdrDJOGMmwEI3ZgJs132KnEMkQASCA/z4edXx7epj886w0CdIioD4EiK4UOJcieY8xp3z+LGIRf8OsdAnqLydmR0k+AGkh4GhKskRNINrFddnyIqL+xE+pv0vAR1fpPyS+NUmxm4wC31qnHLv37R8+5+s4T9D8x9XnD64T7cpyJ1n/hnUT4XQZMrLiAwJv26RIaFNC30PKaNpH7/CGLmiV7v+Xz4EMDeChT5B9Z3IweMegOIDofhQSJeB3HlKJ1ROKHaJqgTpBUKAHnLhya1Hk4zH98/jBk06tp0yJEX2hwG/fPQ3b5qFPjECPAqX/G71FID/dP8H/PtvPyX1Y8Dh2xB2gouK7zJtgp9sPevo+GK35LStGZKnjQU5C0Py5OzYXVT0u4LqVJk9VVwTKU62SB/RrocYX/xigF8ev80JvDYW+gQd+Yb3whqAf334Of7+2a987UX2zNoFx6lg2AlNVyHJ0Q8lWR1pKIjJs/1iRXNZs/g5FLUSLnukE1zTk/OLXXvJytWu/peNvwCuJgAt9FfNQp+gz4ZD/rp9wIeccJGVkCMF4EUQQBjfO4SCzPuh49AlVrNTfqfoGLKnS4GswpA9KTuaoqZvAu5Wxj9KxF1md5yIvdK2ShyUXSpoYkEXPRdNTc6CcwqisPPQOsI2Uz0b5wbY7GCIdsz/CljoE6MIP+7ustiU/Dv+kpOcCannwAkl4HA4BC9CwFNJ5neLdhxjqx2o+8p4+/zjR4IqbPPAOvVcpoIf90fscsFpXLDLJcf9iifdnGE34+zkLkP2hJBwTtGnJZyUzJ4kyr/qkHWH/OIJbBs0JtC3/+ab62ShT9Bw7Gh/Ol4k89nPjoiLimrh8YUgBbgA8xC5VfYEydQy4CXjUZyk/ajPftQHEETGr51dJPsE2XHX97SqVMnT5sS8dKwqWJc9C4SYHSFkRJSGirYoyBpIn1VkCYQy4LqwP7d/LT+qd4aFPjUZ2v8B678Yx+L/9l/+MT/94BGXv1cyHHn09oAeRj5anfIvHnzMQWj4VvGMhYvUkigkE4BCxl37SsY9ACeCQwgiHFCw8MIt15BpSWxRFZJ6kjqyOvoHHlRwMu5lfPxh4JM28KOP3+e/t/+I4dhz9GyGT4wX8MR4vT+3t5yFPkH5QknrMfSzz2u+yEvOVhVd48nDgHYDRdfypKjoisyqrug9VJIp3T50B06g8n4M3Y2xi4ATARSR/civAEIlCe8SDnBFRoBCFEFJRUGqPM8OjsgriI0jVx6KAN5uyXhZFvoEaYrkPJ5H54sT3IWweuJZVIJWGYpMKjv+elZRVjN++PBDwixA5ZBCIAhaCFoKw21HLiEvM7lSQpkpykjhM4uqQ4B1V9FHz+1Zw+3ZjpkbuFNuWLiB3y6fceg6DiTzh2XgZNWT3uvpi5L0YEHSClJE2h7VPJ6bN//fLPQpurqUFWC9xW1K6uOvvkSBpxRQV/Cd92E5J80CqfLkUoi1I82E7SMhzmC4nUiLTFFHqnlPFSK3Zg0iyrPtgmYoeLS84P18yUFo+ECFI99yxz+jlMjSCQuBgyqRDzOpV9JBRd4UuLIA58btNt+IhW5+vZjg2Tmst0jh8MEhXnCFkALwc0ELSPNMLhUfMr5IBJ+RYtxrmPUdIXlc1bKtO1IBzFecLOf0/yxwcL/jD2Zn/E59TqsJ0HGfH168Ny/FQje/Xozw9BkwTr4p+5l2gQKor1735SDHg/Lnn5rD2K7ABtiWBeerI+S+429WD/FBGW7/JffLE3aaXpyy+/L0vnkpFrr5zfTFB/J3Pvebfb1TRclNhMaTh4DPnpgdGajCwP3Fmk0bkeWcYekpKof3468ZTXY+/Zuw6Uzz5sVI3mxImy1dLzSxoM+eBBzNdvzT937GP/zgU/zjxPrDguGoxFXleHONje7fiIVurkceb3Udb3MfDwkU8JKpw0BVDFBALgT1AiKIWOXflIVubpSojiaXtKlEkyAZu8flFbDQzY2ijDfKRHVoHkOXL80RmG/GJuPMjdKmwHG75KJZoI0j7BQZxnXsNNtyVd+UhW5ulC4FjtsF62ZB3jlCo7hex5Vr7FbVb8x23c31+crkmqA63tHWdCVdW0AHoc1IzPur+Sz0b8pCN9dDrpa3GF3NuvdD4Hw95/KyRi6U8jLi24imOI7q5hux0M31ka+8AxhvZ81Czg4SkBT2M+9qB+jfmIVurpWgiCgOJezf29nyV89CN9frS6P6Vy5rt8H7lbLQzTX56ritQELI+0k5/cqrrPqXZaGb6yHsZ93HlDPjxPrVatAo40MgbKb9lbDz6ObNkfGada0K8q0F7r7jcDVQ1T3eKxe5pu0r3NrjLwW3GZBthD5CThb9S7DQzZsjgnhPPlww/MEHlA+U9+5/zNHqghASn6cDLnZz/NOC8qnDH+/wz1ryrrXbU1+ShW7eoP2uunfkOqB1pgiJ2kVEIOr4MAiJgkTGC2VisnXiXgEL3bw5++PyXAj9SvArmJU9K9/gJdNpIGaP6xkvex3i+GahvzSbjDNvloA6yKWQS/A+U7o4Lvms4wMbJTM+o+3Lz2M3L8VCN2+MiCDOkWtHe1fp72YWdcMtvyOp4+mw4nKYwSC4gRfT8DYJ99IsdPPmiIBz5FLoDyEeKlXZs3QtWYXLNKNJ5f74fBzV1SJ/JewY3bw5bgzdl0q56qlWHfOiYy49l23Njy/vsT1f4i/BbRLSJ0gJe/Day7PQzZsjDgkeX8PsTsPiTstB1bByLc82S/7XFx9SP3EcHWf8eUTaiMY4LjhhXoqFbl6//YUyVAV5UVMs4O5sy1HVsPADtYzPYYvZkbKMk29Xg7g1/kpY6Oa1kyKA96S7B6QP7nDvo3P+6OHfcvvWhm/XG+65zELUHtbwGtlknHm9hHGXXRyUgTwrCbVwVDXcKhtmLlGK4uTqInfzOljo5s0QQZ2gQcBDIZFSIl4U4cVqM1ePWRaL/pWy0M1rJi+O0QXUCeIULxkvGcfX9tb1a+/NK2GhmzdOdVy/Xb/0hJbxAxvHXxcL3VyDF5GPy8HZanCvm826m9dsfwmrKhIzvkt0refT7RFNGXjkWgjQDR5ah3SK9AmJebzeHbsE9lWw0M3rpTx/oKLfRdxpx9lxzZ//4vdZ7RrC+5nH81NO13PcswJ3FvHrDr/tUbtF9ZWx0M3rp/ubU4aIND1p41ifFOSUOCtLlouS7lTw5wm/jkg3jKvKpP2dazaivzQLfeIes379M9xxP+v+7BQuA/lzGH4o+CLTVC2f+ILD7af8y905vssUl3mMfOjHdd1/TeiPWb/mjX83WOgTdUFFg+d7fP/1f7OrqfVu/7YBTl7dl2/wXFC9ui/4DrLQJ+pY5nxX/5hDuuvelJd2QcWxzK97M240C33CjmXOMRbIFNh5dGMmwEI3ZgIsdGMmwEI3ZgIsdGMmwEI3ZgIsdGMmwEI3ZgIsdGMmwEI3ZgIsdGMmwEI3ZgIsdGMm4I3evXZPd+/EbZG/id02aW6aNxb6Pd3xp/wZM9Kb+pbXpsHzXf1ji93cGG8s9EM6ZiT+hH/OJ6ze1Ld94x6z5nt8n0M6u9fb3BhvfOGJT1jxI7n1pr/ty5GvPfnv1y1WaOsYmhvIVpj5VWT/H+eQq0cKIYCiaXzsgC1FbN4WFvrXXQW+H8XFOfB+/2cyPogA0Kzj0wBtKWLzFphm6Fe74l+P1HtcVY1/7j04hy5rdFY9/3sSE7JpkCGhTYP2/fMnkRhzU1noX/60d0hZgnNQhDH0wxWsZuPLRaCPOBWkH8jDADGiOVvo5kZ7t0MXQfYjM8FDCOAEfHj+CN/n752Qy0BcVGhwpLknF0K8V5IOHWSBLPhGqOvF+HihNOBSehG7MTfUuxf6l2fIRcYROgRkVsOsBu/QqhhH6yqAFzQ41DtS5YhLTyqF9rYj1dA9Sgy38/i0kd5RrIWjqqS4yNS7Ft8MAGiM4/e0gd3cQG936FdROwdOEHHPP1bvwDvSvIIyoPMSnY+jdZ6NI7sWfj+aO9Q5UinEhSNXwO2EzJTFUYc7HNAoMIwTc/mwYlChnAeoCtCMxDCO6jawmxvo7Q1dBAkBxCF1iYRiPK4uAloG8sEMLTzdnYI48wwrx7By5BLiEtQBCKIgUXARcgGpBlcnZg931POBDw9PubdYo+rIKqzXM/5m+JDNaUV5MqPaCrJtxq8V434X/5p/NsZ8zdsZusj+uDuAd1CXaFGMI3dVoFUgH1bk0hNvFcSZIx5AXEEuYVjoeBotjcfdrgPfC7lQUqlQAKUiRaYsI/NqQFAcOu7CzzOxg1z78TAgRRgKtAda+Q0bb8yb9/aELjKO3sEjIaB1Sb57iNYFw62SOPek2pHmjlwL/R1BSyEvElomwjxSzRPZQfBCjkI8rtHeE3YQdvpiYq7wxHZOritOHw+oFx6UF3xn/oxZ7CkPWtCC/n5B0xYQaxhWyMUO/evPmMDl/OYt81aFLm48vy1lAXUFt1borGC4V9CvPHEOw0JIM6W7m9BScVVGQsbXEV/1OATJgvaeLEpO4HsILbC/BiZ7YdCSVCm7OxV+qLlXXHLkdwyFx9cR+kRcFfQH/vkmes84q2/MDXPzQ3fjBJvMZ0hVElcl3e0ZaRHovlWQ5p50K5PnGS0VKiCAVOOpMwaHDo7YCp3zyCC4HbhWWPwiI+uBsI6ETdzPmAtaCP3tCp0JvC/0ySMKc9cx9yXzqqeeDaS7nq26/dV0UHgl+1/9TzHmutzs0K/Og3uPrJbIakG6X7P71oJhJWw+UtJc8asBVyecZJxXSA7aAMmhzRh3HAJ5EMIO6hPwjbL4tKPYJtxlg9t0z7+nlp5+K+RlQH9X6FNAVFlKR+sLFlXHLPdsHhS084AK4JRaId/sn6iZqJv9v6UIFAUSAvGgJN8pifcd8nDAL2B2mMmVEgMkFWTwEMENQrlxEAXZKTKA6zK+A99kymcJ32X8RY80CWkHJCU0eLQKpNozHDjSgbCcD8zKHQvf4SXjXWYWehZFT1PUaFDIgiRB0rjrb8xNc7NDdw43n6Gzit23lzSPl4QPOorfW1OWmaN6vFjl6eWSTVsh5x73LOAbWB4rrleKTcb1Gd8MuF1EugF3sYM4Bk7K42k678l1QbqzYFgFLn67IB8pHz7c8K2DJzwqLykkUbuB+/MNeMd2VrHpK9zO4Tcev7OVuczNdLNDF8ZLVwtPnjvSoVAdRlYHOySMs+Q5O1wE1zjcRvAXStgp4TzjeiVsIm5Q/G7ANQP0A7Ltx1tMY0JR1IGWnjzzDCtPOhDcQUIOMmU9MPMDIsoulzS5JCWPJhnPvw/g23HW3neK2Dl0cwPd7NCdQ+sKXVTo+5n8Ow2P73/Bv3rwQ87jjP95+ttcNDP8z0qWTwL15y31zxtcl3CXHRIz0kWIaYx62J/30vFry6xGikB/b85we0Z317P5TqBYDbz3W6fMVx33ji6YuZ7TuOCz7ojLdsb//fwD1rua/LOa1TNPeZ6oTgbcsx43XOtPzJhf6maHLjKO6GVAV4rejhwcbvjO4pgn7QGaoesD4dJTngjVk8Tssxb6CE0DKaH9MC4UkTOaM+IdFMW4t1AEqEvSQc1wZ0Z/D9pHiiyV5d0dh4sds7LHi3KRKj5rj7hsZhyvVzSbiuWpUJ8I1ZkyO45wmZBkB+nm5rm5oe9Pq2npyaUfd68VsgpRHZHxklTNQtgmijPBn7foxXo/eveQdX/KTCF4nC+gLNHVHK0D3cMFaRFoPwh0DzPlrY73H20pqkh0jouh5mJXQ4RuW7E7WzBsA/OPHfUmUX/aU5xF/LpHzltoO8h2tYy5eW5m6DLeoKLekffXrqvPQCbjSDq+vQg9U51H5KIlX67H4+/9baPix/vKJQSkLNFFjd49Ii0Kdh9V9Iee7oNI/zAxW7W8f/8UEWUTK3b9jOOLJeebOf7MU3xeUKyVwx8PFJuEP97i1y3atuiuGfccbEQ3N9DNDB1QzZAVSRliRjshNYGhDzS5JOKZFwNd1SGLGcPKExYFoa5h/3cRoCrHGfVZic4K8qJkuFeQFp54V8kHCb+KVFUkI5xt50gW+nVJ7h1yVlCvhXCeKb5oCduMf9YiuwibBm27/eFB2v9ysdDNzXMzQ79amiklXDeQ20C+rBnOSnbzGWdpQUPJncWOUpRn92o2m4J5P6e8ONzfPabgHHk1Q6uCuCoYVgXDUtg99OS5Eh/36DIzm/dU9UDfBX765B7SOOqfe/zWUZ5k5peKv2gpn2yhG+B0DcMAQySltN/ePDZunZsb6GaGfkUVYkaGhHTgdkLqAttYMYhn5nu0cJzNEmkJaeWJq/LFRStOSAclWgWGZWBYeuJCSLWQK1Av41VtUZDWITuHnntoBM5ANhl/HvGXGX854DY99AO5HyPXvB/FbRkpc8Pd7NBzRnbj3Sb1FxVaVDTVnB9++JB52fFBfYYWwvGHK3b1nLQs6A9uj4uzMt5znpaOXAqpEnIl4+cKwIFuC2gCQ1tCp4Rz5e6nim8ixRc7XJOQbYc0PdIN5F03zt7H+GI33Ro3b4GbG/rVTWApIUPE75RiMx6nXwwzxGdWvsP7TLnsx93oLhC7AlV5HvqwGBeUyIHxclUFyfs6OwEVdC3kLchpov6ixzcJf9zjuh5tWrTr0RjHFV/BRnDz1rk5oX/pBhaCH9d6KwJ6uESrAufCeOXZMHZWSeRbxRkL19HeC3x7eczmaMHlgxXbWPJkt6LPnqyBpA7pBTc4fAPVmeKiEpo0XjW3HvCbiN/0+Ce7cSnnbUtOCR3ieD7edtHNW+xmhH619lsISFEgVYks5mgRyAdzcukR5/H9GKgilJJ45C+5U2zxtxMfpRM+u33EJw/vctrNOT3zDEOJbiAPHnfpcTsIO5g/VXyrVGcJ3yXcusFtWrRpyReXkDKaE2rrtZt3xPWHfrVSq/fo4Zy0qMjzgnRUkStHuhXQSnCHETePLN7b8l59zt1yg5OMAoduoARiKNnlNbFz6CaQ2pLixFM0jnCRCeeZsEmUT3pcl5HLDrqENh25HaDvx9H7KnCL3Lwjrj9055DFHKkq+vcPGe4v6Y4cu4eeVCv9vQx15ujWlvmi4/bBM/7w8DNu+R3eJTLwwDeUoaWWDJLIG0d+VjJc1qw+hvIMqrOe8mw/sXa2GWfNt824JntK4wy6YnGbd9INCF3QukDnJfEw0N/2cJSp73RQZ1a3O1yVODhomM066mpgUE+TC07jnNYV4AdwmVbHi2n6oYBG8FshXAyE84S76JB1j3Q90vZoTGPsKdopMvPOu/bQNTj69w/Jd5asf0/YfpS5e7Tmtx6dsCpbPlo+ZRE6sh9PjV3EOT9u7iMoP5SHVC7y+7MveFhe8qP2Ln+1e8TZxQHh88DiJDH74el4oUvTo02HqiJxvMhlnGDLdorMvPOuP3Qn5HlBXJXkWxFuR8qjjqNbGw7LHe/Nzlm4ljaX9OrZxJrLbkZW4VyU0iXuFZfMfMtFqjmPM7Z9hWsF34Bf9/jLltz3aDcuF/W8axvFzURce+g5wO6hED+Ex4+PefitY+7WGz5anuBF6TSwGw74m/MHfLY9ormYsXm6IiNoqRRFYvatgfXtmtNhQcahIvsnsDCej/9VT081ZiKuPXT10B3BcA/u37vgj+59xqHf8TCcM2jgk+EOl2nGj9b3+cHpA/xJQfHzAkSINYQqcnhrQ1wKQ/bjZDmgsn8ai+xXg7X9czNh1xd6EcbZ9lsluhJYRoqqZ+46mlTwg/4R7VDyi2d32bQ1zc+XLE4c4Vmk+GxAnZBWBW6eke+MkVcSOSwbzgvhWDIJW3vZGLjO0OsKuX8XuVvAHYfeGqhnHYd+x8fDXf7i8jGbzZzP/vYe7UXF8qfK0RMlnLcUTzZo4Yl3lsgB8A+ELgVuVVs+rM+oS+UnkuktdGOA6wzdObQIUHokgDil1cBFnHPZzliv5+zWNXrq8eeCPxvwFwm3GcZ14GDcGxdhfyCOIuOxOzL+WeZqWRrbczeTdm2ha3DkZUlaelyZ8UXm8+GI/71Vnpzc4hc/eYCeOQ7+TyactYSna/xlCzGP5799RZwJzD05jDexDOrZpYo2FeP67lFhSOgwPF9xxpgpur4RXQC/fyrqfo3kNhdcDHM2XU23K3A7wW17wk7HS1b7BCgqQvZCLgUqAa84ySR1NKmgT2F8qMLViG6Xs5qJu77QsyJDRoZM6h197zhvZ6gTNl2NZkFFGJYB1BHcEresx0UigzAceC5/KyBHyvuHPbeKHeftjI/PbxNPa/JaKLYJGfL+2vVr+5cac+2uLXRRxoUUo5KTJ0VohgLplT4WkMfj7lQ7XHIIAmUmByGXjuFAaO853GHEzyNz3/NFOuDnm1v4TWDZCqHP+1Ed1Eo3E3Z9I3rKuF2Lq0rC2YxQB1QCQ5FIBcitHp0LrRf6DtxacK1HA+RCYJlZPmjwy8hWCr5oDlmfLuDTCvc5hNMOfx6Rfthf5mqhm+m6ntBFxqeorFucy4TjBYULaOXplx4tFLnXo1nY3fXkJMhFgN0YuhZKPUs8+GBNUUXWWnK5rWiOl8hPZrjjnuLpBr/uyW0cl2G20M2EXeMxeoK+R1ooLiJaBtKhIy8C6sflnxBwRcZ5SJVHE6hXNCjJC+0QSCpo49FB4MRRPhsoznrYtdAOkMYJPGOm7HpCV0X7gXyxhr5k+dMleipcpsCuW5BXiXhvwIXMrB5wouyioxcZD+4dDE44uVjhslJ9GijOHfXPOo5+tEbWDfLklNzHLy3kaMx0Xd+IroqmiAwOv42oG/CXDnfJOJIvBFcInv3TmQYdH2B4daMKoNmRI3AucCq480xY9+huQC1yY5671tDJGe0H5OQcWW9ZxiX18Yy09MQ7AYJDKg8Cq+RIScYr4RzI1eKOvVI82Y0LPJ7t0GdbdLDIjfmy6717TRVyQnYN9AOFD5S9J84L4i6Mp9KK8QGL/moU39+N5galuFTckAknHW7bkrfNuDyUZovcmC+59ttUUd0vqZzhYo22LVJ43GnAeRmvh3fy/HZT3T8dVVLGNRGJGd315CGOz0DLyWbYjfmaGxA647XoAPsVYGA/gotDimJcQNK5Mfb9c87JeXywoSqqaqvGGPNrXH/ov8rVc833CzdqzmPo+uJzf2f33CI35pe6uaHDfrd+2M+yy1c+b4z5+7vZoV/R5/8xxnwDbzz0x6zf6WYfs77uTTDm73hjoV9Q0eD5Ht9/U9/y2jR4LqiuezOMee6NhX4sc76rf8wh3W9+8VvugopjmV/3Zhjz3BvddT+WOcdYAMa8ae66N8AY8/pZ6MZMgIVuzARY6MZMgIVuzARY6MZMgIVuzARY6MZMgIVuzARY6MZMgKjazd3GvOtsRDdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAix0YybAQjdmAv4fxtX/TNuKzrcAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current bbox is {'x_min': 7, 'x_max': np.int64(32), 'y_min': 69, 'y_max': np.int64(94), 'x_center': 22, 'y_center': 81, 'width': np.int64(25), 'height': np.int64(25), 'class_value': np.uint8(8)}\n",
            "mapped prediction is [ 1. 22. 81. 25. 25.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
            "current bbox is {'x_min': 50, 'x_max': np.int64(72), 'y_min': 38, 'y_max': np.int64(62), 'x_center': 60, 'y_center': 54, 'width': np.int64(22), 'height': np.int64(24), 'class_value': np.uint8(7)}\n",
            "mapped prediction is [ 1. 60. 54. 22. 24.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "bb6G8EUtCZqe"
      },
      "id": "bb6G8EUtCZqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utils"
      ],
      "metadata": {
        "id": "zGOJZdF7TAJz"
      },
      "id": "zGOJZdF7TAJz"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# helper function to convert box values to corner coordinates\n",
        "\n",
        "\n",
        "def convert_boxes_to_corners(box_center_format):\n",
        "    # we'll use the following formulas\n",
        "    # x_min = floor(x_center - (width/2))\n",
        "    # x_max = floor(x_center + (width/2))\n",
        "    # y_min = floor(y_center - (height/2))\n",
        "    # y_max = floor(y_cetner + (height /2))\n",
        "    # calculate true values.\n",
        "    x_min = tf.floor(\n",
        "        box_center_format[:, :, :, 0] - (box_center_format[:, :, :, 2])/2)\n",
        "    x_max = tf.floor(\n",
        "        box_center_format[:, :, :, 0] + (box_center_format[:, :, :, 2])/2)\n",
        "    y_min = tf.floor(\n",
        "        box_center_format[:, :, :, 1] - (box_center_format[:, :, :, 3])/2)\n",
        "    y_max = tf.floor(\n",
        "        box_center_format[:, :, :, 1] + (box_center_format[:, :, :, 3])/2)\n",
        "\n",
        "    coordinates = tf.stack(values=[x_min, y_min, x_max, y_max], axis=3)\n",
        "    return coordinates\n",
        "\n",
        "# helper function to find the intersection box corners from given 2 boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_corners(box_1_corners, box_2_corners):\n",
        "    x_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 0], box_2_corners[:, :, :, 0])\n",
        "    y_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 1], box_2_corners[:, :, :, 1])\n",
        "    x_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 2], box_2_corners[:, :, :, 2])\n",
        "    y_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 3], box_2_corners[:, :, :, 3])\n",
        "    intersection_box_corners = tf.stack(\n",
        "        values=[x_min_for_intersection, y_min_for_intersection, x_max_for_intersection, y_max_for_intersection], axis=3)\n",
        "    return intersection_box_corners\n",
        "\n",
        "# helper function to calculate the area of intersection between two boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_area(intersection_box_corners):\n",
        "    # find the width = x_max - x_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_width = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 2] - intersection_box_corners[:, :, :, 0])\n",
        "    # find the height = y_max - y_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_height = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 3] - intersection_box_corners[:, :, :, 1])\n",
        "    # intersection area = width * height\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    return intersection_area\n",
        "\n",
        "# helper function to calcualte the area of union between two boxes\n",
        "\n",
        "\n",
        "def calculate_union_area(box_1_dimensions, box_2_dimensions, intersection_area):\n",
        "    box_1_area = box_1_dimensions[:, :, :, 0] * box_1_dimensions[:, :, :, 1]\n",
        "    box_2_area = box_2_dimensions[:, :, :, 0] * box_2_dimensions[:, :, :, 1]\n",
        "    union_area = box_1_area + box_2_area - intersection_area\n",
        "    return union_area\n",
        "\n",
        "# helper function to calculate the IOU ration between boxes\n",
        "\n",
        "\n",
        "def calculate_iou(intersection_area, union_area):\n",
        "    iou = intersection_area / (union_area + 1e-8)\n",
        "    return iou\n",
        "\n",
        "# helper function to calculate indices for the grid cells that contain the object\n",
        "\n",
        "\n",
        "def calculate_grid_cell_indices(y_true, y_pred):\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "\n",
        "\n",
        "    # Read the bounding box centers\n",
        "    # Each instance in the bach will have 5 bounding box centers\n",
        "    # select boxes with objectness equal to 1\n",
        "    # objectness_mask = y_true[:, :, 0] == 1.0\n",
        "    # bounding_boxes_with_objects = tf.boolean_mask(y_true, mask=objectness_mask)\n",
        "    # print(f\"bounding_boxes_with_objects.shape : {bounding_boxes_with_objects.shape}\")\n",
        "    bounding_box_centers = y_true[:, :, 1:3]\n",
        "\n",
        "    # TODO:  here we are assuming number of rows and columns in grid is same. Confirm the assumption.\n",
        "    # The general formula is: grid_index = floor(pixel_coordinate * (grid_size / image_size))\n",
        "    # convert each 5 bounding box centers to 5 possible grids for each instance\n",
        "\n",
        "    grid_indices = tf.cast(\n",
        "        tf.floor(bounding_box_centers * tf.cast((x_grid_size / 100),tf.float32)), dtype=tf.int32)\n",
        "\n",
        "    ## grid_indices can be out of bounds if the box centers lie on coordinates that are in multiple of 100\n",
        "    ## so we need to clip them to be maximum of 5 and minimum of 0\n",
        "    grid_indices = tf.clip_by_value(grid_indices, clip_value_min=0, clip_value_max=x_grid_size - 1)\n",
        "\n",
        "    # grid_indices = tf.reshape(grid_indices,shape=(batch_size,-1,2))\n",
        "    # print(f\"grid indices shape {tf.shape(grid_indices)}\")\n",
        "    # print(f\"grid indices {grid_indices}\")\n",
        "\n",
        "\n",
        "    return grid_indices\n",
        "\n",
        "# Helper function to calculate anchor box indices.\n",
        "\n",
        "\n",
        "def calculate_anchorbox_indices(y_true, y_pred, grid_cell_indices):\n",
        "    x_grid_size = y_pred.shape[1]\n",
        "    y_grid_size = y_pred.shape[2]\n",
        "\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(-1, x_grid_size, y_grid_size, 3, 15))\n",
        "    # print(f\"anchor_boxes.shape {anchor_boxes.shape}\")\n",
        "\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    # print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    # calcualte the IOU between anchor boxes and ground truth\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "\n",
        "    # calculate min and max values for ground truth and anchor boxes\n",
        "    y_true_box_corners = convert_boxes_to_corners(\n",
        "        expanded_y_true[:, :, :, 1:5])\n",
        "    y_pred_box_corners = convert_boxes_to_corners(\n",
        "        selected_anchor_boxes[:, :, :, 1:5])\n",
        "    # print(f\"y_true_boxes.shape {y_true_box_corners.shape}\")\n",
        "    # print(f\"y_pred_boxes.shape {y_pred_box_corners.shape}\")\n",
        "\n",
        "    # calculate the intersection coordinates between ground truth and anchor boxes\n",
        "    intersection_box_corners = calculate_intersection_corners(\n",
        "        y_true_box_corners[:, :, :, 0:], y_pred_box_corners[:, :, :, 0:])\n",
        "    # print(f\"intersection_box_corners.shape {intersection_box_corners.shape}\")\n",
        "\n",
        "    # calculate the IOU\n",
        "    # calculate intersection area\n",
        "    intersection_area = calculate_intersection_area(intersection_box_corners)\n",
        "    # print(f\"intersection_area.shape {intersection_area.shape}\")\n",
        "\n",
        "    # calculate union area\n",
        "    # we just need the width and length for union area\n",
        "    union_area = calculate_union_area(\n",
        "        expanded_y_true[:, :, :, 3:5], selected_anchor_boxes[:, :, :, 3:5], intersection_area)\n",
        "    # print(f\"union_area.shape {union_area.shape}\")\n",
        "\n",
        "    # calculate IOU\n",
        "    iou = calculate_iou(intersection_area, union_area)\n",
        "    # print(f\"iou.shape {iou.shape}\")\n",
        "\n",
        "    # select the anchor box based on best iou score\n",
        "    # select the index with highest iou\n",
        "    highest_iou_index = tf.argmax(iou, axis=2, output_type=tf.int32)\n",
        "    # print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "\n",
        "    highest_iou_index = tf.expand_dims(highest_iou_index, axis=2)\n",
        "    # highest_iou_index = tf.reshape(highest_iou_index, shape=(iou.shape[0],iou.shape[1],-1))\n",
        "    # print(f\"expanded highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    return highest_iou_index\n",
        "\n",
        "\n",
        "\n",
        "def calculate_best_anchor_boxes(y_true, y_pred):\n",
        "    # helper function to calculate best anchor boxes\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "    y_grid_size = tf.shape(y_pred)[2]\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # print(\"----- True Values -----\")\n",
        "    # print(f\"y_true.shape {tf.shape(y_true)}\")\n",
        "\n",
        "    # print(\"----- Pred Values -----\")\n",
        "    # print(f\"y_pred.shape {tf.shape(y_pred)}\")\n",
        "\n",
        "    # we have 6x6, each grid cell has 3 anchor box i.e 108 anchor boxes per insantance\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(batch_size, x_grid_size, y_grid_size, 3, 15))\n",
        "    # print(f\"anchor_boxes.shape {tf.shape(anchor_boxes)}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # out of 36 grid cells (per instance) select at most 5 grid cells that have ground truth bounding box\n",
        "    # so out of 108 anchor boxes (per instance) we only need to check 15 anchor boxes\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        params=anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    # print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "    # select the anchor box based on the index\n",
        "    best_anchor_boxes = tf.gather(\n",
        "        selected_anchor_boxes, indices=highest_iou_index, batch_dims=2)\n",
        "    # print(f\"best_anchor_boxes.shape {best_anchor_boxes.shape}\")\n",
        "\n",
        "    return best_anchor_boxes\n",
        "\n",
        "# helper function to split and calculate loss\n",
        "\n",
        "\n",
        "def calculate_loss(predicted_values, true_values):\n",
        "\n",
        "    objectness_mask = true_values[:, :, :, 0] == 1.0\n",
        "\n",
        "    true_values_with_objects = tf.boolean_mask(\n",
        "        true_values, mask=objectness_mask)\n",
        "    predicted_values_with_objects = tf.boolean_mask(\n",
        "        predicted_values, mask=objectness_mask)\n",
        "\n",
        "    # print(f\"true_values_with_objects.shape : {true_values_with_objects.shape}\")\n",
        "    # print(\n",
        "    #     f\"predicted_values_with_objects.shape : {predicted_values_with_objects.shape}\")\n",
        "    # slice the 3 properties that we are tyring to calculate loss against\n",
        "    # predicted values\n",
        "\n",
        "    y_pred_objectness = predicted_values_with_objects[:, 0]\n",
        "    # print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_bounding_box = predicted_values_with_objects[:, 1:5]\n",
        "    # print(f\"y_pred_bounding_box.shape : {y_pred_bounding_box.shape}\")\n",
        "\n",
        "    y_pred_classification = predicted_values_with_objects[:, 5:]\n",
        "    # print(f\"y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = true_values_with_objects[:, 0]\n",
        "    # print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "    y_true_bounding_box = true_values_with_objects[:, 1:5]\n",
        "    # print(f\"y_true_bounding_box.shape : {y_true_bounding_box.shape}\")\n",
        "\n",
        "    y_true_classification = true_values_with_objects[:, 5:]\n",
        "    # print(f\"y_true_classification.shape : {y_true_classification.shape}\")\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    # print(\n",
        "        # f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_classification = tf.keras.activations.softmax(y_pred_classification)\n",
        "    # print(\n",
        "    #     f\"Post Activation y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectness_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "    bounding_box_loss = tf.keras.losses.MeanSquaredError()(\n",
        "        y_true_bounding_box, y_pred_bounding_box)\n",
        "    classification_loss = tf.keras.losses.CategoricalCrossentropy()(\n",
        "        y_true_classification, y_pred_classification)\n",
        "\n",
        "    return objectness_loss, bounding_box_loss, classification_loss\n",
        "\n",
        "# helper function to calculate loss for object less cells\n",
        "\n",
        "\n",
        "def calculate_objectless_loss(y_true, y_pred):\n",
        "    # step 1: create placeholder y_true\n",
        "    batch_size = tf.shape(y_true)[0] # Get batch size dynamically\n",
        "\n",
        "    bounding_box_with_object_mask = y_true[:, :, 0] == 1.0\n",
        "\n",
        "    # step 2: prepare mask for positive values\n",
        "    # hard coding the grid size\n",
        "    positive_mask = tf.fill(dims=(batch_size, 6, 6, 3), value=False) # Adjust shape\n",
        "\n",
        "    # print(f\"positive_mask.shape {positive_mask.shape}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # grid cell indices will have shape (m, 5, 2)\n",
        "    # here 5 is max images and 2 is row and column index\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "\n",
        "    # highest_iou_index = tf.boolean_mask(\n",
        "    #     highest_iou_index, mask=bounding_box_with_object_mask)\n",
        "    # print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    # highest iou index will have shpae (m,5,1)\n",
        "    # here 5 is max images and 1 represents best anchor box in the cell.\n",
        "\n",
        "    # we need to combine both the indices to create tensor of shape (m, row indices, column indices, box index)\n",
        "    combine_update_index = tf.range(batch_size)\n",
        "    # expand dims\n",
        "    combine_update_index = tf.reshape(\n",
        "        combine_update_index, shape=(batch_size, 1, 1))\n",
        "    # combine_update_index = tf.expand_dims(combine_update_index, axis=2)\n",
        "    combine_update_index = tf.tile(\n",
        "        combine_update_index, [1, 5, 1])\n",
        "    combine_update_index = tf.concat(\n",
        "        [combine_update_index, grid_cell_indices, highest_iou_index], axis=2)\n",
        "\n",
        "    combine_update_index = tf.boolean_mask(\n",
        "        combine_update_index, mask=bounding_box_with_object_mask)\n",
        "\n",
        "    # print(f\"combine_update_index.shape : {combine_update_index.shape}\")\n",
        "\n",
        "    positive_mask_shape = tf.shape(positive_mask)\n",
        "    positive_mask = tf.scatter_nd(\n",
        "        indices=combine_update_index,\n",
        "        shape=positive_mask_shape,\n",
        "        updates=tf.fill(dims=(tf.shape(combine_update_index)[0],), value=True))\n",
        "\n",
        "    # select predicted anchor boxes based on negative masked values\n",
        "    negative_mask = tf.logical_not(positive_mask)\n",
        "    # print(f\"negative_mask.shape : {negative_mask.shape}\")\n",
        "\n",
        "    objectless_anchorboxes = tf.boolean_mask(tf.reshape(\n",
        "        y_pred, shape=(batch_size, 6, 6, 3, -1)), mask=negative_mask)\n",
        "    # print(f\"masked_values.shape : {objectless_anchorboxes.shape}\")\n",
        "\n",
        "    y_true_objectless = tf.zeros(\n",
        "        shape=tf.shape(objectless_anchorboxes), dtype=tf.float32)\n",
        "    # print(f\"y_true_objectless.shape {y_true_objectless.shape}\")\n",
        "\n",
        "\n",
        "    y_pred_objectness = objectless_anchorboxes[:, 0]\n",
        "    # print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = y_true_objectless[:, 0]\n",
        "    # print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    # print(\n",
        "    #     f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectless_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "\n",
        "    return objectless_loss\n",
        "\n",
        "# loss function for the model\n",
        "\n",
        "\n",
        "def calculate_model_loss(y_true, y_pred):\n",
        "    # Find best anchor box\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "\n",
        "    print(\"\\n\\n----- Localization Loss -----\")\n",
        "    print(f\"objectness_loss : {objectness_loss}\")\n",
        "    print(f\"bounding_box_loss : {bounding_box_loss}\")\n",
        "    print(f\"classification_loss : {classification_loss}\")\n",
        "\n",
        "    # objectless loss calculation\n",
        "    print(\"\\n\\n----- Calculation Object Less Loss -----\")\n",
        "    objectless_loss = calculate_objectless_loss(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    print(\"\\n\\n----- Object Less Loss -----\")\n",
        "    print(f\"objectless_loss : {objectless_loss}\")\n",
        "\n",
        "    # scale the losses\n",
        "    lambda_objectness = 1\n",
        "    lambda_bounding_box = 0.001\n",
        "    lambda_classification = 1\n",
        "    lambda_objectless = 1\n",
        "\n",
        "    total_loss = (objectness_loss * lambda_objectness) + (bounding_box_loss *\n",
        "                                                          lambda_bounding_box) + (classification_loss * lambda_classification) + (objectless_loss * lambda_objectless)\n",
        "\n",
        "    print(f\"\\n\\nTotal Loss : {total_loss}\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def objectness_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return objectness_loss\n",
        "\n",
        "\n",
        "def bounding_box_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return bounding_box_loss\n",
        "\n",
        "\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return classification_loss"
      ],
      "metadata": {
        "id": "9N1MLKguS9Hh"
      },
      "id": "9N1MLKguS9Hh",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running The Model Manually"
      ],
      "metadata": {
        "id": "ErjI4ehgTVl0"
      },
      "id": "ErjI4ehgTVl0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Model"
      ],
      "metadata": {
        "id": "-Ci-WgQ3TKaC"
      },
      "id": "-Ci-WgQ3TKaC"
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Rescaling(scale=1./255),\n",
        "\n",
        "    ## starting with a larger filter since we are dealing with 100x100x1 image\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    ## rest of the layers are same as our original mnist classifier\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    ## finaly layers to output 6x6x45 grid of predictions\n",
        "    tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear'),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "GtSDiOxNTFtD"
      },
      "id": "GtSDiOxNTFtD",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "aqzrwQnvTPSM",
        "outputId": "87c5bad8-a992-4c82-eff8-aedbba1598b7"
      },
      "id": "aqzrwQnvTPSM",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run in Eager Mode"
      ],
      "metadata": {
        "id": "ZfPbA4gSg0Hp"
      },
      "id": "ZfPbA4gSg0Hp"
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor,shape=(-1,28,28,1))\n",
        "y_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "batch_size = 32\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5,15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=batch_size)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "# Step 1: Get one batch of data from your dataset pipeline\n",
        "# The .take(1) method creates a new dataset with only the first element.\n",
        "one_batch = processed_dataset.take(1)\n",
        "\n",
        "# Step 2: Iterate over the single batch to get the tensors\n",
        "for images, labels in one_batch:\n",
        "\n",
        "    # --- THIS IS YOUR DEBUGGING ZONE ---\n",
        "    # Now you have the concrete tensors for one batch.\n",
        "    # You can inspect them with regular print() and .numpy()\n",
        "\n",
        "    print(\"--- Inspecting Data Before Loss Calculation ---\")\n",
        "    print(\"Shape of images (X_batch):\", images.shape)\n",
        "    print(\"Shape of labels (y_true_batch):\", labels.shape)\n",
        "    print(\"\\nSample y_true label tensor:\\n\", labels.numpy()[0]) # Print the first label in the batch\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Step 3: Manually run the forward pass and gradient calculation\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the model's raw predictions for this batch\n",
        "        y_pred = model(images, training=True)  # Pass the images through the model\n",
        "\n",
        "        # --- MORE DEBUGGING ---\n",
        "        print(\"\\n--- Inspecting Tensors Passed to Loss Function ---\")\n",
        "        print(\"Shape of y_pred from model:\", y_pred.shape)\n",
        "        # print(\"\\nSample y_pred tensor (first 5 values of first anchor):\\n\", y_pred.numpy()[0, 0, 0, :5])\n",
        "        # ----------------------\n",
        "\n",
        "        # Call your custom loss function\n",
        "        # You can now add print statements INSIDE your loss function too!\n",
        "        loss_value = calculate_model_loss(labels, y_pred)\n",
        "\n",
        "        print(\"\\n--- Final Calculated Loss ---\")\n",
        "\n",
        "        print(\"Total Loss for the batch:\", loss_value.numpy())\n",
        "        # -----------------------------\n",
        "\n",
        "    # Step 4 (Optional): Calculate and apply gradients to see the full loop\n",
        "    # grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    # model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(\"\\n--- Manual Step Complete ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJL4G8cVTYqo",
        "outputId": "84e42466-00a7-4841-ad6f-0e6a40db8e90"
      },
      "id": "lJL4G8cVTYqo",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting Data Before Loss Calculation ---\n",
            "Shape of images (X_batch): (32, 100, 100, 1)\n",
            "Shape of labels (y_true_batch): (32, 5, 15)\n",
            "\n",
            "Sample y_true label tensor:\n",
            " [[ 1. 25. 28. 28. 25.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
            " [ 1. 58. 72. 22. 27.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (32, 6, 6, 45)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : 0.6886429786682129\n",
            "bounding_box_loss : 1770.9659423828125\n",
            "classification_loss : 2.3034982681274414\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6951879858970642\n",
            "\n",
            "\n",
            "Total Loss : 5.4582953453063965\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: 5.4582953\n",
            "\n",
            "--- Manual Step Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model for 20 Epochs\n",
        "\n",
        "* So far our loss functions work as expected in Eager mode.  \n",
        "* In this section we'll run the loss functions in optimized `Graph Mode` to see if it works as expected and also if the loss decreases as we progress.\n",
        "* Our experiments in other notebooks we know that we'll need to create the model using Functional API."
      ],
      "metadata": {
        "id": "p5Ky6tdhbwEr"
      },
      "id": "p5Ky6tdhbwEr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Dataset"
      ],
      "metadata": {
        "id": "svkr5mXxfHNG"
      },
      "id": "svkr5mXxfHNG"
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor,shape=(-1,28,28,1))\n",
        "y_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "batch_size = 32\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5,15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=batch_size)"
      ],
      "metadata": {
        "id": "gkzQx-sBeYr_"
      },
      "id": "gkzQx-sBeYr_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Model"
      ],
      "metadata": {
        "id": "YgkYBImJfQbl"
      },
      "id": "YgkYBImJfQbl"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(100,100,1),batch_size=batch_size ,name=\"input_layer\")\n",
        "\n",
        "x = tf.keras.layers.Rescaling(scale=1./255, name=\"rescaling\")(inputs)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear')(x)\n",
        "\n",
        "# Define the final model by specifying its inputs and outputs\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "XOoSfkYEfNyj",
        "outputId": "1cfed83a-7054-4cbe-fd93-bd21fa791bf3"
      },
      "id": "XOoSfkYEfNyj",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m1,608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m1,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m2,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m45\u001b[0m)         │         \u001b[38;5;34m1,485\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,485</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Callbacks"
      ],
      "metadata": {
        "id": "oYkmsyYFfVG_"
      },
      "id": "oYkmsyYFfVG_"
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: Define the callbacks\n",
        "checkpoint_filepath = './models/experiment_1_{epoch:02d}.keras'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=1,\n",
        "    )"
      ],
      "metadata": {
        "id": "j-30sp7AfSYt"
      },
      "id": "j-30sp7AfSYt",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile & Fit"
      ],
      "metadata": {
        "id": "Yzyrwlo6fZR1"
      },
      "id": "Yzyrwlo6fZR1"
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "## step 5: Fit the model\n",
        "epochs=20\n",
        "\n",
        "## commenting this out to avoid long running fit method\n",
        "# history = model.fit(\n",
        "#   processed_dataset,\n",
        "#   epochs=epochs,\n",
        "#   callbacks=[model_checkpoint_callback]\n",
        "# )"
      ],
      "metadata": {
        "id": "gAB0cfrqfW3s"
      },
      "id": "gAB0cfrqfW3s",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing Training\n",
        "* Right now a single epoch takes ~30 mins to fit, which means to train the model for 20 epochs we'll need ~10 hours, which is too slow for this model.\n",
        "* We have 2 potential reasons for this slowness,\n",
        "  * Numpy based data generation code. Since our data generation implementation uses Numpy APIs which is CPU bound process it might not be able to keep up with GPU based TF Training process.\n",
        "  * Batch size - with current batch size of 32 we have 1875 batches in single epoch which adds to the CPU bound data generation overhead.\n",
        "* Our first step is to profile the data generation scripts to see how long does it take to generate data for a CPU bound script."
      ],
      "metadata": {
        "id": "namrr1vJfbEV"
      },
      "id": "namrr1vJfbEV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Profiling\n",
        "\n",
        "* We'll use a simple benchmark function to measure the time taken to generate the data per epoch.\n"
      ],
      "metadata": {
        "id": "CafkXvAx6d9g"
      },
      "id": "CafkXvAx6d9g"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def benchmark(dataset, num_epochs=2):\n",
        "    start_time = time.perf_counter()\n",
        "    for epoch_num in range(num_epochs):\n",
        "        print(f\"---- Epoch {epoch_num} ----\")\n",
        "        for sample in dataset:\n",
        "            # Performing a training step\n",
        "            time.sleep(0.01)\n",
        "        print(f\"Execution time for epoch {epoch_num} : {time.perf_counter() - start_time}\")\n",
        "    print(\"Total Execution time:\", time.perf_counter() - start_time)"
      ],
      "metadata": {
        "id": "wsBgobMa6hRT"
      },
      "id": "wsBgobMa6hRT",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## commenthing this out to void running this again when notebook restarts\n",
        "# benchmark(processed_dataset.prefetch(tf.data.AUTOTUNE))"
      ],
      "metadata": {
        "id": "6t7pgNGY62ib"
      },
      "id": "6t7pgNGY62ib",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "---- Epoch 0 ----\n",
        "Execution time for epoch 0 : 2485.765105039\n",
        "---- Epoch 1 ----\n",
        "Execution time for epoch 1 : 4950.8231753479995\n",
        "Total Execution time: 4950.82328547\n",
        "```\n",
        "\n",
        "Observations\n",
        "* We made a mistake in above calculations, the time for `epoch 1` is total time of both the epochs.\n",
        "* Still on average each epoch took ~2475 seconds\n",
        "* If we subtract sleep time which is 0.01s per batch, total \"training\" time would be 0.01 * 1875 = 18.75 seconds per epoch.\n",
        "* Which means ~2456s or 40 mins per epoch goes into data generation.\n",
        "* This proves that the data generation pipeline is the bottle neck in our training. We'll need to convert the script into TF and see if we can speed up this step"
      ],
      "metadata": {
        "id": "L0c2TXqvqFG5"
      },
      "id": "L0c2TXqvqFG5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Numpy Funtions to TF\n",
        "* We'll reuse the Numpy functions and try to convert them into TF one step at a time."
      ],
      "metadata": {
        "id": "VwpRGNnz-F7a"
      },
      "id": "VwpRGNnz-F7a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TensorFlow Data Generation Scripts"
      ],
      "metadata": {
        "id": "7pYqvFUmwJN0"
      },
      "id": "7pYqvFUmwJN0"
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_MNIST_DATA_PIXELS_TF = tf.constant(x_train, dtype=tf.float32)\n",
        "ALL_MNIST_DATA_CLASSES_TF = tf.constant(y_train, dtype=tf.float32)\n",
        "\n",
        "# number of digits to overlay on canvas\n",
        "num_of_digits = 2\n",
        "\n",
        "# max digits to define the shape of prediction output\n",
        "MAX_DIGITS = 5\n",
        "\n",
        "\n",
        "# Sample Base Digits\n",
        "def get_sample_indices(dataset, size=5):\n",
        "  dataset_len = tf.shape(dataset)[0] - 1\n",
        "  random_indices = tf.random.uniform(shape=[size], minval=0,maxval=dataset_len,dtype=tf.int32)\n",
        "  return random_indices\n",
        "\n",
        "# helper function to sample number of digits from master dataset\n",
        "def sample_base_digits(num_of_digits):\n",
        "    \"\"\"\n",
        "    Sample a specified number of digit images and their class labels from the master MNIST dataset.\n",
        "\n",
        "    Args:\n",
        "        num_of_digits (int): Number of digit samples to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sample_pixels, sample_values)\n",
        "            sample_pixels (np.ndarray): Array of digit images with shape (num_of_digits, 28, 28, 1).\n",
        "            sample_values (np.ndarray): Array of class labels with shape (num_of_digits, 1).\n",
        "    \"\"\"\n",
        "    sample_indices = get_sample_indices(ALL_MNIST_DATA_PIXELS, size=num_of_digits)\n",
        "    sample_pixels = tf.gather(ALL_MNIST_DATA_PIXELS_TF, indices=sample_indices, axis=0, batch_dims=1)\n",
        "    sample_pixels = tf.reshape(sample_pixels, shape=(num_of_digits,28,28))\n",
        "\n",
        "    sample_values = tf.gather(ALL_MNIST_DATA_CLASSES_TF, indices=sample_indices, axis=0, batch_dims=1)\n",
        "    sample_values = tf.reshape(sample_values, shape=(num_of_digits,1))\n",
        "    return sample_pixels, sample_values\n",
        "# Augment Digits\n",
        "\n",
        "\n",
        "def plot_before_after(before_image, after_image):\n",
        "    \"\"\"\n",
        "    Display two images side by side for visual comparison (e.g., before and after augmentation).\n",
        "\n",
        "    Args:\n",
        "        before_image (np.ndarray): The original image.\n",
        "        after_image (np.ndarray): The image after transformation or augmentation.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "    axs[0].imshow(before_image)\n",
        "    axs[1].imshow(after_image)\n",
        "\n",
        "    plt.axis(\"off\")  # Remove axes for better visualization\n",
        "    plt.show()\n",
        "# helper function to apply random augmentation to digits\n",
        "\n",
        "\n",
        "augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomTranslation(\n",
        "      height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "  tf.keras.layers.RandomZoom(\n",
        "      height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "  tf.keras.layers.RandomRotation(\n",
        "      factor=0.1, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "  ])\n",
        "\n",
        "\n",
        "def augment_digits(digits, debug=False):\n",
        "    \"\"\"\n",
        "    Apply random augmentations (translation, zoom, rotation) to a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        digits (np.ndarray): Array of digit images to augment.\n",
        "        debug (bool, optional): If True, displays before/after images for each digit. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Augmented digit images as a numpy array.\n",
        "    \"\"\"\n",
        "    # step 2: apply random augmentation\n",
        "    augmented_tensor_digits = augmentation(digits)\n",
        "    return augmented_tensor_digits\n",
        "\n",
        "\n",
        "# Calculate Tight BBox\n",
        "# helper function to calculate bounding box for each instance and return it.\n",
        "# we are going to refactor the POC that we created ealier to use it with numpy arrays in the map function\n",
        "\n",
        "\n",
        "def calculate_bounding_box(pixels, class_value, padding=1):\n",
        "    \"\"\"\n",
        "    Calculate the tight bounding box for a digit image and return its coordinates and class value.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): 2D array representing the digit image.\n",
        "        class_value (int): The class label of the digit.\n",
        "        padding (int, optional): Padding to add around the bounding box. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        dict: Bounding box information including coordinates, center, width, height, and class value.\n",
        "    \"\"\"\n",
        "    # calculate active rows & columns\n",
        "    active_rows = tf.reduce_sum(pixels, axis=1, keepdims=True)\n",
        "    active_columns = tf.reduce_sum(pixels, axis=0, keepdims=True)\n",
        "\n",
        "    # calculate x_min and x_max coordinate\n",
        "    x_min = tf.cast(tf.where(active_columns != 0)[0][0], tf.int32)\n",
        "    x_max = tf.cast(tf.where(active_columns != 0)[0][-1], tf.int32)\n",
        "    y_min = tf.cast(tf.where(active_rows != 0)[0][0], tf.int32)\n",
        "    y_max = tf.cast(tf.where(active_rows != 0)[0][-1], tf.int32)\n",
        "\n",
        "    # add padding to pixels\n",
        "    x_min = x_min - (padding if (x_min != 0) else 0)\n",
        "    x_max = x_max + (padding if (x_max != 27) else 0)\n",
        "    y_min = y_min - (padding if (y_min != 0) else 0)\n",
        "    y_max = y_max + (padding if (y_max != 27) else 0)\n",
        "\n",
        "    # calcualte x_center and y_center\n",
        "    x_center = tf.round((x_min + x_max) / 2)\n",
        "    y_center = tf.round((y_min + y_max) / 2)\n",
        "\n",
        "    # calculate width and height\n",
        "    width = x_max - x_min + 1\n",
        "    height = y_max - y_min + 1\n",
        "\n",
        "    # return {\n",
        "    #     \"x_min\": x_min,\n",
        "    #     \"x_max\": x_max,\n",
        "    #     \"y_min\": y_min,\n",
        "    #     \"y_max\": y_max,\n",
        "    #     \"x_center\": x_center,\n",
        "    #     \"y_center\": y_center,\n",
        "    #     \"width\": width,\n",
        "    #     \"height\": height,\n",
        "    #     \"class_value\": class_value\n",
        "    # }\n",
        "    return [x_min,x_max,y_min,y_max,x_center,y_center,width,height,class_value]\n",
        "\n",
        "\n",
        "# helper function to visualize the bounding box\n",
        "\n",
        "\n",
        "def visualize_bounding_box(pixel_data, bounding_boxes, num_of_columns=5):\n",
        "    \"\"\"\n",
        "    Visualize digit images with their corresponding bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        pixel_data (np.ndarray): Array of digit images.\n",
        "        bounding_boxes (list): List of bounding box dictionaries for each digit.\n",
        "        num_of_columns (int, optional): Number of columns in the plot grid. Defaults to 5.\n",
        "    \"\"\"\n",
        "    num_of_columns = num_of_columns if num_of_columns <= 5 else 5\n",
        "    num_instances = pixel_data.shape[0]\n",
        "    num_of_rows = int(num_instances / num_of_columns) + \\\n",
        "        (1 if int(num_instances % num_of_columns) > 0 else 0)\n",
        "\n",
        "    fig, axs = plt.subplots(num_of_rows, num_of_columns, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for idx in range(0, num_instances, 1):\n",
        "\n",
        "        original = tf.constant(pixel_data[idx].reshape(28, 28, 1))\n",
        "        converted = tf.image.grayscale_to_rgb(original)\n",
        "        target_data = bounding_boxes[idx]\n",
        "        x_center = target_data[\"x_center\"]\n",
        "        y_center = target_data[\"y_center\"]\n",
        "        width = target_data[\"width\"]\n",
        "        height = target_data[\"height\"]\n",
        "\n",
        "        x = target_data[\"x_min\"]\n",
        "        y = target_data[\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        image_data = converted.numpy().astype(\"uint8\")\n",
        "        axs[idx].imshow(image_data)\n",
        "        axs[idx].add_patch(rect)\n",
        "\n",
        "        axs[idx].set_title(target_data[\"class_value\"])\n",
        "        axs[idx].axis(\"off\")\n",
        "    plt.show()\n",
        "# helper function to calculate bounding box for digits.\n",
        "\n",
        "\n",
        "def calculate_tight_bbox(pixels, class_values, debug=False):\n",
        "    \"\"\"\n",
        "    Calculate tight bounding boxes for a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_values (np.ndarray): Array of class labels for each digit.\n",
        "        debug (bool, optional): If True, visualizes the bounding boxes. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    num_of_boxes = tf.shape(pixels)[0]\n",
        "    class_with_bbox = []\n",
        "    for idx in range(num_of_boxes):\n",
        "        class_with_bbox.append(calculate_bounding_box(\n",
        "            pixels[idx], class_values[idx][0]))\n",
        "\n",
        "    # # if debug true render digits with bbox\n",
        "    # if debug == True:\n",
        "    #     visualize_bounding_box(pixels, class_with_bbox, pixels.shape[0])\n",
        "\n",
        "    return tf.convert_to_tensor(class_with_bbox, dtype=tf.float32)\n",
        "    # return class_with_bbox\n",
        "# Create Blank Canvas\n",
        "# helper function to create  blank canvas\n",
        "\n",
        "\n",
        "def create_blank_canvas(shape=(100, 100, 1)):\n",
        "    \"\"\"\n",
        "    Create a blank canvas for placing digit images.\n",
        "\n",
        "    Args:\n",
        "        shape (tuple, optional): Shape of the canvas. Defaults to (100, 100, 1).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Blank canvas array.\n",
        "    \"\"\"\n",
        "    canvas = tf.zeros(shape=(100, 100, 1), dtype=tf.float32)\n",
        "    return canvas\n",
        "# Create Prediction Object\n",
        "# helper function to create empty predition structure based on MAX_DIGITS\n",
        "\n",
        "\n",
        "def create_prediction_object():\n",
        "    \"\"\"\n",
        "    Create an empty prediction object for storing digit detection results.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Prediction array of shape (MAX_DIGITS, 15).\n",
        "    \"\"\"\n",
        "    prediction = tf.zeros(shape=(MAX_DIGITS, 15), dtype=tf.float32)\n",
        "    return prediction\n",
        "# Place Digit On Canvas\n",
        "\n",
        "\n",
        "def is_valid_coordinates(top, left, class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Check if the proposed top-left coordinates for a digit's bounding box are valid (within canvas and non-overlapping).\n",
        "\n",
        "    Args:\n",
        "        top (int): Proposed top coordinate.\n",
        "        left (int): Proposed left coordinate.\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if coordinates are valid, False otherwise.\n",
        "    \"\"\"\n",
        "    # # make sure the top and left are withing the canvas\n",
        "    # if (top + 28 >= 100 or left + 28 >= 100):\n",
        "    #     return False\n",
        "    # read current class values\n",
        "    # curr_x_center = class_bbox_value[\"x_center\"]\n",
        "    # curr_y_center = class_bbox_value[\"y_center\"]\n",
        "    curr_width = class_bbox_value[\"width\"]\n",
        "    curr_height = class_bbox_value[\"height\"]\n",
        "    curr_x_min = left\n",
        "    curr_y_min = top\n",
        "    curr_x_max = left + curr_width\n",
        "    curr_y_max = top + curr_height\n",
        "\n",
        "    # recalculate center with proposed top and left values\n",
        "    # curr_x_center = curr_x_center + left\n",
        "    # curr_y_center = curr_y_center + top\n",
        "\n",
        "    # check 1: will the new bounding box go beyond the grid?\n",
        "    if ((curr_x_min + curr_width) >= 100) or ((curr_y_min + curr_height) >= 100):\n",
        "        return False\n",
        "\n",
        "    # check 2: do bounding boxes overlap\n",
        "    # check the current bounding box with every existing box\n",
        "    for coord_idx in range(len(existing_coordinates)):\n",
        "        existing_x_min = existing_coordinates[coord_idx][\"x_min\"]\n",
        "        existing_y_min = existing_coordinates[coord_idx][\"y_min\"]\n",
        "        existing_x_max = existing_coordinates[coord_idx][\"x_max\"]\n",
        "        existing_y_max = existing_coordinates[coord_idx][\"y_max\"]\n",
        "        if ((curr_x_min <= existing_x_max and curr_x_max >= existing_x_min) and (curr_y_min <= existing_y_max and curr_y_max >= existing_y_min)):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def select_top_left(class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Randomly select valid top-left coordinates for placing a digit on the canvas.\n",
        "\n",
        "    Args:\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (top, left) coordinates if valid, otherwise (-1, -1).\n",
        "    \"\"\"\n",
        "    got_valid_coordinates = False\n",
        "    # limiting the loop to run only 20 times\n",
        "    retries = 0\n",
        "    while ((not got_valid_coordinates) and (retries < 50)):\n",
        "        top = tf.random.uniform(shape=[1], minval=0,maxval=100,dtype=tf.int32)[0]\n",
        "        left = tf.random.uniform(shape=[1], minval=0,maxval=100,dtype=tf.int32)[0]\n",
        "        got_valid_coordinates = is_valid_coordinates(\n",
        "            top, left, class_bbox_value, existing_coordinates)\n",
        "        retries = retries+1\n",
        "\n",
        "    if got_valid_coordinates:\n",
        "        return top, left\n",
        "    return -1, -1\n",
        "\n",
        "\n",
        "# helper function to render the canvas\n",
        "# update the original plotting function to plot canvas as well.\n",
        "\n",
        "\n",
        "def render_canvas(canvas, class_bbox):\n",
        "    \"\"\"\n",
        "    Render the canvas with all placed digits and their bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The canvas image.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    num_of_digits = len(class_bbox)\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
        "    axs.imshow(canvas)  # Use 'gray' colormap to render grayscale\n",
        "    axs.axis(\"off\")\n",
        "\n",
        "    for idx in range(0, num_of_digits, 1):\n",
        "        width = class_bbox[idx][\"width\"]\n",
        "        height = class_bbox[idx][\"height\"]\n",
        "\n",
        "        x = class_bbox[idx][\"x_min\"]\n",
        "        y = class_bbox[idx][\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        axs.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def place_digit_on_canvas(canvas, pixels, class_bbox, debug=False):\n",
        "    \"\"\"\n",
        "    Place digit images on the canvas at valid, non-overlapping locations.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The blank canvas to place digits on.\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "        debug (bool, optional): If True, renders the canvas after placement. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, class_bbox) with updated canvas and bounding boxes.\n",
        "    \"\"\"\n",
        "    # list to save all the valid existing coordinates\n",
        "    existing_coordinates = []\n",
        "\n",
        "    # in case if the algorithm cannot place a digit on canvas we'll drop that digit from pixel and class_bbox\n",
        "    digits_to_drop = []\n",
        "\n",
        "    total_digits = tf.shape(pixels)[0]\n",
        "    # loop thru all the pixel values\n",
        "    for idx in range(total_digits):\n",
        "        print(f\"idx type {idx.dtype}\")\n",
        "        class_bbox_value = class_bbox[idx]\n",
        "        x_center = class_bbox_value[\"x_center\"]\n",
        "        y_center = class_bbox_value[\"y_center\"]\n",
        "        width = class_bbox_value[\"width\"]\n",
        "        height = class_bbox_value[\"height\"]\n",
        "        class_value = class_bbox_value[\"class_value\"]\n",
        "        x_min = class_bbox_value[\"x_min\"]\n",
        "        y_min = class_bbox_value[\"y_min\"]\n",
        "        x_max = class_bbox_value[\"x_max\"]\n",
        "        y_max = class_bbox_value[\"y_max\"]\n",
        "\n",
        "        # print(f\"Width {width} & {int(width)}, Height {height} & {int(height)}\")\n",
        "        # width = int(width)\n",
        "        # height = int(height)\n",
        "\n",
        "        # step 1: find the right coordinates to place the digit\n",
        "        top, left = select_top_left(class_bbox_value, existing_coordinates)\n",
        "        if top != -1 and left != -1:\n",
        "            # step 2: place the digit\n",
        "            # canvas[y_min + top:y_min + top+height, x_min + left:x_min +\n",
        "            #        left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "            canvas[top:top+height, left:\n",
        "                   left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            # step 3: recalculate the center based on top,left and update the class values with new center\n",
        "            class_bbox_value[\"x_center\"] = x_center + left\n",
        "            class_bbox_value[\"x_min\"] = left\n",
        "            class_bbox_value[\"x_max\"] = left + width\n",
        "\n",
        "            class_bbox_value[\"y_center\"] = y_center + top\n",
        "            class_bbox_value[\"y_min\"] = top\n",
        "            class_bbox_value[\"y_max\"] = top + height\n",
        "\n",
        "            # update the array\n",
        "            class_bbox[idx] = class_bbox_value\n",
        "            # step 5: save the existing bounding box coordinates to help select the new one\n",
        "            existing_coordinates.append(\n",
        "                class_bbox_value\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Error placing digit {class_value} on canvas. Couldn't fild valid coordinates\")\n",
        "            digits_to_drop.append(idx)\n",
        "\n",
        "    # drop any bbox for which we couldn't find space in canvas\n",
        "    filtered_bbox = [bbox for idx, bbox in enumerate(\n",
        "        class_bbox) if idx not in digits_to_drop]\n",
        "\n",
        "    # if debug == True:\n",
        "    #     render_canvas(canvas=canvas, class_bbox=filtered_bbox)\n",
        "\n",
        "    return canvas, class_bbox\n",
        "# Translate BBox To Prediction Object\n",
        "# helper function to convert bbox diction to prediction object\n",
        "\n",
        "\n",
        "def translate_bbox_to_prediction(current_bbox, prediction, debug=False):\n",
        "    \"\"\"\n",
        "    Convert bounding box dictionaries to a prediction object suitable for training.\n",
        "\n",
        "    Args:\n",
        "        current_bbox (list): List of bounding box dictionaries.\n",
        "        prediction (np.ndarray): Prediction array to update.\n",
        "        debug (bool, optional): If True, prints mapping details. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Updated prediction array.\n",
        "    \"\"\"\n",
        "    # Sanity check - ideally prediction shape should be larger than or equal to number of elements in bbox\n",
        "    if (prediction.shape[0] < len(current_bbox)):\n",
        "        print(f\"Error shape mismatch between prediction and bbox\")\n",
        "        return prediction\n",
        "\n",
        "    for idx, bbox in enumerate(current_bbox):\n",
        "        # set the flag indicating the digit is present\n",
        "        prediction[idx][0] = 1\n",
        "        # set x_center\n",
        "        prediction[idx][1] = bbox[\"x_center\"]\n",
        "        # set y_center\n",
        "        prediction[idx][2] = bbox[\"y_center\"]\n",
        "        # set width\n",
        "        prediction[idx][3] = bbox[\"width\"]\n",
        "        # set height\n",
        "        prediction[idx][4] = bbox[\"height\"]\n",
        "        # set one hot encoded value of the class\n",
        "        # read the class value\n",
        "        class_value = int(bbox[\"class_value\"])\n",
        "        # set the cell corresponding to class value to 1\n",
        "        prediction[idx][5 + class_value] = 1\n",
        "        if debug == True:\n",
        "            print(f\"current bbox is {bbox}\")\n",
        "            print(f\"mapped prediction is {prediction[idx]}\")\n",
        "\n",
        "    return prediction\n",
        "# Generate Training Example\n",
        "# helper map function to map 28x28x1 image and its class to 100x100x1 canvas and prediction object\n",
        "\n",
        "\n",
        "def generate_training_example_tf(x, y, debug=False):\n",
        "    \"\"\"\n",
        "    Generate a training example by placing digits on a canvas and creating the corresponding prediction object.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Input digit image(s).\n",
        "        y (np.ndarray): Corresponding class label(s).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, prediction) where canvas is the composed image and prediction is the label array.\n",
        "    \"\"\"\n",
        "    pixels = tf.reshape(x, shape = (1,28,28))\n",
        "    class_values = tf.reshape(y,shape=(1,1))\n",
        "\n",
        "    # step 1: sample additional digits\n",
        "    if num_of_digits - 1 > 0:\n",
        "        sample_pixels, sample_values = sample_base_digits(num_of_digits - 1)\n",
        "        pixels = tf.concat([pixels, sample_pixels], axis=0)\n",
        "        class_values = tf.concat([class_values, sample_values], axis=0)\n",
        "\n",
        "    # # step 2: augment digits\n",
        "    pixels = augment_digits(pixels, debug=True)\n",
        "\n",
        "    # # step 3: calculate bounding box and return it as tensor\n",
        "    class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n",
        "\n",
        "    # # step 4: create blank canvas and prediction\n",
        "    canvas = create_blank_canvas()\n",
        "    prediction = create_prediction_object()\n",
        "\n",
        "    # # step 5: place digit on canvas\n",
        "    # canvas, class_bbox = place_digit_on_canvas(\n",
        "    #     canvas, pixels, class_with_bbox, debug=debug)\n",
        "\n",
        "    # # step 6: translate bbox to prediction object\n",
        "    # prediction = translate_bbox_to_prediction(\n",
        "    #     class_bbox, prediction, debug=debug)\n",
        "\n",
        "    # # print(f\"Final canvas shape {canvas.shape}, final prediction shape {prediction.shape}\")\n",
        "    return canvas, prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "wxHNsEFottIb"
      },
      "id": "wxHNsEFottIb",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Data Generation Scripts"
      ],
      "metadata": {
        "id": "NpSL1IE5wONP"
      },
      "id": "NpSL1IE5wONP"
    },
    {
      "cell_type": "code",
      "source": [
        "## temporary just selecting first 32 records to test quickly\n",
        "X_tensor = tf.convert_to_tensor(x_train[:32], dtype=tf.float32)\n",
        "y_tensor = tf.convert_to_tensor(y_train[:32], dtype=tf.float32)\n",
        "batch_size = 32\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "tf_processed_dataset = raw_dataset.map(generate_training_example_tf).batch(batch_size=batch_size)"
      ],
      "metadata": {
        "id": "-f5Lux7lwTss",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18e61037-d260-4e20-ba52-33f689da7711"
      },
      "id": "-f5Lux7lwTss",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InaccessibleTensorError",
          "evalue": "in user code:\n\n    File \"/tmp/ipython-input-4224655351.py\", line 491, in generate_training_example_tf  *\n        class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n    File \"/tmp/ipython-input-1949637839.py\", line 208, in calculate_tight_bbox  *\n        return tf.convert_to_tensor(class_with_bbox, dtype=tf.float32)\n    File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 144, in capture_by_value\n        graph._validate_in_scope(tensor)  # pylint: disable=protected-access\n\n    InaccessibleTensorError: <tf.Tensor 'while/Cast:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/Cast:0' shape=() dtype=int32> was defined here:\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n        File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n        File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n        File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n        File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n        File \"/tmp/ipython-input-1812299449.py\", line 9, in <cell line: 0>\n        File \"/tmp/ipython-input-4224655351.py\", line 491, in generate_training_example_tf\n        File \"/tmp/ipython-input-1949637839.py\", line 200, in calculate_tight_bbox\n        File \"/tmp/ipython-input-1949637839.py\", line 201, in calculate_tight_bbox\n        File \"/tmp/ipython-input-2374686352.py\", line 108, in calculate_bounding_box\n    \n    The tensor <tf.Tensor 'while/Cast:0' shape=() dtype=int32> cannot be accessed from FuncGraph(name=Dataset_map_generate_training_example_tf, id=139505907279808), because it was defined in FuncGraph(name=while_body_18182, id=139505778666688), which is out of scope.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInaccessibleTensorError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1812299449.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Use the wrapper inside the map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf_processed_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_training_example_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m   2339\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2341\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, synchronous, use_unbounded_threadpool, name)\u001b[0m\n\u001b[1;32m     41\u001b[0m           \u001b[0;34m\"`num_parallel_calls` argument is specified.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       )\n\u001b[0;32m---> 43\u001b[0;31m     return _MapDataset(\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, force_synchronous, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1224\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    229\u001b[0m       \u001b[0;31m# Note: wrapper_helper will apply autograph based on context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInaccessibleTensorError\u001b[0m: in user code:\n\n    File \"/tmp/ipython-input-4224655351.py\", line 491, in generate_training_example_tf  *\n        class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n    File \"/tmp/ipython-input-1949637839.py\", line 208, in calculate_tight_bbox  *\n        return tf.convert_to_tensor(class_with_bbox, dtype=tf.float32)\n    File \"/usr/local/lib/python3.12/dist-packages/tensorflow/core/function/capture/capture_container.py\", line 144, in capture_by_value\n        graph._validate_in_scope(tensor)  # pylint: disable=protected-access\n\n    InaccessibleTensorError: <tf.Tensor 'while/Cast:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\n    Please see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n    \n    <tf.Tensor 'while/Cast:0' shape=() dtype=int32> was defined here:\n        File \"<frozen runpy>\", line 198, in _run_module_as_main\n        File \"<frozen runpy>\", line 88, in _run_code\n        File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n        File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n        File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n        File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n        File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n        File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n        File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n        File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n        File \"/tmp/ipython-input-1812299449.py\", line 9, in <cell line: 0>\n        File \"/tmp/ipython-input-4224655351.py\", line 491, in generate_training_example_tf\n        File \"/tmp/ipython-input-1949637839.py\", line 200, in calculate_tight_bbox\n        File \"/tmp/ipython-input-1949637839.py\", line 201, in calculate_tight_bbox\n        File \"/tmp/ipython-input-2374686352.py\", line 108, in calculate_bounding_box\n    \n    The tensor <tf.Tensor 'while/Cast:0' shape=() dtype=int32> cannot be accessed from FuncGraph(name=Dataset_map_generate_training_example_tf, id=139505907279808), because it was defined in FuncGraph(name=while_body_18182, id=139505778666688), which is out of scope.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "# Step 1: Get one batch of data from your dataset pipeline\n",
        "# The .take(1) method creates a new dataset with only the first element.\n",
        "one_batch = tf_processed_dataset.take(1)\n",
        "\n",
        "# Step 2: Iterate over the single batch to get the tensors\n",
        "for images, labels in one_batch:\n",
        "\n",
        "    # --- THIS IS YOUR DEBUGGING ZONE ---\n",
        "    # Now you have the concrete tensors for one batch.\n",
        "    # You can inspect them with regular print() and .numpy()\n",
        "\n",
        "    print(\"--- Inspecting Data Before Loss Calculation ---\")\n",
        "    print(\"Shape of images (X_batch):\", images.shape)\n",
        "    print(\"Shape of labels (y_true_batch):\", labels.shape)\n",
        "    print(\"\\nSample y_true label tensor:\\n\", labels.numpy()[0]) # Print the first label in the batch\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Step 3: Manually run the forward pass and gradient calculation\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the model's raw predictions for this batch\n",
        "        y_pred = model(images, training=True)  # Pass the images through the model\n",
        "\n",
        "        # --- MORE DEBUGGING ---\n",
        "        print(\"\\n--- Inspecting Tensors Passed to Loss Function ---\")\n",
        "        print(\"Shape of y_pred from model:\", y_pred.shape)\n",
        "        # print(\"\\nSample y_pred tensor (first 5 values of first anchor):\\n\", y_pred.numpy()[0, 0, 0, :5])\n",
        "        # ----------------------\n",
        "\n",
        "        # Call your custom loss function\n",
        "        # You can now add print statements INSIDE your loss function too!\n",
        "        loss_value = calculate_model_loss(labels, y_pred)\n",
        "\n",
        "        print(\"\\n--- Final Calculated Loss ---\")\n",
        "\n",
        "        print(\"Total Loss for the batch:\", loss_value.numpy())\n",
        "        # -----------------------------\n",
        "\n",
        "    # Step 4 (Optional): Calculate and apply gradients to see the full loop\n",
        "    # grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    # model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(\"\\n--- Manual Step Complete ---\")\n"
      ],
      "metadata": {
        "id": "yIworpzlY081",
        "outputId": "68f56aad-2b1d-4b01-91c5-1821b1afc9ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yIworpzlY081",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Inspecting Data Before Loss Calculation ---\n",
            "Shape of images (X_batch): (32, 100, 100, 1)\n",
            "Shape of labels (y_true_batch): (32, 5, 15)\n",
            "\n",
            "Sample y_true label tensor:\n",
            " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (32, 6, 6, 45)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : nan\n",
            "bounding_box_loss : []\n",
            "classification_loss : []\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6931465268135071\n",
            "\n",
            "\n",
            "Total Loss : []\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: []\n",
            "\n",
            "--- Manual Step Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# benchmark(tf_processed_dataset.prefetch(tf.data.AUTOTUNE), num_epochs=1)"
      ],
      "metadata": {
        "id": "rps-RGq-wXTN"
      },
      "id": "rps-RGq-wXTN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BumIrE8lA1MN"
      },
      "id": "BumIrE8lA1MN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}