{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurangdave/mnist_object_detection/blob/main/notebooks/03_google_colab_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d9b8e6",
      "metadata": {
        "id": "88d9b8e6"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurangdave/mnist_object_detection/blob/main/notebooks/03_google_colab_migration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5547be1",
      "metadata": {
        "id": "d5547be1"
      },
      "source": [
        "# MNIST Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "0tP3N0G6DNgp"
      },
      "id": "0tP3N0G6DNgp"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.datasets import fetch_openml\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "import PIL.Image\n",
        "from matplotlib import patches\n",
        "\n"
      ],
      "metadata": {
        "id": "C88hhW3fDQFO"
      },
      "id": "C88hhW3fDQFO",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dMCwJ7fDeN1",
        "outputId": "e191246e-9577-4b0d-af1f-c4d25eb08eee"
      },
      "id": "4dMCwJ7fDeN1",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Access"
      ],
      "metadata": {
        "id": "VPEcmSCfCH8g"
      },
      "id": "VPEcmSCfCH8g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Constants  "
      ],
      "metadata": {
        "id": "wa9FmTPCDbn1"
      },
      "id": "wa9FmTPCDbn1"
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = Path(\"data\")\n",
        "models_dir = Path(\"models\")"
      ],
      "metadata": {
        "id": "xIdEzErNEHs3"
      },
      "id": "xIdEzErNEHs3",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "id": "17DdP_OEEW9f"
      },
      "id": "17DdP_OEEW9f",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBYeASoEEXRF",
        "outputId": "d48623c6-8778-4a09-e373-b7941549d88f"
      },
      "id": "WBYeASoEEXRF",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xe9omI65EXT6"
      },
      "id": "xe9omI65EXT6",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YL1_1CCfEXWQ"
      },
      "id": "YL1_1CCfEXWQ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Generation"
      ],
      "metadata": {
        "id": "52Wv6_QDCRi5"
      },
      "id": "52Wv6_QDCRi5"
    },
    {
      "cell_type": "code",
      "source": [
        "ALL_MNIST_DATA_PIXELS = x_train\n",
        "ALL_MNIST_DATA_CLASSES = y_train\n",
        "\n",
        "# number of digits to overlay on canvas\n",
        "num_of_digits = 2\n",
        "\n",
        "# max digits to define the shape of prediction output\n",
        "MAX_DIGITS = 5\n",
        "\n",
        "\n",
        "# Sample Base Digits\n",
        "def get_sample_indices(dataset, size=5):\n",
        "  random_indices = np.random.choice(len(dataset), size=size, replace=False)\n",
        "  return random_indices\n",
        "\n",
        "# helper function to sample number of digits from master dataset\n",
        "def sample_base_digits(num_of_digits):\n",
        "    \"\"\"\n",
        "    Sample a specified number of digit images and their class labels from the master MNIST dataset.\n",
        "\n",
        "    Args:\n",
        "        num_of_digits (int): Number of digit samples to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (sample_pixels, sample_values)\n",
        "            sample_pixels (np.ndarray): Array of digit images with shape (num_of_digits, 28, 28, 1).\n",
        "            sample_values (np.ndarray): Array of class labels with shape (num_of_digits, 1).\n",
        "    \"\"\"\n",
        "    sample_indices = get_sample_indices(ALL_MNIST_DATA_PIXELS, size=num_of_digits)\n",
        "    sample_pixels = ALL_MNIST_DATA_PIXELS[sample_indices]\n",
        "    sample_pixels = sample_pixels.reshape(-1,28,28,1)\n",
        "\n",
        "    sample_values = ALL_MNIST_DATA_CLASSES[sample_indices]\n",
        "    sample_values = sample_values.reshape(-1, 1)\n",
        "\n",
        "    # split the digits into pixels and class values\n",
        "    # reshape the data to expected values\n",
        "    # sample_pixels = sample.drop(\n",
        "    #     columns=[\"class\"]).to_numpy().reshape(-1, 28, 28, 1)\n",
        "    # sample_values = sample[\"class\"].values.reshape(-1, 1)\n",
        "    return sample_pixels, sample_values\n",
        "# Augment Digits\n",
        "\n",
        "\n",
        "def plot_before_after(before_image, after_image):\n",
        "    \"\"\"\n",
        "    Display two images side by side for visual comparison (e.g., before and after augmentation).\n",
        "\n",
        "    Args:\n",
        "        before_image (np.ndarray): The original image.\n",
        "        after_image (np.ndarray): The image after transformation or augmentation.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "    axs[0].imshow(before_image)\n",
        "    axs[1].imshow(after_image)\n",
        "\n",
        "    plt.axis(\"off\")  # Remove axes for better visualization\n",
        "    plt.show()\n",
        "# helper function to apply random augmentation to digits\n",
        "\n",
        "\n",
        "def augment_digits(digits, debug=False):\n",
        "    \"\"\"\n",
        "    Apply random augmentations (translation, zoom, rotation) to a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        digits (np.ndarray): Array of digit images to augment.\n",
        "        debug (bool, optional): If True, displays before/after images for each digit. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Augmented digit images as a numpy array.\n",
        "    \"\"\"\n",
        "    tensor_digits = tf.convert_to_tensor(digits)\n",
        "\n",
        "    # step 2: apply random augmentation\n",
        "    augmentation = tf.keras.Sequential([\n",
        "        tf.keras.layers.RandomTranslation(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "\n",
        "        tf.keras.layers.RandomRotation(\n",
        "            factor=0.1, fill_value=0.0, fill_mode=\"constant\", seed=42),\n",
        "    ])\n",
        "    augmented_tensor_digits = augmentation(tensor_digits)\n",
        "\n",
        "    # if debug is true render before digits\n",
        "    if debug == True:\n",
        "        for translated_imgs in range(tensor_digits.shape[0]):\n",
        "            plot_before_after(\n",
        "                tensor_digits[translated_imgs], augmented_tensor_digits[translated_imgs])\n",
        "\n",
        "    # convert the tensor back to numpy to simplify use in map function\n",
        "    return augmented_tensor_digits.numpy()\n",
        "# Calculate Tight BBox\n",
        "# helper function to calculate bounding box for each instance and return it.\n",
        "# we are going to refactor the POC that we created ealier to use it with numpy arrays in the map function\n",
        "\n",
        "\n",
        "def calculate_bounding_box(pixels, class_value, padding=1):\n",
        "    \"\"\"\n",
        "    Calculate the tight bounding box for a digit image and return its coordinates and class value.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): 2D array representing the digit image.\n",
        "        class_value (int): The class label of the digit.\n",
        "        padding (int, optional): Padding to add around the bounding box. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        dict: Bounding box information including coordinates, center, width, height, and class value.\n",
        "    \"\"\"\n",
        "    # calculate active rows & columns\n",
        "    active_rows = np.sum(pixels, axis=1)\n",
        "    active_columns = np.sum(pixels, axis=0)\n",
        "\n",
        "    # calculate x_min and x_max coordinate\n",
        "    x_min = np.nonzero(active_columns)[0][0]\n",
        "    x_max = np.nonzero(active_columns)[0][-1]\n",
        "    y_min = np.nonzero(active_rows)[0][0]\n",
        "    y_max = np.nonzero(active_rows)[0][-1]\n",
        "\n",
        "    # add padding to pixels\n",
        "    x_min = x_min - (padding if (x_min != 0) else 0)\n",
        "    x_max = x_max + (padding if (x_max != 27) else 0)\n",
        "    y_min = y_min - (padding if (y_min != 0) else 0)\n",
        "    y_max = y_max + (padding if (y_max != 27) else 0)\n",
        "\n",
        "    # calcualte x_center and y_center\n",
        "    x_center = round((x_min + x_max) / 2)\n",
        "    y_center = round((y_min + y_max) / 2)\n",
        "\n",
        "    # calculate width and height\n",
        "    width = x_max - x_min + 1\n",
        "    height = y_max - y_min + 1\n",
        "\n",
        "    return {\n",
        "        \"x_min\": x_min,\n",
        "        \"x_max\": x_max,\n",
        "        \"y_min\": y_min,\n",
        "        \"y_max\": y_max,\n",
        "        \"x_center\": x_center,\n",
        "        \"y_center\": y_center,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"class_value\": class_value\n",
        "    }\n",
        "\n",
        "\n",
        "# helper function to visualize the bounding box\n",
        "\n",
        "\n",
        "def visualize_bounding_box(pixel_data, bounding_boxes, num_of_columns=5):\n",
        "    \"\"\"\n",
        "    Visualize digit images with their corresponding bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        pixel_data (np.ndarray): Array of digit images.\n",
        "        bounding_boxes (list): List of bounding box dictionaries for each digit.\n",
        "        num_of_columns (int, optional): Number of columns in the plot grid. Defaults to 5.\n",
        "    \"\"\"\n",
        "    num_of_columns = num_of_columns if num_of_columns <= 5 else 5\n",
        "    num_instances = pixel_data.shape[0]\n",
        "    num_of_rows = int(num_instances / num_of_columns) + \\\n",
        "        (1 if int(num_instances % num_of_columns) > 0 else 0)\n",
        "\n",
        "    fig, axs = plt.subplots(num_of_rows, num_of_columns, figsize=(10, 3))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for idx in range(0, num_instances, 1):\n",
        "\n",
        "        original = tf.constant(pixel_data[idx].reshape(28, 28, 1))\n",
        "        converted = tf.image.grayscale_to_rgb(original)\n",
        "        target_data = bounding_boxes[idx]\n",
        "        x_center = target_data[\"x_center\"]\n",
        "        y_center = target_data[\"y_center\"]\n",
        "        width = target_data[\"width\"]\n",
        "        height = target_data[\"height\"]\n",
        "\n",
        "        x = target_data[\"x_min\"]\n",
        "        y = target_data[\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        image_data = converted.numpy().astype(\"uint8\")\n",
        "        axs[idx].imshow(image_data)\n",
        "        axs[idx].add_patch(rect)\n",
        "\n",
        "        axs[idx].set_title(target_data[\"class_value\"])\n",
        "        axs[idx].axis(\"off\")\n",
        "    plt.show()\n",
        "# helper function to calculate bounding box for digits.\n",
        "\n",
        "\n",
        "def calculate_tight_bbox(pixels, class_values, debug=False):\n",
        "    \"\"\"\n",
        "    Calculate tight bounding boxes for a batch of digit images.\n",
        "\n",
        "    Args:\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_values (np.ndarray): Array of class labels for each digit.\n",
        "        debug (bool, optional): If True, visualizes the bounding boxes. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    class_with_bbox = []\n",
        "    for idx in range(pixels.shape[0]):\n",
        "        class_with_bbox.append(calculate_bounding_box(\n",
        "            pixels[idx], class_values[idx][0]))\n",
        "\n",
        "    # if debug true render digits with bbox\n",
        "    if debug == True:\n",
        "        visualize_bounding_box(pixels, class_with_bbox, pixels.shape[0])\n",
        "\n",
        "    return class_with_bbox\n",
        "# Create Blank Canvas\n",
        "# helper function to create  blank canvas\n",
        "\n",
        "\n",
        "def create_blank_canvas(shape=(100, 100, 1)):\n",
        "    \"\"\"\n",
        "    Create a blank canvas for placing digit images.\n",
        "\n",
        "    Args:\n",
        "        shape (tuple, optional): Shape of the canvas. Defaults to (100, 100, 1).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Blank canvas array.\n",
        "    \"\"\"\n",
        "    canvas = np.zeros(shape=(100, 100, 1), dtype=np.float32)\n",
        "    return canvas\n",
        "# Create Prediction Object\n",
        "# helper function to create empty predition structure based on MAX_DIGITS\n",
        "\n",
        "\n",
        "def create_prediction_object():\n",
        "    \"\"\"\n",
        "    Create an empty prediction object for storing digit detection results.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Prediction array of shape (MAX_DIGITS, 15).\n",
        "    \"\"\"\n",
        "    prediction = np.zeros(shape=(MAX_DIGITS, 15), dtype=np.float32)\n",
        "    return prediction\n",
        "# Place Digit On Canvas\n",
        "\n",
        "\n",
        "def is_valid_coordinates(top, left, class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Check if the proposed top-left coordinates for a digit's bounding box are valid (within canvas and non-overlapping).\n",
        "\n",
        "    Args:\n",
        "        top (int): Proposed top coordinate.\n",
        "        left (int): Proposed left coordinate.\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if coordinates are valid, False otherwise.\n",
        "    \"\"\"\n",
        "    # # make sure the top and left are withing the canvas\n",
        "    # if (top + 28 >= 100 or left + 28 >= 100):\n",
        "    #     return False\n",
        "    # read current class values\n",
        "    # curr_x_center = class_bbox_value[\"x_center\"]\n",
        "    # curr_y_center = class_bbox_value[\"y_center\"]\n",
        "    curr_width = class_bbox_value[\"width\"]\n",
        "    curr_height = class_bbox_value[\"height\"]\n",
        "    curr_x_min = left\n",
        "    curr_y_min = top\n",
        "    curr_x_max = left + curr_width\n",
        "    curr_y_max = top + curr_height\n",
        "\n",
        "    # recalculate center with proposed top and left values\n",
        "    # curr_x_center = curr_x_center + left\n",
        "    # curr_y_center = curr_y_center + top\n",
        "\n",
        "    # check 1: will the new bounding box go beyond the grid?\n",
        "    if ((curr_x_min + curr_width) >= 100) or ((curr_y_min + curr_height) >= 100):\n",
        "        return False\n",
        "\n",
        "    # check 2: do bounding boxes overlap\n",
        "    # check the current bounding box with every existing box\n",
        "    for coord_idx in range(len(existing_coordinates)):\n",
        "        existing_x_min = existing_coordinates[coord_idx][\"x_min\"]\n",
        "        existing_y_min = existing_coordinates[coord_idx][\"y_min\"]\n",
        "        existing_x_max = existing_coordinates[coord_idx][\"x_max\"]\n",
        "        existing_y_max = existing_coordinates[coord_idx][\"y_max\"]\n",
        "        if ((curr_x_min <= existing_x_max and curr_x_max >= existing_x_min) and (curr_y_min <= existing_y_max and curr_y_max >= existing_y_min)):\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def select_top_left(class_bbox_value, existing_coordinates):\n",
        "    \"\"\"\n",
        "    Randomly select valid top-left coordinates for placing a digit on the canvas.\n",
        "\n",
        "    Args:\n",
        "        class_bbox_value (dict): Bounding box info for the digit.\n",
        "        existing_coordinates (list): List of existing bounding boxes on the canvas.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (top, left) coordinates if valid, otherwise (-1, -1).\n",
        "    \"\"\"\n",
        "    got_valid_coordinates = False\n",
        "    # limiting the loop to run only 20 times\n",
        "    retries = 0\n",
        "    while ((not got_valid_coordinates) and (retries < 50)):\n",
        "        top = np.random.randint(0, high=100)\n",
        "        left = np.random.randint(0, high=100)\n",
        "        got_valid_coordinates = is_valid_coordinates(\n",
        "            top, left, class_bbox_value, existing_coordinates)\n",
        "        retries = retries+1\n",
        "\n",
        "    if got_valid_coordinates:\n",
        "        return top, left\n",
        "    return -1, -1\n",
        "\n",
        "\n",
        "# helper function to render the canvas\n",
        "# update the original plotting function to plot canvas as well.\n",
        "\n",
        "\n",
        "def render_canvas(canvas, class_bbox):\n",
        "    \"\"\"\n",
        "    Render the canvas with all placed digits and their bounding boxes.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The canvas image.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "    \"\"\"\n",
        "    num_of_digits = len(class_bbox)\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 3))\n",
        "    axs.imshow(canvas)  # Use 'gray' colormap to render grayscale\n",
        "    axs.axis(\"off\")\n",
        "\n",
        "    for idx in range(0, num_of_digits, 1):\n",
        "        width = class_bbox[idx][\"width\"]\n",
        "        height = class_bbox[idx][\"height\"]\n",
        "\n",
        "        x = class_bbox[idx][\"x_min\"]\n",
        "        y = class_bbox[idx][\"y_min\"]\n",
        "\n",
        "        rect = patches.Rectangle(\n",
        "            (x, y), width=width, height=height, linewidth=1, edgecolor='r', facecolor='none')\n",
        "        axs.add_patch(rect)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def place_digit_on_canvas(canvas, pixels, class_bbox, debug=False):\n",
        "    \"\"\"\n",
        "    Place digit images on the canvas at valid, non-overlapping locations.\n",
        "\n",
        "    Args:\n",
        "        canvas (np.ndarray): The blank canvas to place digits on.\n",
        "        pixels (np.ndarray): Array of digit images.\n",
        "        class_bbox (list): List of bounding box dictionaries for each digit.\n",
        "        debug (bool, optional): If True, renders the canvas after placement. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, class_bbox) with updated canvas and bounding boxes.\n",
        "    \"\"\"\n",
        "    # list to save all the valid existing coordinates\n",
        "    existing_coordinates = []\n",
        "\n",
        "    # in case if the algorithm cannot place a digit on canvas we'll drop that digit from pixel and class_bbox\n",
        "    digits_to_drop = []\n",
        "\n",
        "    total_digits = pixels.shape[0]\n",
        "    # loop thru all the pixel values\n",
        "    for idx in range(total_digits):\n",
        "        class_bbox_value = class_bbox[idx]\n",
        "        x_center = class_bbox_value[\"x_center\"]\n",
        "        y_center = class_bbox_value[\"y_center\"]\n",
        "        width = class_bbox_value[\"width\"]\n",
        "        height = class_bbox_value[\"height\"]\n",
        "        class_value = class_bbox_value[\"class_value\"]\n",
        "        x_min = class_bbox_value[\"x_min\"]\n",
        "        y_min = class_bbox_value[\"y_min\"]\n",
        "        x_max = class_bbox_value[\"x_max\"]\n",
        "        y_max = class_bbox_value[\"y_max\"]\n",
        "\n",
        "        # print(f\"Width {width} & {int(width)}, Height {height} & {int(height)}\")\n",
        "        # width = int(width)\n",
        "        # height = int(height)\n",
        "\n",
        "        # step 1: find the right coordinates to place the digit\n",
        "        top, left = select_top_left(class_bbox_value, existing_coordinates)\n",
        "        if top != -1 and left != -1:\n",
        "            # step 2: place the digit\n",
        "            # canvas[y_min + top:y_min + top+height, x_min + left:x_min +\n",
        "            #        left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            canvas[top:top+height, left:\n",
        "                   left + width] = pixels[idx][y_min:y_min+height, x_min:x_min+width]\n",
        "\n",
        "            # step 3: recalculate the center based on top,left and update the class values with new center\n",
        "            class_bbox_value[\"x_center\"] = x_center + left\n",
        "            class_bbox_value[\"x_min\"] = left\n",
        "            class_bbox_value[\"x_max\"] = left + width\n",
        "\n",
        "            class_bbox_value[\"y_center\"] = y_center + top\n",
        "            class_bbox_value[\"y_min\"] = top\n",
        "            class_bbox_value[\"y_max\"] = top + height\n",
        "\n",
        "            # update the array\n",
        "            class_bbox[idx] = class_bbox_value\n",
        "            # step 5: save the existing bounding box coordinates to help select the new one\n",
        "            existing_coordinates.append(\n",
        "                class_bbox_value\n",
        "            )\n",
        "        else:\n",
        "            print(\n",
        "                f\"Error placing digit {class_value} on canvas. Couldn't fild valid coordinates\")\n",
        "            digits_to_drop.append(idx)\n",
        "\n",
        "    # drop any bbox for which we couldn't find space in canvas\n",
        "    filtered_bbox = [bbox for idx, bbox in enumerate(\n",
        "        class_bbox) if idx not in digits_to_drop]\n",
        "\n",
        "    if debug == True:\n",
        "        render_canvas(canvas=canvas, class_bbox=filtered_bbox)\n",
        "\n",
        "    return canvas, class_bbox\n",
        "# Translate BBox To Prediction Object\n",
        "# helper function to convert bbox diction to prediction object\n",
        "\n",
        "\n",
        "def translate_bbox_to_prediction(current_bbox, prediction, debug=False):\n",
        "    \"\"\"\n",
        "    Convert bounding box dictionaries to a prediction object suitable for training.\n",
        "\n",
        "    Args:\n",
        "        current_bbox (list): List of bounding box dictionaries.\n",
        "        prediction (np.ndarray): Prediction array to update.\n",
        "        debug (bool, optional): If True, prints mapping details. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Updated prediction array.\n",
        "    \"\"\"\n",
        "    # Sanity check - ideally prediction shape should be larger than or equal to number of elements in bbox\n",
        "    if (prediction.shape[0] < len(current_bbox)):\n",
        "        print(f\"Error shape mismatch between prediction and bbox\")\n",
        "        return prediction\n",
        "\n",
        "    for idx, bbox in enumerate(current_bbox):\n",
        "        # set the flag indicating the digit is present\n",
        "        prediction[idx][0] = 1\n",
        "        # set x_center\n",
        "        prediction[idx][1] = bbox[\"x_center\"]\n",
        "        # set y_center\n",
        "        prediction[idx][2] = bbox[\"y_center\"]\n",
        "        # set width\n",
        "        prediction[idx][3] = bbox[\"width\"]\n",
        "        # set height\n",
        "        prediction[idx][4] = bbox[\"height\"]\n",
        "        # set one hot encoded value of the class\n",
        "        # read the class value\n",
        "        class_value = int(bbox[\"class_value\"])\n",
        "        # set the cell corresponding to class value to 1\n",
        "        prediction[idx][5 + class_value] = 1\n",
        "        if debug == True:\n",
        "            print(f\"current bbox is {bbox}\")\n",
        "            print(f\"mapped prediction is {prediction[idx]}\")\n",
        "\n",
        "    return prediction\n",
        "# Generate Training Example\n",
        "# helper map function to map 28x28x1 image and its class to 100x100x1 canvas and prediction object\n",
        "\n",
        "\n",
        "def generate_training_example(x, y, debug=False):\n",
        "    \"\"\"\n",
        "    Generate a training example by placing digits on a canvas and creating the corresponding prediction object.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Input digit image(s).\n",
        "        y (np.ndarray): Corresponding class label(s).\n",
        "\n",
        "    Returns:\n",
        "        tuple: (canvas, prediction) where canvas is the composed image and prediction is the label array.\n",
        "    \"\"\"\n",
        "    pixels = x.reshape(-1, 28, 28, 1)\n",
        "    class_values = y.reshape(-1, 1)\n",
        "    # step 1: sample additional digits\n",
        "    if num_of_digits - 1 > 0:\n",
        "        sample_pixels, sample_values = sample_base_digits(num_of_digits - 1)\n",
        "        pixels = np.concatenate((pixels, sample_pixels))\n",
        "        class_values = np.concatenate((class_values, sample_values), axis=0)\n",
        "\n",
        "    # step 2: augment digits\n",
        "    pixels = augment_digits(pixels, debug=debug)\n",
        "\n",
        "    # step 3: calculate bounding box\n",
        "    class_with_bbox = calculate_tight_bbox(pixels, class_values, debug=debug)\n",
        "\n",
        "    # step 4: create blank canvas and prediction\n",
        "    canvas = create_blank_canvas()\n",
        "    prediction = create_prediction_object()\n",
        "\n",
        "    # step 5: place digit on canvas\n",
        "    canvas, class_bbox = place_digit_on_canvas(\n",
        "        canvas, pixels, class_with_bbox, debug=debug)\n",
        "\n",
        "    # step 6: translate bbox to prediction object\n",
        "    prediction = translate_bbox_to_prediction(\n",
        "        class_bbox, prediction, debug=debug)\n",
        "\n",
        "    # print(f\"Final canvas shape {canvas.shape}, final prediction shape {prediction.shape}\")\n",
        "    return (canvas, prediction)\n"
      ],
      "metadata": {
        "id": "ZzOBbarPERb0"
      },
      "id": "ZzOBbarPERb0",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_indices = get_sample_indices(x_train, 1)\n",
        "test_x = x_train[sample_indices]\n",
        "test_y = y_train[sample_indices]\n",
        "canvas, prediction = generate_training_example(test_x,test_y,debug=True)"
      ],
      "metadata": {
        "id": "noLqDnfCEtSQ",
        "outputId": "5bf1365d-c665-40d6-9b41-249515165ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "noLqDnfCEtSQ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current bbox is {'x_min': 50, 'x_max': np.int64(76), 'y_min': 40, 'y_max': np.int64(65), 'x_center': 64, 'y_center': 52, 'width': np.int64(26), 'height': np.int64(25), 'class_value': np.uint8(6)}\n",
            "mapped prediction is [ 1. 64. 52. 26. 25.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
            "current bbox is {'x_min': 0, 'x_max': np.int64(27), 'y_min': 36, 'y_max': np.int64(59), 'x_center': 13, 'y_center': 50, 'width': np.int64(27), 'height': np.int64(23), 'class_value': np.uint8(5)}\n",
            "mapped prediction is [ 1. 13. 50. 27. 23.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training & Evaluation"
      ],
      "metadata": {
        "id": "bb6G8EUtCZqe"
      },
      "id": "bb6G8EUtCZqe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utils"
      ],
      "metadata": {
        "id": "zGOJZdF7TAJz"
      },
      "id": "zGOJZdF7TAJz"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# helper function to convert box values to corner coordinates\n",
        "\n",
        "\n",
        "def convert_boxes_to_corners(box_center_format):\n",
        "    # we'll use the following formulas\n",
        "    # x_min = floor(x_center - (width/2))\n",
        "    # x_max = floor(x_center + (width/2))\n",
        "    # y_min = floor(y_center - (height/2))\n",
        "    # y_max = floor(y_cetner + (height /2))\n",
        "    # calculate true values.\n",
        "    x_min = tf.floor(\n",
        "        box_center_format[:, :, :, 0] - (box_center_format[:, :, :, 2])/2)\n",
        "    x_max = tf.floor(\n",
        "        box_center_format[:, :, :, 0] + (box_center_format[:, :, :, 2])/2)\n",
        "    y_min = tf.floor(\n",
        "        box_center_format[:, :, :, 1] - (box_center_format[:, :, :, 3])/2)\n",
        "    y_max = tf.floor(\n",
        "        box_center_format[:, :, :, 1] + (box_center_format[:, :, :, 3])/2)\n",
        "\n",
        "    coordinates = tf.stack(values=[x_min, y_min, x_max, y_max], axis=3)\n",
        "    return coordinates\n",
        "\n",
        "# helper function to find the intersection box corners from given 2 boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_corners(box_1_corners, box_2_corners):\n",
        "    x_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 0], box_2_corners[:, :, :, 0])\n",
        "    y_min_for_intersection = tf.maximum(\n",
        "        box_1_corners[:, :, :, 1], box_2_corners[:, :, :, 1])\n",
        "    x_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 2], box_2_corners[:, :, :, 2])\n",
        "    y_max_for_intersection = tf.minimum(\n",
        "        box_1_corners[:, :, :, 3], box_2_corners[:, :, :, 3])\n",
        "    intersection_box_corners = tf.stack(\n",
        "        values=[x_min_for_intersection, y_min_for_intersection, x_max_for_intersection, y_max_for_intersection], axis=3)\n",
        "    return intersection_box_corners\n",
        "\n",
        "# helper function to calculate the area of intersection between two boxes\n",
        "\n",
        "\n",
        "def calculate_intersection_area(intersection_box_corners):\n",
        "    # find the width = x_max - x_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_width = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 2] - intersection_box_corners[:, :, :, 0])\n",
        "    # find the height = y_max - y_min, if the boxes are not intersecting, this value could be negative or 0\n",
        "    intersection_height = tf.maximum(\n",
        "        0.0, intersection_box_corners[:, :, :, 3] - intersection_box_corners[:, :, :, 1])\n",
        "    # intersection area = width * height\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    return intersection_area\n",
        "\n",
        "# helper function to calcualte the area of union between two boxes\n",
        "\n",
        "\n",
        "def calculate_union_area(box_1_dimensions, box_2_dimensions, intersection_area):\n",
        "    box_1_area = box_1_dimensions[:, :, :, 0] * box_1_dimensions[:, :, :, 1]\n",
        "    box_2_area = box_2_dimensions[:, :, :, 0] * box_2_dimensions[:, :, :, 1]\n",
        "    union_area = box_1_area + box_2_area - intersection_area\n",
        "    return union_area\n",
        "\n",
        "# helper function to calculate the IOU ration between boxes\n",
        "\n",
        "\n",
        "def calculate_iou(intersection_area, union_area):\n",
        "    iou = intersection_area / (union_area + 1e-8)\n",
        "    return iou\n",
        "\n",
        "# helper function to calculate indices for the grid cells that contain the object\n",
        "\n",
        "\n",
        "def calculate_grid_cell_indices(y_true, y_pred):\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "\n",
        "\n",
        "    # Read the bounding box centers\n",
        "    # Each instance in the bach will have 5 bounding box centers\n",
        "    # select boxes with objectness equal to 1\n",
        "    # objectness_mask = y_true[:, :, 0] == 1.0\n",
        "    # bounding_boxes_with_objects = tf.boolean_mask(y_true, mask=objectness_mask)\n",
        "    # print(f\"bounding_boxes_with_objects.shape : {bounding_boxes_with_objects.shape}\")\n",
        "    bounding_box_centers = y_true[:, :, 1:3]\n",
        "\n",
        "    # TODO:  here we are assuming number of rows and columns in grid is same. Confirm the assumption.\n",
        "    # The general formula is: grid_index = floor(pixel_coordinate * (grid_size / image_size))\n",
        "    # convert each 5 bounding box centers to 5 possible grids for each instance\n",
        "\n",
        "    grid_indices = tf.cast(\n",
        "        tf.floor(bounding_box_centers * tf.cast((x_grid_size / 100),tf.float32)), dtype=tf.int32)\n",
        "\n",
        "    ## grid_indices can be out of bounds if the box centers lie on coordinates that are in multiple of 100\n",
        "    ## so we need to clip them to be maximum of 5 and minimum of 0\n",
        "    grid_indices = tf.clip_by_value(grid_indices, clip_value_min=0, clip_value_max=x_grid_size - 1)\n",
        "\n",
        "    # grid_indices = tf.reshape(grid_indices,shape=(batch_size,-1,2))\n",
        "    print(f\"grid indices shape {tf.shape(grid_indices)}\")\n",
        "    print(f\"grid indices {grid_indices}\")\n",
        "\n",
        "\n",
        "    return grid_indices\n",
        "\n",
        "# Helper function to calculate anchor box indices.\n",
        "\n",
        "\n",
        "def calculate_anchorbox_indices(y_true, y_pred, grid_cell_indices):\n",
        "    x_grid_size = y_pred.shape[1]\n",
        "    y_grid_size = y_pred.shape[2]\n",
        "\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(-1, x_grid_size, y_grid_size, 3, 15))\n",
        "    print(f\"anchor_boxes.shape {anchor_boxes.shape}\")\n",
        "\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    # calcualte the IOU between anchor boxes and ground truth\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "\n",
        "    # calculate min and max values for ground truth and anchor boxes\n",
        "    y_true_box_corners = convert_boxes_to_corners(\n",
        "        expanded_y_true[:, :, :, 1:5])\n",
        "    y_pred_box_corners = convert_boxes_to_corners(\n",
        "        selected_anchor_boxes[:, :, :, 1:5])\n",
        "    print(f\"y_true_boxes.shape {y_true_box_corners.shape}\")\n",
        "    print(f\"y_pred_boxes.shape {y_pred_box_corners.shape}\")\n",
        "\n",
        "    # calculate the intersection coordinates between ground truth and anchor boxes\n",
        "    intersection_box_corners = calculate_intersection_corners(\n",
        "        y_true_box_corners[:, :, :, 0:], y_pred_box_corners[:, :, :, 0:])\n",
        "    print(f\"intersection_box_corners.shape {intersection_box_corners.shape}\")\n",
        "\n",
        "    # calculate the IOU\n",
        "    # calculate intersection area\n",
        "    intersection_area = calculate_intersection_area(intersection_box_corners)\n",
        "    print(f\"intersection_area.shape {intersection_area.shape}\")\n",
        "\n",
        "    # calculate union area\n",
        "    # we just need the width and length for union area\n",
        "    union_area = calculate_union_area(\n",
        "        expanded_y_true[:, :, :, 3:5], selected_anchor_boxes[:, :, :, 3:5], intersection_area)\n",
        "    print(f\"union_area.shape {union_area.shape}\")\n",
        "\n",
        "    # calculate IOU\n",
        "    iou = calculate_iou(intersection_area, union_area)\n",
        "    print(f\"iou.shape {iou.shape}\")\n",
        "\n",
        "    # select the anchor box based on best iou score\n",
        "    # select the index with highest iou\n",
        "    highest_iou_index = tf.argmax(iou, axis=2, output_type=tf.int32)\n",
        "    print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "\n",
        "    highest_iou_index = tf.expand_dims(highest_iou_index, axis=2)\n",
        "    # highest_iou_index = tf.reshape(highest_iou_index, shape=(iou.shape[0],iou.shape[1],-1))\n",
        "    print(f\"expanded highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    return highest_iou_index\n",
        "\n",
        "\n",
        "\n",
        "def calculate_best_anchor_boxes(y_true, y_pred):\n",
        "    # helper function to calculate best anchor boxes\n",
        "    x_grid_size = tf.shape(y_pred)[1]\n",
        "    y_grid_size = tf.shape(y_pred)[2]\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    print(\"----- True Values -----\")\n",
        "    print(f\"y_true.shape {tf.shape(y_true)}\")\n",
        "\n",
        "    print(\"----- Pred Values -----\")\n",
        "    print(f\"y_pred.shape {tf.shape(y_pred)}\")\n",
        "\n",
        "    # we have 6x6, each grid cell has 3 anchor box i.e 108 anchor boxes per insantance\n",
        "    anchor_boxes = tf.reshape(\n",
        "        y_pred, shape=(batch_size, x_grid_size, y_grid_size, 3, 15))\n",
        "    print(f\"anchor_boxes.shape {tf.shape(anchor_boxes)}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # out of 36 grid cells (per instance) select at most 5 grid cells that have ground truth bounding box\n",
        "    # so out of 108 anchor boxes (per instance) we only need to check 15 anchor boxes\n",
        "    selected_anchor_boxes = tf.gather_nd(\n",
        "        params=anchor_boxes, batch_dims=1, indices=grid_cell_indices)\n",
        "    print(f\"selected_anchor_boxes.shape :{selected_anchor_boxes.shape}\")\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "    # select the anchor box based on the index\n",
        "    best_anchor_boxes = tf.gather(\n",
        "        selected_anchor_boxes, indices=highest_iou_index, batch_dims=2)\n",
        "    print(f\"best_anchor_boxes.shape {best_anchor_boxes.shape}\")\n",
        "\n",
        "    return best_anchor_boxes\n",
        "\n",
        "# helper function to split and calculate loss\n",
        "\n",
        "\n",
        "def calculate_loss(predicted_values, true_values):\n",
        "\n",
        "    objectness_mask = true_values[:, :, :, 0] == 1.0\n",
        "\n",
        "    true_values_with_objects = tf.boolean_mask(\n",
        "        true_values, mask=objectness_mask)\n",
        "    predicted_values_with_objects = tf.boolean_mask(\n",
        "        predicted_values, mask=objectness_mask)\n",
        "\n",
        "    print(f\"true_values_with_objects.shape : {true_values_with_objects.shape}\")\n",
        "    print(\n",
        "        f\"predicted_values_with_objects.shape : {predicted_values_with_objects.shape}\")\n",
        "    # slice the 3 properties that we are tyring to calculate loss against\n",
        "    # predicted values\n",
        "\n",
        "    y_pred_objectness = predicted_values_with_objects[:, 0]\n",
        "    print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_bounding_box = predicted_values_with_objects[:, 1:5]\n",
        "    print(f\"y_pred_bounding_box.shape : {y_pred_bounding_box.shape}\")\n",
        "\n",
        "    y_pred_classification = predicted_values_with_objects[:, 5:]\n",
        "    print(f\"y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = true_values_with_objects[:, 0]\n",
        "    print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "    y_true_bounding_box = true_values_with_objects[:, 1:5]\n",
        "    print(f\"y_true_bounding_box.shape : {y_true_bounding_box.shape}\")\n",
        "\n",
        "    y_true_classification = true_values_with_objects[:, 5:]\n",
        "    print(f\"y_true_classification.shape : {y_true_classification.shape}\")\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    print(\n",
        "        f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    y_pred_classification = tf.keras.activations.softmax(y_pred_classification)\n",
        "    print(\n",
        "        f\"Post Activation y_pred_classification.shape : {y_pred_classification.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectness_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "    bounding_box_loss = tf.keras.losses.MeanSquaredError()(\n",
        "        y_true_bounding_box, y_pred_bounding_box)\n",
        "    classification_loss = tf.keras.losses.CategoricalCrossentropy()(\n",
        "        y_true_classification, y_pred_classification)\n",
        "\n",
        "    return objectness_loss, bounding_box_loss, classification_loss\n",
        "\n",
        "# helper function to calculate loss for object less cells\n",
        "\n",
        "\n",
        "def calculate_objectless_loss(y_true, y_pred):\n",
        "    # step 1: create placeholder y_true\n",
        "    batch_size = tf.shape(y_true)[0] # Get batch size dynamically\n",
        "\n",
        "    bounding_box_with_object_mask = y_true[:, :, 0] == 1.0\n",
        "\n",
        "    # step 2: prepare mask for positive values\n",
        "    # hard coding the grid size\n",
        "    positive_mask = tf.fill(dims=(batch_size, 6, 6, 3), value=False) # Adjust shape\n",
        "\n",
        "    print(f\"positive_mask.shape {positive_mask.shape}\")\n",
        "\n",
        "    grid_cell_indices = calculate_grid_cell_indices(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    # grid cell indices will have shape (m, 5, 2)\n",
        "    # here 5 is max images and 2 is row and column index\n",
        "\n",
        "    highest_iou_index = calculate_anchorbox_indices(\n",
        "        y_true=y_true, y_pred=y_pred, grid_cell_indices=grid_cell_indices)\n",
        "\n",
        "    # highest_iou_index = tf.boolean_mask(\n",
        "    #     highest_iou_index, mask=bounding_box_with_object_mask)\n",
        "    print(f\"highest_iou_index.shape {highest_iou_index.shape}\")\n",
        "    # highest iou index will have shpae (m,5,1)\n",
        "    # here 5 is max images and 1 represents best anchor box in the cell.\n",
        "\n",
        "    # we need to combine both the indices to create tensor of shape (m, row indices, column indices, box index)\n",
        "    combine_update_index = tf.range(batch_size)\n",
        "    # expand dims\n",
        "    combine_update_index = tf.reshape(\n",
        "        combine_update_index, shape=(batch_size, 1, 1))\n",
        "    # combine_update_index = tf.expand_dims(combine_update_index, axis=2)\n",
        "    combine_update_index = tf.tile(\n",
        "        combine_update_index, [1, 5, 1])\n",
        "    combine_update_index = tf.concat(\n",
        "        [combine_update_index, grid_cell_indices, highest_iou_index], axis=2)\n",
        "\n",
        "    combine_update_index = tf.boolean_mask(\n",
        "        combine_update_index, mask=bounding_box_with_object_mask)\n",
        "\n",
        "    print(f\"combine_update_index.shape : {combine_update_index.shape}\")\n",
        "\n",
        "    positive_mask_shape = tf.shape(positive_mask)\n",
        "    positive_mask = tf.scatter_nd(\n",
        "        indices=combine_update_index,\n",
        "        shape=positive_mask_shape,\n",
        "        updates=tf.fill(dims=(tf.shape(combine_update_index)[0],), value=True))\n",
        "\n",
        "    # select predicted anchor boxes based on negative masked values\n",
        "    negative_mask = tf.logical_not(positive_mask)\n",
        "    print(f\"negative_mask.shape : {negative_mask.shape}\")\n",
        "    objectless_anchorboxes = tf.boolean_mask(tf.reshape(\n",
        "        y_pred, shape=(batch_size, 6, 6, 3, -1)), mask=negative_mask)\n",
        "    print(f\"masked_values.shape : {objectless_anchorboxes.shape}\")\n",
        "    y_true_objectless = tf.zeros(\n",
        "        shape=tf.shape(objectless_anchorboxes), dtype=tf.float32)\n",
        "    print(f\"y_true_objectless.shape {y_true_objectless.shape}\")\n",
        "\n",
        "\n",
        "    y_pred_objectness = objectless_anchorboxes[:, 0]\n",
        "    print(f\"y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # True Values\n",
        "    y_true_objectness = y_true_objectless[:, 0]\n",
        "    print(f\"y_true_objectness.shape : {y_true_objectness.shape}\")\n",
        "\n",
        "\n",
        "    # Apply activation functions to predicted values\n",
        "    y_pred_objectness = tf.keras.activations.sigmoid(y_pred_objectness)\n",
        "    print(\n",
        "        f\"Post Activation y_pred_objectness.shape : {y_pred_objectness.shape}\")\n",
        "\n",
        "    # Calculate loss\n",
        "    objectless_loss = tf.keras.losses.BinaryCrossentropy(\n",
        "        from_logits=False)(y_true_objectness, y_pred_objectness)\n",
        "\n",
        "    return objectless_loss\n",
        "\n",
        "# loss function for the model\n",
        "\n",
        "\n",
        "def calculate_model_loss(y_true, y_pred):\n",
        "    # Find best anchor box\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "\n",
        "    print(\"\\n\\n----- Localization Loss -----\")\n",
        "    print(f\"objectness_loss : {objectness_loss}\")\n",
        "    print(f\"bounding_box_loss : {bounding_box_loss}\")\n",
        "    print(f\"classification_loss : {classification_loss}\")\n",
        "\n",
        "    # objectless loss calculation\n",
        "    print(\"\\n\\n----- Calculation Object Less Loss -----\")\n",
        "    objectless_loss = calculate_objectless_loss(\n",
        "        y_true=y_true, y_pred=y_pred)\n",
        "\n",
        "    print(\"\\n\\n----- Object Less Loss -----\")\n",
        "    print(f\"objectless_loss : {objectless_loss}\")\n",
        "\n",
        "    # scale the losses\n",
        "    lambda_objectness = 1\n",
        "    lambda_bounding_box = 0.001\n",
        "    lambda_classification = 1\n",
        "    lambda_objectless = 1\n",
        "\n",
        "    total_loss = (objectness_loss * lambda_objectness) + (bounding_box_loss *\n",
        "                                                          lambda_bounding_box) + (classification_loss * lambda_classification) + (objectless_loss * lambda_objectless)\n",
        "\n",
        "    print(f\"\\n\\nTotal Loss : {total_loss}\")\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def objectness_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return objectness_loss\n",
        "\n",
        "\n",
        "def bounding_box_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return bounding_box_loss\n",
        "\n",
        "\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    expanded_y_true = tf.expand_dims(y_true, axis=2)\n",
        "    best_anchor_boxes = calculate_best_anchor_boxes(y_true, y_pred)\n",
        "\n",
        "    # Loss Calculation\n",
        "    objectness_loss, bounding_box_loss, classification_loss = calculate_loss(\n",
        "        best_anchor_boxes, expanded_y_true)\n",
        "    return classification_loss"
      ],
      "metadata": {
        "id": "9N1MLKguS9Hh"
      },
      "id": "9N1MLKguS9Hh",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running The Model Manually"
      ],
      "metadata": {
        "id": "ErjI4ehgTVl0"
      },
      "id": "ErjI4ehgTVl0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the Model"
      ],
      "metadata": {
        "id": "-Ci-WgQ3TKaC"
      },
      "id": "-Ci-WgQ3TKaC"
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "\n",
        "    tf.keras.layers.Rescaling(scale=1./255),\n",
        "\n",
        "    ## starting with a larger filter since we are dealing with 100x100x1 image\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    ## rest of the layers are same as our original mnist classifier\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    ## finaly layers to output 6x6x45 grid of predictions\n",
        "    tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear'),\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "GtSDiOxNTFtD"
      },
      "id": "GtSDiOxNTFtD",
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "aqzrwQnvTPSM",
        "outputId": "8b751bab-fe4a-4ffa-b12b-246980d2fd79"
      },
      "id": "aqzrwQnvTPSM",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling_10 (\u001b[38;5;33mRescaling\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_108 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_109 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_48 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_110 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_111 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_49 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_112 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_113 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_50 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_114 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_115 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_51 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_116 (\u001b[38;5;33mConv2D\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ rescaling_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_108 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_109 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_110 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_111 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_112 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_113 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_114 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_115 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_116 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run in Eager Mode"
      ],
      "metadata": {
        "id": "ZfPbA4gSg0Hp"
      },
      "id": "ZfPbA4gSg0Hp"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is temp code to test the loss function do not use this for training.\n",
        "X_tensor = tf.convert_to_tensor(x_train[0:10], dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor, shape=(-1, 28, 28, 1))\n",
        "y_tensor = tf.convert_to_tensor(y_train[0:10], dtype=tf.float32)\n",
        "\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5, 15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "\n",
        "processed_dataset = raw_dataset.map(lambda X, y: tf.numpy_function(generate_training_example, inp=[X, y], Tout=(\n",
        "    tf.float32, tf.float32)), num_parallel_calls=15);\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=8)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "# Step 1: Get one batch of data from your dataset pipeline\n",
        "# The .take(1) method creates a new dataset with only the first element.\n",
        "one_batch = processed_dataset.take(10)\n",
        "\n",
        "# Step 2: Iterate over the single batch to get the tensors\n",
        "for images, labels in one_batch:\n",
        "\n",
        "    # --- THIS IS YOUR DEBUGGING ZONE ---\n",
        "    # Now you have the concrete tensors for one batch.\n",
        "    # You can inspect them with regular print() and .numpy()\n",
        "\n",
        "    # print(\"--- Inspecting Data Before Loss Calculation ---\")\n",
        "    # print(\"Shape of images (X_batch):\", images.shape)\n",
        "    # print(\"Shape of labels (y_true_batch):\", labels.shape)\n",
        "    # print(\"\\nSample y_true label tensor:\\n\", labels.numpy()[0]) # Print the first label in the batch\n",
        "    # ------------------------------------\n",
        "\n",
        "    # Step 3: Manually run the forward pass and gradient calculation\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Get the model's raw predictions for this batch\n",
        "        y_pred = model(images, training=True)  # Pass the images through the model\n",
        "\n",
        "        # --- MORE DEBUGGING ---\n",
        "        print(\"\\n--- Inspecting Tensors Passed to Loss Function ---\")\n",
        "        print(\"Shape of y_pred from model:\", y_pred.shape)\n",
        "        # print(\"\\nSample y_pred tensor (first 5 values of first anchor):\\n\", y_pred.numpy()[0, 0, 0, :5])\n",
        "        # ----------------------\n",
        "\n",
        "        # Call your custom loss function\n",
        "        # You can now add print statements INSIDE your loss function too!\n",
        "        loss_value = calculate_model_loss(labels, y_pred)\n",
        "\n",
        "        print(\"\\n--- Final Calculated Loss ---\")\n",
        "\n",
        "        print(\"Total Loss for the batch:\", loss_value.numpy())\n",
        "        # -----------------------------\n",
        "\n",
        "    # Step 4 (Optional): Calculate and apply gradients to see the full loop\n",
        "    # grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    # model.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    print(\"\\n--- Manual Step Complete ---\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJL4G8cVTYqo",
        "outputId": "a53a152a-fd4d-4752-ca20-87c5d7852fdf"
      },
      "id": "lJL4G8cVTYqo",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (8, 6, 6, 45)\n",
            "----- True Values -----\n",
            "y_true.shape [ 8  5 15]\n",
            "----- Pred Values -----\n",
            "y_pred.shape [ 8  6  6 45]\n",
            "anchor_boxes.shape [ 8  6  6  3 15]\n",
            "grid indices shape [8 5 2]\n",
            "grid indices [[[2 3]\n",
            "  [0 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 4]\n",
            "  [4 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 4]\n",
            "  [5 3]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[1 2]\n",
            "  [1 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 2]\n",
            "  [2 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 4]\n",
            "  [1 2]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 1]\n",
            "  [3 3]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 4]\n",
            "  [1 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "anchor_boxes.shape (8, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "y_true_boxes.shape (8, 5, 1, 4)\n",
            "y_pred_boxes.shape (8, 5, 3, 4)\n",
            "intersection_box_corners.shape (8, 5, 3, 4)\n",
            "intersection_area.shape (8, 5, 3)\n",
            "union_area.shape (8, 5, 3)\n",
            "iou.shape (8, 5, 3)\n",
            "highest_iou_index.shape (8, 5)\n",
            "expanded highest_iou_index.shape (8, 5, 1)\n",
            "best_anchor_boxes.shape (8, 5, 1, 15)\n",
            "true_values_with_objects.shape : (16, 15)\n",
            "predicted_values_with_objects.shape : (16, 15)\n",
            "y_pred_objectness.shape : (16,)\n",
            "y_pred_bounding_box.shape : (16, 4)\n",
            "y_pred_classification.shape : (16, 10)\n",
            "y_true_objectness.shape : (16,)\n",
            "y_true_bounding_box.shape : (16, 4)\n",
            "y_true_classification.shape : (16, 10)\n",
            "Post Activation y_pred_objectness.shape : (16,)\n",
            "Post Activation y_pred_classification.shape : (16, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : 0.6918237209320068\n",
            "bounding_box_loss : 1743.3218994140625\n",
            "classification_loss : 2.302119255065918\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (8, 6, 6, 3)\n",
            "grid indices shape [8 5 2]\n",
            "grid indices [[[2 3]\n",
            "  [0 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 4]\n",
            "  [4 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 4]\n",
            "  [5 3]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[1 2]\n",
            "  [1 5]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[4 2]\n",
            "  [2 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[0 4]\n",
            "  [1 2]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 1]\n",
            "  [3 3]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[2 4]\n",
            "  [1 1]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "anchor_boxes.shape (8, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(8, 5, 3, 15)\n",
            "y_true_boxes.shape (8, 5, 1, 4)\n",
            "y_pred_boxes.shape (8, 5, 3, 4)\n",
            "intersection_box_corners.shape (8, 5, 3, 4)\n",
            "intersection_area.shape (8, 5, 3)\n",
            "union_area.shape (8, 5, 3)\n",
            "iou.shape (8, 5, 3)\n",
            "highest_iou_index.shape (8, 5)\n",
            "expanded highest_iou_index.shape (8, 5, 1)\n",
            "highest_iou_index.shape (8, 5, 1)\n",
            "combine_update_index.shape : (16, 4)\n",
            "negative_mask.shape : (8, 6, 6, 3)\n",
            "masked_values.shape : (848, 15)\n",
            "y_true_objectless.shape (848, 15)\n",
            "y_pred_objectness.shape : (848,)\n",
            "y_true_objectness.shape : (848,)\n",
            "Post Activation y_pred_objectness.shape : (848,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6915498971939087\n",
            "\n",
            "\n",
            "Total Loss : 5.428814888000488\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: 5.428815\n",
            "\n",
            "--- Manual Step Complete ---\n",
            "\n",
            "--- Inspecting Tensors Passed to Loss Function ---\n",
            "Shape of y_pred from model: (2, 6, 6, 45)\n",
            "----- True Values -----\n",
            "y_true.shape [ 2  5 15]\n",
            "----- Pred Values -----\n",
            "y_pred.shape [ 2  6  6 45]\n",
            "anchor_boxes.shape [ 2  6  6  3 15]\n",
            "grid indices shape [2 5 2]\n",
            "grid indices [[[1 3]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 2]\n",
            "  [4 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "anchor_boxes.shape (2, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "y_true_boxes.shape (2, 5, 1, 4)\n",
            "y_pred_boxes.shape (2, 5, 3, 4)\n",
            "intersection_box_corners.shape (2, 5, 3, 4)\n",
            "intersection_area.shape (2, 5, 3)\n",
            "union_area.shape (2, 5, 3)\n",
            "iou.shape (2, 5, 3)\n",
            "highest_iou_index.shape (2, 5)\n",
            "expanded highest_iou_index.shape (2, 5, 1)\n",
            "best_anchor_boxes.shape (2, 5, 1, 15)\n",
            "true_values_with_objects.shape : (4, 15)\n",
            "predicted_values_with_objects.shape : (4, 15)\n",
            "y_pred_objectness.shape : (4,)\n",
            "y_pred_bounding_box.shape : (4, 4)\n",
            "y_pred_classification.shape : (4, 10)\n",
            "y_true_objectness.shape : (4,)\n",
            "y_true_bounding_box.shape : (4, 4)\n",
            "y_true_classification.shape : (4, 10)\n",
            "Post Activation y_pred_objectness.shape : (4,)\n",
            "Post Activation y_pred_classification.shape : (4, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : 0.6912966370582581\n",
            "bounding_box_loss : 2211.255615234375\n",
            "classification_loss : 2.306511402130127\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (2, 6, 6, 3)\n",
            "grid indices shape [2 5 2]\n",
            "grid indices [[[1 3]\n",
            "  [3 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]\n",
            "\n",
            " [[3 2]\n",
            "  [4 4]\n",
            "  [0 0]\n",
            "  [0 0]\n",
            "  [0 0]]]\n",
            "anchor_boxes.shape (2, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(2, 5, 3, 15)\n",
            "y_true_boxes.shape (2, 5, 1, 4)\n",
            "y_pred_boxes.shape (2, 5, 3, 4)\n",
            "intersection_box_corners.shape (2, 5, 3, 4)\n",
            "intersection_area.shape (2, 5, 3)\n",
            "union_area.shape (2, 5, 3)\n",
            "iou.shape (2, 5, 3)\n",
            "highest_iou_index.shape (2, 5)\n",
            "expanded highest_iou_index.shape (2, 5, 1)\n",
            "highest_iou_index.shape (2, 5, 1)\n",
            "combine_update_index.shape : (4, 4)\n",
            "negative_mask.shape : (2, 6, 6, 3)\n",
            "masked_values.shape : (212, 15)\n",
            "y_true_objectless.shape (212, 15)\n",
            "y_pred_objectness.shape : (212,)\n",
            "y_true_objectness.shape : (212,)\n",
            "Post Activation y_pred_objectness.shape : (212,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : 0.6913722157478333\n",
            "\n",
            "\n",
            "Total Loss : 5.900435924530029\n",
            "\n",
            "--- Final Calculated Loss ---\n",
            "Total Loss for the batch: 5.900436\n",
            "\n",
            "--- Manual Step Complete ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model for 20 Epochs\n",
        "\n",
        "* So far our loss functions work as expected in Eager mode.  \n",
        "* In this section we'll run the loss functions in optimized `Graph Mode` to see if it works as expected and also if the loss decreases as we progress.\n",
        "* Our experiments in other notebooks we know that we'll need to create the model using Functional API."
      ],
      "metadata": {
        "id": "p5Ky6tdhbwEr"
      },
      "id": "p5Ky6tdhbwEr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Dataset"
      ],
      "metadata": {
        "id": "svkr5mXxfHNG"
      },
      "id": "svkr5mXxfHNG"
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "X_tensor = tf.reshape(X_tensor,shape=(-1,28,28,1))\n",
        "y_tensor = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "batch_size = 32\n",
        "\n",
        "raw_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))\n",
        "\n",
        "def generative_py_function(func, inp, Tout, shape_out):\n",
        "    # This is the bridge that calls your NumPy code\n",
        "    y = tf.numpy_function(func, inp, Tout)\n",
        "    # This is the crucial step: re-apply the shape information\n",
        "    y[0].set_shape(shape_out[0]) # Set shape for the image\n",
        "    y[1].set_shape(shape_out[1]) # Set shape for the labels\n",
        "    return y\n",
        "\n",
        "# Define the exact output shapes you expect\n",
        "output_shapes = ([100, 100, 1], [5,15])\n",
        "# Define the exact output data types you expect\n",
        "output_types = (tf.float32, tf.float32)\n",
        "\n",
        "# Use the wrapper inside the map\n",
        "processed_dataset = raw_dataset.map(lambda X, y: generative_py_function(\n",
        "    generate_training_example,\n",
        "    inp=[X, y],\n",
        "    Tout=output_types, # Pass the dtypes to Tout\n",
        "    shape_out=output_shapes # Pass the shapes to our new argument\n",
        ")).batch(batch_size=batch_size)"
      ],
      "metadata": {
        "id": "gkzQx-sBeYr_"
      },
      "id": "gkzQx-sBeYr_",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize Model"
      ],
      "metadata": {
        "id": "YgkYBImJfQbl"
      },
      "id": "YgkYBImJfQbl"
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(100,100,1),batch_size=batch_size ,name=\"input_layer\")\n",
        "\n",
        "x = tf.keras.layers.Rescaling(scale=1./255, name=\"rescaling\")(inputs)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=5, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D()(x)\n",
        "\n",
        "outputs = tf.keras.layers.Conv2D(filters=45, kernel_size=1, padding='same', activation='linear')(x)\n",
        "\n",
        "# Define the final model by specifying its inputs and outputs\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "XOoSfkYEfNyj",
        "outputId": "049ea34f-13a3-46a8-ae0e-a3cd2079c2cd"
      },
      "id": "XOoSfkYEfNyj",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_99 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │           \u001b[38;5;34m208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_100 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m8\u001b[0m)      │         \u001b[38;5;34m1,608\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_44 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_101 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_102 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │           \u001b[38;5;34m584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_45 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m8\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_103 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m1,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_104 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │         \u001b[38;5;34m2,320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_46 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_105 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m4,640\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_106 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m9,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_47 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_107 (\u001b[38;5;33mConv2D\u001b[0m)             │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m45\u001b[0m)         │         \u001b[38;5;34m1,485\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_99 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_100 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,608</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_101 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_102 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_103 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_104 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_105 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_106 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_107 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,485</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,845\u001b[0m (85.33 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,845</span> (85.33 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Callbacks"
      ],
      "metadata": {
        "id": "oYkmsyYFfVG_"
      },
      "id": "oYkmsyYFfVG_"
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: Define the callbacks\n",
        "checkpoint_filepath = './models/experiment_1_{epoch:02d}.keras'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='loss',\n",
        "    mode='min',\n",
        "    save_best_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=1,\n",
        "    )"
      ],
      "metadata": {
        "id": "j-30sp7AfSYt"
      },
      "id": "j-30sp7AfSYt",
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile & Fit"
      ],
      "metadata": {
        "id": "Yzyrwlo6fZR1"
      },
      "id": "Yzyrwlo6fZR1"
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001,clipnorm=1.0),\n",
        "              loss=calculate_model_loss,\n",
        "              metrics=[objectness_metrics, bounding_box_metrics, classification_metrics])\n",
        "## step 5: Fit the model\n",
        "epochs=20\n",
        "\n",
        "history = model.fit(\n",
        "  processed_dataset,\n",
        "  epochs=epochs,\n",
        "  callbacks=[model_checkpoint_callback]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAB0cfrqfW3s",
        "outputId": "f40284f7-9fcc-4586-9217-0dab59079395"
      },
      "id": "gAB0cfrqfW3s",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"compile_loss/calculate_model_loss/Shape_3:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"compile_loss/calculate_model_loss/Shape_4:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"compile_loss/calculate_model_loss/Shape_5:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_7:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy/Mean:0\", shape=(), dtype=float32)\n",
            "bounding_box_loss : Tensor(\"compile_loss/calculate_model_loss/mean_squared_error/div_no_nan:0\", shape=(), dtype=float32)\n",
            "classification_loss : Tensor(\"compile_loss/calculate_model_loss/categorical_crossentropy/div_no_nan:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (None, 6, 6, 3)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_10:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "highest_iou_index.shape (None, 5, 1)\n",
            "combine_update_index.shape : (None, 4)\n",
            "negative_mask.shape : (None, 6, 6, 3)\n",
            "masked_values.shape : (None, None)\n",
            "y_true_objectless.shape (None, None)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_true_objectness.shape : (None,)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy_1/Mean:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "Total Loss : Tensor(\"compile_loss/calculate_model_loss/add_14:0\", shape=(), dtype=float32)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_5:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_6:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_7:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_9:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_13:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_14:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_15:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_17:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_21:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_22:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_23:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_25:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_2:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"compile_loss/calculate_model_loss/Shape_3:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"compile_loss/calculate_model_loss/Shape_4:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"compile_loss/calculate_model_loss/Shape_5:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_7:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\n",
            "\n",
            "----- Localization Loss -----\n",
            "objectness_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy/Mean:0\", shape=(), dtype=float32)\n",
            "bounding_box_loss : Tensor(\"compile_loss/calculate_model_loss/mean_squared_error/div_no_nan:0\", shape=(), dtype=float32)\n",
            "classification_loss : Tensor(\"compile_loss/calculate_model_loss/categorical_crossentropy/div_no_nan:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "----- Calculation Object Less Loss -----\n",
            "positive_mask.shape (None, 6, 6, 3)\n",
            "grid indices shape Tensor(\"compile_loss/calculate_model_loss/Shape_10:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"compile_loss/calculate_model_loss/clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "highest_iou_index.shape (None, 5, 1)\n",
            "combine_update_index.shape : (None, 4)\n",
            "negative_mask.shape : (None, 6, 6, 3)\n",
            "masked_values.shape : (None, None)\n",
            "y_true_objectless.shape (None, None)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_true_objectness.shape : (None,)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "\n",
            "\n",
            "----- Object Less Loss -----\n",
            "objectless_loss : Tensor(\"compile_loss/calculate_model_loss/binary_crossentropy_1/Mean:0\", shape=(), dtype=float32)\n",
            "\n",
            "\n",
            "Total Loss : Tensor(\"compile_loss/calculate_model_loss/add_14:0\", shape=(), dtype=float32)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_5:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_6:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_7:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_9:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_13:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_14:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_15:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_17:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_1:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "----- True Values -----\n",
            "y_true.shape Tensor(\"Shape_21:0\", shape=(3,), dtype=int32)\n",
            "----- Pred Values -----\n",
            "y_pred.shape Tensor(\"Shape_22:0\", shape=(4,), dtype=int32)\n",
            "anchor_boxes.shape Tensor(\"Shape_23:0\", shape=(5,), dtype=int32)\n",
            "grid indices shape Tensor(\"Shape_25:0\", shape=(3,), dtype=int32)\n",
            "grid indices Tensor(\"clip_by_value_2:0\", shape=(None, 5, 2), dtype=int32)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "anchor_boxes.shape (None, 6, 6, 3, 15)\n",
            "selected_anchor_boxes.shape :(None, 5, 3, 15)\n",
            "y_true_boxes.shape (None, 5, 1, 4)\n",
            "y_pred_boxes.shape (None, 5, 3, 4)\n",
            "intersection_box_corners.shape (None, 5, 3, 4)\n",
            "intersection_area.shape (None, 5, 3)\n",
            "union_area.shape (None, 5, 3)\n",
            "iou.shape (None, 5, 3)\n",
            "highest_iou_index.shape (None, 5)\n",
            "expanded highest_iou_index.shape (None, 5, 1)\n",
            "best_anchor_boxes.shape (None, 5, 1, 15)\n",
            "true_values_with_objects.shape : (None, 15)\n",
            "predicted_values_with_objects.shape : (None, 15)\n",
            "y_pred_objectness.shape : (None,)\n",
            "y_pred_bounding_box.shape : (None, 4)\n",
            "y_pred_classification.shape : (None, 10)\n",
            "y_true_objectness.shape : (None,)\n",
            "y_true_bounding_box.shape : (None, 4)\n",
            "y_true_classification.shape : (None, 10)\n",
            "Post Activation y_pred_objectness.shape : (None,)\n",
            "Post Activation y_pred_classification.shape : (None, 10)\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 9s/step - bounding_box_metrics: 1846.1829 - classification_metrics: 2.3027 - loss: 5.5163 - objectness_metrics: 0.6778\n",
            "Epoch 1: loss improved from inf to 5.52071, saving model to ./models/experiment_1_01.keras\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 115ms/step - bounding_box_metrics: 1852.3219 - classification_metrics: 2.3038 - loss: 5.5192 - objectness_metrics: 0.6779\n",
            "Epoch 2/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 503ms/step - bounding_box_metrics: 1588.7897 - classification_metrics: 2.3037 - loss: 5.2593 - objectness_metrics: 0.6769\n",
            "Epoch 2: loss improved from 5.52071 to 5.39858, saving model to ./models/experiment_1_02.keras\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - bounding_box_metrics: 1821.1385 - classification_metrics: 2.3035 - loss: 5.3522 - objectness_metrics: 0.6768\n",
            "Epoch 3/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 494ms/step - bounding_box_metrics: 1979.9556 - classification_metrics: 2.3044 - loss: 5.6518 - objectness_metrics: 0.6776\n",
            "Epoch 3: loss did not improve from 5.39858\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - bounding_box_metrics: 1806.7489 - classification_metrics: 2.3051 - loss: 5.5827 - objectness_metrics: 0.6773 \n",
            "Epoch 4/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 563ms/step - bounding_box_metrics: 1580.0244 - classification_metrics: 2.3041 - loss: 5.2479 - objectness_metrics: 0.6742\n",
            "Epoch 4: loss improved from 5.39858 to 5.33313, saving model to ./models/experiment_1_04.keras\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 108ms/step - bounding_box_metrics: 1717.8290 - classification_metrics: 2.3065 - loss: 5.3047 - objectness_metrics: 0.6760\n",
            "Epoch 5/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 529ms/step - bounding_box_metrics: 1942.7998 - classification_metrics: 2.3060 - loss: 5.6110 - objectness_metrics: 0.6731\n",
            "Epoch 5: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - bounding_box_metrics: 1932.1882 - classification_metrics: 2.3060 - loss: 5.6067 - objectness_metrics: 0.6728 \n",
            "Epoch 6/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 553ms/step - bounding_box_metrics: 1651.2871 - classification_metrics: 2.3019 - loss: 5.3097 - objectness_metrics: 0.6676\n",
            "Epoch 6: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - bounding_box_metrics: 1779.9619 - classification_metrics: 2.3009 - loss: 5.3616 - objectness_metrics: 0.6695 \n",
            "Epoch 7/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 445ms/step - bounding_box_metrics: 1832.7372 - classification_metrics: 2.3031 - loss: 5.4915 - objectness_metrics: 0.6668\n",
            "Epoch 7: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - bounding_box_metrics: 1929.6765 - classification_metrics: 2.3013 - loss: 5.5309 - objectness_metrics: 0.6700 \n",
            "Epoch 8/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - bounding_box_metrics: 1837.9221 - classification_metrics: 2.3018 - loss: 5.4978 - objectness_metrics: 0.6692\n",
            "Epoch 8: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - bounding_box_metrics: 2023.0900 - classification_metrics: 2.2989 - loss: 5.5710 - objectness_metrics: 0.6701 \n",
            "Epoch 9/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 448ms/step - bounding_box_metrics: 1987.6261 - classification_metrics: 2.3030 - loss: 5.6485 - objectness_metrics: 0.6691\n",
            "Epoch 9: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - bounding_box_metrics: 1746.0779 - classification_metrics: 2.3002 - loss: 5.5504 - objectness_metrics: 0.6684 \n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - bounding_box_metrics: 2050.2815 - classification_metrics: 2.3041 - loss: 5.6774 - objectness_metrics: 0.6646 \n",
            "Epoch 10: loss did not improve from 5.33313\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - bounding_box_metrics: 2066.4563 - classification_metrics: 2.3043 - loss: 5.6840 - objectness_metrics: 0.6646\n",
            "Epoch 11/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - bounding_box_metrics: 1559.7577 - classification_metrics: 2.3005 - loss: 5.2081 - objectness_metrics: 0.6599\n",
            "Epoch 11: loss improved from 5.33313 to 5.28098, saving model to ./models/experiment_1_11.keras\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - bounding_box_metrics: 1676.8727 - classification_metrics: 2.3052 - loss: 5.2567 - objectness_metrics: 0.6596\n",
            "Epoch 12/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 432ms/step - bounding_box_metrics: 2198.6982 - classification_metrics: 2.3055 - loss: 5.8527 - objectness_metrics: 0.6608\n",
            "Epoch 12: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - bounding_box_metrics: 2272.3350 - classification_metrics: 2.2997 - loss: 5.8793 - objectness_metrics: 0.6592 \n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - bounding_box_metrics: 2115.8101 - classification_metrics: 2.3017 - loss: 5.6728 - objectness_metrics: 0.6541 \n",
            "Epoch 13: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 78ms/step - bounding_box_metrics: 2163.6614 - classification_metrics: 2.3020 - loss: 5.6920 - objectness_metrics: 0.6539\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - bounding_box_metrics: 1871.0066 - classification_metrics: 2.3104 - loss: 5.4245 - objectness_metrics: 0.6491\n",
            "Epoch 14: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - bounding_box_metrics: 1923.2229 - classification_metrics: 2.3090 - loss: 5.4453 - objectness_metrics: 0.6502\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - bounding_box_metrics: 1856.1792 - classification_metrics: 2.3003 - loss: 5.4385 - objectness_metrics: 0.6464 \n",
            "Epoch 15: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - bounding_box_metrics: 1883.6456 - classification_metrics: 2.3016 - loss: 5.4499 - objectness_metrics: 0.6462\n",
            "Epoch 16/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 1s/step - bounding_box_metrics: 1830.4783 - classification_metrics: 2.2960 - loss: 5.4553 - objectness_metrics: 0.6419\n",
            "Epoch 16: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - bounding_box_metrics: 1869.0768 - classification_metrics: 2.2970 - loss: 5.4717 - objectness_metrics: 0.6432\n",
            "Epoch 17/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 492ms/step - bounding_box_metrics: 1987.3054 - classification_metrics: 2.2963 - loss: 5.6087 - objectness_metrics: 0.6385\n",
            "Epoch 17: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - bounding_box_metrics: 1857.3328 - classification_metrics: 2.2968 - loss: 5.5597 - objectness_metrics: 0.6452 \n",
            "Epoch 18/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - bounding_box_metrics: 1632.9368 - classification_metrics: 2.2847 - loss: 5.2310 - objectness_metrics: 0.6273\n",
            "Epoch 18: loss did not improve from 5.28098\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - bounding_box_metrics: 1761.5447 - classification_metrics: 2.2939 - loss: 5.2856 - objectness_metrics: 0.6259 \n",
            "Epoch 19/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 570ms/step - bounding_box_metrics: 1550.2909 - classification_metrics: 2.3001 - loss: 5.1632 - objectness_metrics: 0.6269\n",
            "Epoch 19: loss improved from 5.28098 to 5.21827, saving model to ./models/experiment_1_19.keras\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 107ms/step - bounding_box_metrics: 1641.1279 - classification_metrics: 2.2952 - loss: 5.1999 - objectness_metrics: 0.6327\n",
            "Epoch 20/20\n",
            "\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - bounding_box_metrics: 1951.8774 - classification_metrics: 2.2900 - loss: 5.5496 - objectness_metrics: 0.6222\n",
            "Epoch 20: loss did not improve from 5.21827\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - bounding_box_metrics: 1829.9503 - classification_metrics: 2.2971 - loss: 5.5041 - objectness_metrics: 0.6232 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "namrr1vJfbEV"
      },
      "id": "namrr1vJfbEV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}